{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packed ensemble submission process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T07:56:44.432182600Z",
     "start_time": "2023-12-18T07:56:39.706414800Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lips import get_root_path\n",
    "from lips.dataset.scaler.standard_scaler import StandardScaler\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "from lips.dataset.airfransDataSet import download_data\n",
    "from lips.augmented_simulators.torch_simulator import TorchSimulator\n",
    "from lips.dataset.scaler.standard_scaler_iterative import StandardScalerIterative\n",
    "\n",
    "#from my_custom_packed_ensemble import *\n",
    "#from my_packed_cv import *\n",
    "from my_augmented_simulator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Step (Load the required data) <a id='generic_step'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T07:56:44.448159900Z",
     "start_time": "2023-12-18T07:56:44.432182600Z"
    }
   },
   "outputs": [],
   "source": [
    "# indicate required paths\n",
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = '../ml4physim_startingkit/Dataset'\n",
    "BENCHMARK_NAME = \"Case1\"\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the configuration files path, that aim to describe specific caracteristics of the use case or the augmented simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T07:56:44.470184200Z",
     "start_time": "2023-12-18T07:56:44.448159900Z"
    }
   },
   "outputs": [],
   "source": [
    "BENCH_CONFIG_PATH = os.path.join(\"airfoilConfigurations\", \"benchmarks\",\n",
    "                                 \"confAirfoil.ini\")  #Configuration file related to the benchmark\n",
    "SIM_CONFIG_PATH = r\"config.ini\"  #Configuration file re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T07:56:44.481696800Z",
     "start_time": "2023-12-18T07:56:44.463673300Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(DIRECTORY_NAME):\n",
    "    download_data(root_path=\".\", directory_name=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset using the dedicated class used by LIPS platform offers a list of advantages:\n",
    "\n",
    "1. Ease the importing of datasets\n",
    "1. A set of functions to organize the `inputs` and `outputs` required by augmented simulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T07:56:44.497714200Z",
     "start_time": "2023-12-18T07:56:44.483696700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the required benchmark datasets\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load the airfrans dataset as a benchmark object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    benchmark : AirfRANSBenchmark\n",
    "        The airfrans benchmark object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('benchmark.pkl', 'rb') as f:\n",
    "            benchmark = pickle.load(f)\n",
    "    except:\n",
    "        benchmark = AirfRANSBenchmark(benchmark_path=DIRECTORY_NAME,\n",
    "                                    config_path=BENCH_CONFIG_PATH,\n",
    "                                    benchmark_name=BENCHMARK_NAME,\n",
    "                                    log_path=LOG_PATH)\n",
    "        benchmark.load(path=DIRECTORY_NAME)\n",
    "        with open('benchmark.pkl', 'wb') as f:\n",
    "            pickle.dump(benchmark, f)\n",
    "    \n",
    "    return benchmark\n",
    "\n",
    "#benchmark = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T07:56:44.510229800Z",
     "start_time": "2023-12-18T07:56:44.495207500Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate(config_names):\n",
    "    \"\"\"\n",
    "    Creates a packed MLP model, trains it and evaluates it on the test dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config_names : list\n",
    "        List of the names of the configurations to be used for training the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        0 if we have successfully trained and evaluated the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    for config_name in config_names:\n",
    "        print(\"Config name : \", config_name)\n",
    "        print(\"loading data...\")\n",
    "        benchmark = load_dataset()\n",
    "\n",
    "        chunk_sizes=benchmark.train_dataset.get_simulations_sizes()\n",
    "        no_norm_x=benchmark.train_dataset.get_no_normalization_axis_indices()\n",
    "        scalerParams={\"chunk_sizes\":chunk_sizes,\"no_norm_x\":no_norm_x}\n",
    "\n",
    "        name = \"packed_mlp\"\n",
    "\n",
    "        print(\"defining model...\")\n",
    "        # PackedMLP model definition \n",
    "        torch_sim = TorchSimulator(name=name,\n",
    "                           model=PackedMLP,\n",
    "                           scaler=StandardScalerIterative,\n",
    "                           scalerParams=scalerParams,\n",
    "                           log_path=None,\n",
    "                           device=\"cuda:0\",\n",
    "                           seed=42,\n",
    "                           bench_config_path=BENCH_CONFIG_PATH,\n",
    "                           bench_config_name=\"Benchmark1\",\n",
    "                           sim_config_path=SIM_CONFIG_PATH,\n",
    "                           sim_config_name=config_name,\n",
    "                          )\n",
    "        \n",
    "        print(\"training...\")\n",
    "        # model training \n",
    "        start = time.perf_counter()\n",
    "        torch_sim.train(benchmark.train_dataset, \n",
    "                save_path=None,\n",
    "                pin_memory=True, \n",
    "                non_blocking=True, \n",
    "                num_workers=6\n",
    "                )\n",
    "        end = time.perf_counter()\n",
    "        train_time = end-start\n",
    "        \n",
    "        print(\"saving model...\")\n",
    "        # saving the model \n",
    "        torch_sim.save(path=\"./models\")\n",
    "\n",
    "        print(\"evaluating model...\")\n",
    "        # evaluating the model \n",
    "        start = time.perf_counter()\n",
    "        torch_sim_metrics = benchmark.evaluate_simulator(augmented_simulator=torch_sim,\n",
    "                                                  eval_batch_size=256000,\n",
    "                                                  dataset=\"all\",\n",
    "                                                  shuffle=False,\n",
    "                                                  save_path=\"./evaluations\",\n",
    "                                                  save_predictions=True\n",
    "                                                 )\n",
    "        end = time.perf_counter()\n",
    "        evaluation_time = end-start\n",
    "        \n",
    "        # save the evaluation time to file\n",
    "        with open(f\"evaluations/{name}_{config_name}/time.txt\", \"a\") as f:\n",
    "            f.write(f\"Training took {train_time:.2f} seconds\\n\")\n",
    "            f.write(f\"Evaluation took {evaluation_time:.2f} seconds\")\n",
    "        \n",
    "        del benchmark\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-18T12:46:11.652442400Z",
     "start_time": "2023-12-18T07:56:44.512573900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config name :  DEEP_SMALL_A2\n",
      "loading data...\n",
      "defining model...\n",
      "training...\n",
      "Train Epoch: 0   Avg_Loss: 6.07765 ['MAELoss: 4.11119']\n",
      "Train Epoch: 1   Avg_Loss: 5.85345 ['MAELoss: 3.86453']\n",
      "Train Epoch: 2   Avg_Loss: 5.61441 ['MAELoss: 3.64932']\n",
      "Train Epoch: 3   Avg_Loss: 5.36938 ['MAELoss: 3.48381']\n",
      "Train Epoch: 4   Avg_Loss: 5.13334 ['MAELoss: 3.39187']\n",
      "Train Epoch: 5   Avg_Loss: 4.91894 ['MAELoss: 3.34034']\n",
      "Train Epoch: 6   Avg_Loss: 4.73939 ['MAELoss: 3.30213']\n",
      "Train Epoch: 7   Avg_Loss: 4.58647 ['MAELoss: 3.26578']\n",
      "Train Epoch: 8   Avg_Loss: 4.45419 ['MAELoss: 3.22700']\n",
      "Train Epoch: 9   Avg_Loss: 4.33539 ['MAELoss: 3.18295']\n",
      "Train Epoch: 10   Avg_Loss: 4.22725 ['MAELoss: 3.13750']\n",
      "Train Epoch: 11   Avg_Loss: 4.13377 ['MAELoss: 3.09346']\n",
      "Train Epoch: 12   Avg_Loss: 4.05200 ['MAELoss: 3.05124']\n",
      "Train Epoch: 13   Avg_Loss: 3.97870 ['MAELoss: 3.01030']\n",
      "Train Epoch: 14   Avg_Loss: 3.91198 ['MAELoss: 2.97115']\n",
      "Train Epoch: 15   Avg_Loss: 3.85033 ['MAELoss: 2.93454']\n",
      "Train Epoch: 16   Avg_Loss: 3.79240 ['MAELoss: 2.89949']\n",
      "Train Epoch: 17   Avg_Loss: 3.73701 ['MAELoss: 2.86621']\n",
      "Train Epoch: 18   Avg_Loss: 3.68429 ['MAELoss: 2.83470']\n",
      "Train Epoch: 19   Avg_Loss: 3.63340 ['MAELoss: 2.80507']\n",
      "Train Epoch: 20   Avg_Loss: 3.58434 ['MAELoss: 2.77733']\n",
      "Train Epoch: 21   Avg_Loss: 3.53689 ['MAELoss: 2.75126']\n",
      "Train Epoch: 22   Avg_Loss: 3.49082 ['MAELoss: 2.72612']\n",
      "Train Epoch: 23   Avg_Loss: 3.44602 ['MAELoss: 2.70214']\n",
      "Train Epoch: 24   Avg_Loss: 3.40223 ['MAELoss: 2.67876']\n",
      "Train Epoch: 25   Avg_Loss: 3.35943 ['MAELoss: 2.65614']\n",
      "Train Epoch: 26   Avg_Loss: 3.31747 ['MAELoss: 2.63409']\n",
      "Train Epoch: 27   Avg_Loss: 3.27652 ['MAELoss: 2.61272']\n",
      "Train Epoch: 28   Avg_Loss: 3.23642 ['MAELoss: 2.59175']\n",
      "Train Epoch: 29   Avg_Loss: 3.19668 ['MAELoss: 2.57094']\n",
      "Train Epoch: 30   Avg_Loss: 3.15805 ['MAELoss: 2.55087']\n",
      "Train Epoch: 31   Avg_Loss: 3.12054 ['MAELoss: 2.53140']\n",
      "Train Epoch: 32   Avg_Loss: 3.08402 ['MAELoss: 2.51237']\n",
      "Train Epoch: 33   Avg_Loss: 3.04843 ['MAELoss: 2.49373']\n",
      "Train Epoch: 34   Avg_Loss: 3.01383 ['MAELoss: 2.47550']\n",
      "Train Epoch: 35   Avg_Loss: 2.98008 ['MAELoss: 2.45762']\n",
      "Train Epoch: 36   Avg_Loss: 2.94726 ['MAELoss: 2.44022']\n",
      "Train Epoch: 37   Avg_Loss: 2.91539 ['MAELoss: 2.42312']\n",
      "Train Epoch: 38   Avg_Loss: 2.88437 ['MAELoss: 2.40633']\n",
      "Train Epoch: 39   Avg_Loss: 2.85405 ['MAELoss: 2.38970']\n",
      "Train Epoch: 40   Avg_Loss: 2.82448 ['MAELoss: 2.37312']\n",
      "Train Epoch: 41   Avg_Loss: 2.79551 ['MAELoss: 2.35637']\n",
      "Train Epoch: 42   Avg_Loss: 2.76759 ['MAELoss: 2.34063']\n",
      "Train Epoch: 43   Avg_Loss: 2.74059 ['MAELoss: 2.32580']\n",
      "Train Epoch: 44   Avg_Loss: 2.71454 ['MAELoss: 2.31149']\n",
      "Train Epoch: 45   Avg_Loss: 2.68941 ['MAELoss: 2.29756']\n",
      "Train Epoch: 46   Avg_Loss: 2.66500 ['MAELoss: 2.28393']\n",
      "Train Epoch: 47   Avg_Loss: 2.64133 ['MAELoss: 2.27067']\n",
      "Train Epoch: 48   Avg_Loss: 2.61833 ['MAELoss: 2.25772']\n",
      "Train Epoch: 49   Avg_Loss: 2.59599 ['MAELoss: 2.24515']\n",
      "Train Epoch: 50   Avg_Loss: 2.57430 ['MAELoss: 2.23294']\n",
      "Train Epoch: 51   Avg_Loss: 2.55325 ['MAELoss: 2.22105']\n",
      "Train Epoch: 52   Avg_Loss: 2.53283 ['MAELoss: 2.20950']\n",
      "Train Epoch: 53   Avg_Loss: 2.51302 ['MAELoss: 2.19829']\n",
      "Train Epoch: 54   Avg_Loss: 2.49378 ['MAELoss: 2.18745']\n",
      "Train Epoch: 55   Avg_Loss: 2.47495 ['MAELoss: 2.17670']\n",
      "Train Epoch: 56   Avg_Loss: 2.45651 ['MAELoss: 2.16586']\n",
      "Train Epoch: 57   Avg_Loss: 2.43861 ['MAELoss: 2.15549']\n",
      "Train Epoch: 58   Avg_Loss: 2.42126 ['MAELoss: 2.14563']\n",
      "Train Epoch: 59   Avg_Loss: 2.40437 ['MAELoss: 2.13609']\n",
      "Train Epoch: 60   Avg_Loss: 2.38791 ['MAELoss: 2.12682']\n",
      "Train Epoch: 61   Avg_Loss: 2.37185 ['MAELoss: 2.11781']\n",
      "Train Epoch: 62   Avg_Loss: 2.35623 ['MAELoss: 2.10904']\n",
      "Train Epoch: 63   Avg_Loss: 2.34097 ['MAELoss: 2.10051']\n",
      "Train Epoch: 64   Avg_Loss: 2.32606 ['MAELoss: 2.09222']\n",
      "Train Epoch: 65   Avg_Loss: 2.31149 ['MAELoss: 2.08411']\n",
      "Train Epoch: 66   Avg_Loss: 2.29722 ['MAELoss: 2.07627']\n",
      "Train Epoch: 67   Avg_Loss: 2.28332 ['MAELoss: 2.06861']\n",
      "Train Epoch: 68   Avg_Loss: 2.26975 ['MAELoss: 2.06122']\n",
      "Train Epoch: 69   Avg_Loss: 2.25643 ['MAELoss: 2.05396']\n",
      "Train Epoch: 70   Avg_Loss: 2.24351 ['MAELoss: 2.04695']\n",
      "Train Epoch: 71   Avg_Loss: 2.23086 ['MAELoss: 2.04011']\n",
      "Train Epoch: 72   Avg_Loss: 2.21854 ['MAELoss: 2.03345']\n",
      "Train Epoch: 73   Avg_Loss: 2.20651 ['MAELoss: 2.02696']\n",
      "Train Epoch: 74   Avg_Loss: 2.19472 ['MAELoss: 2.02072']\n",
      "Train Epoch: 75   Avg_Loss: 2.18308 ['MAELoss: 2.01383']\n",
      "Train Epoch: 76   Avg_Loss: 2.17142 ['MAELoss: 2.00616']\n",
      "Train Epoch: 77   Avg_Loss: 2.16026 ['MAELoss: 1.99974']\n",
      "Train Epoch: 78   Avg_Loss: 2.14942 ['MAELoss: 1.99373']\n",
      "Train Epoch: 79   Avg_Loss: 2.13891 ['MAELoss: 1.98800']\n",
      "Train Epoch: 80   Avg_Loss: 2.12860 ['MAELoss: 1.98239']\n",
      "Train Epoch: 81   Avg_Loss: 2.11855 ['MAELoss: 1.97699']\n",
      "Train Epoch: 82   Avg_Loss: 2.10858 ['MAELoss: 1.97167']\n",
      "Train Epoch: 83   Avg_Loss: 2.09879 ['MAELoss: 1.96644']\n",
      "Train Epoch: 84   Avg_Loss: 2.08903 ['MAELoss: 1.96126']\n",
      "Train Epoch: 85   Avg_Loss: 2.07913 ['MAELoss: 1.95602']\n",
      "Train Epoch: 86   Avg_Loss: 2.06876 ['MAELoss: 1.95067']\n",
      "Train Epoch: 87   Avg_Loss: 2.05945 ['MAELoss: 1.94569']\n",
      "Train Epoch: 88   Avg_Loss: 2.05049 ['MAELoss: 1.94080']\n",
      "Train Epoch: 89   Avg_Loss: 2.04182 ['MAELoss: 1.93615']\n",
      "Train Epoch: 90   Avg_Loss: 2.03319 ['MAELoss: 1.93154']\n",
      "Train Epoch: 91   Avg_Loss: 2.02469 ['MAELoss: 1.92708']\n",
      "Train Epoch: 92   Avg_Loss: 2.01631 ['MAELoss: 1.92255']\n",
      "Train Epoch: 93   Avg_Loss: 2.00811 ['MAELoss: 1.91814']\n",
      "Train Epoch: 94   Avg_Loss: 1.99998 ['MAELoss: 1.91375']\n",
      "Train Epoch: 95   Avg_Loss: 1.99209 ['MAELoss: 1.90952']\n",
      "Train Epoch: 96   Avg_Loss: 1.98454 ['MAELoss: 1.90539']\n",
      "Train Epoch: 97   Avg_Loss: 1.97698 ['MAELoss: 1.90135']\n",
      "Train Epoch: 98   Avg_Loss: 1.96971 ['MAELoss: 1.89748']\n",
      "Train Epoch: 99   Avg_Loss: 1.96254 ['MAELoss: 1.89366']\n",
      "Train Epoch: 100   Avg_Loss: 1.95553 ['MAELoss: 1.89001']\n",
      "Train Epoch: 101   Avg_Loss: 1.94878 ['MAELoss: 1.88638']\n",
      "Train Epoch: 102   Avg_Loss: 1.94198 ['MAELoss: 1.88289']\n",
      "Train Epoch: 103   Avg_Loss: 1.93537 ['MAELoss: 1.87942']\n",
      "Train Epoch: 104   Avg_Loss: 1.92900 ['MAELoss: 1.87612']\n",
      "Train Epoch: 105   Avg_Loss: 1.92265 ['MAELoss: 1.87278']\n",
      "Train Epoch: 106   Avg_Loss: 1.91644 ['MAELoss: 1.86954']\n",
      "Train Epoch: 107   Avg_Loss: 1.91029 ['MAELoss: 1.86629']\n",
      "Train Epoch: 108   Avg_Loss: 1.90430 ['MAELoss: 1.86313']\n",
      "Train Epoch: 109   Avg_Loss: 1.89861 ['MAELoss: 1.86004']\n",
      "Train Epoch: 110   Avg_Loss: 1.89279 ['MAELoss: 1.85693']\n",
      "Train Epoch: 111   Avg_Loss: 1.88720 ['MAELoss: 1.85393']\n",
      "Train Epoch: 112   Avg_Loss: 1.88174 ['MAELoss: 1.85094']\n",
      "Train Epoch: 113   Avg_Loss: 1.87624 ['MAELoss: 1.84788']\n",
      "Train Epoch: 114   Avg_Loss: 1.87087 ['MAELoss: 1.84491']\n",
      "Train Epoch: 115   Avg_Loss: 1.86575 ['MAELoss: 1.84201']\n",
      "Train Epoch: 116   Avg_Loss: 1.86055 ['MAELoss: 1.83922']\n",
      "Train Epoch: 117   Avg_Loss: 1.85546 ['MAELoss: 1.83650']\n",
      "Train Epoch: 118   Avg_Loss: 1.85062 ['MAELoss: 1.83389']\n",
      "Train Epoch: 119   Avg_Loss: 1.84577 ['MAELoss: 1.83129']\n",
      "Train Epoch: 120   Avg_Loss: 1.84086 ['MAELoss: 1.82871']\n",
      "Train Epoch: 121   Avg_Loss: 1.83629 ['MAELoss: 1.82620']\n",
      "Train Epoch: 122   Avg_Loss: 1.83163 ['MAELoss: 1.82371']\n",
      "Train Epoch: 123   Avg_Loss: 1.82720 ['MAELoss: 1.82130']\n",
      "Train Epoch: 124   Avg_Loss: 1.82273 ['MAELoss: 1.81895']\n",
      "Train Epoch: 125   Avg_Loss: 1.81852 ['MAELoss: 1.81671']\n",
      "Train Epoch: 126   Avg_Loss: 1.81419 ['MAELoss: 1.81441']\n",
      "Train Epoch: 127   Avg_Loss: 1.81009 ['MAELoss: 1.81221']\n",
      "Train Epoch: 128   Avg_Loss: 1.80600 ['MAELoss: 1.81002']\n",
      "Train Epoch: 129   Avg_Loss: 1.80189 ['MAELoss: 1.80788']\n",
      "Train Epoch: 130   Avg_Loss: 1.79810 ['MAELoss: 1.80579']\n",
      "Train Epoch: 131   Avg_Loss: 1.79412 ['MAELoss: 1.80372']\n",
      "Train Epoch: 132   Avg_Loss: 1.79044 ['MAELoss: 1.80169']\n",
      "Train Epoch: 133   Avg_Loss: 1.78658 ['MAELoss: 1.79965']\n",
      "Train Epoch: 134   Avg_Loss: 1.78305 ['MAELoss: 1.79771']\n",
      "Train Epoch: 135   Avg_Loss: 1.77944 ['MAELoss: 1.79571']\n",
      "Train Epoch: 136   Avg_Loss: 1.77583 ['MAELoss: 1.79373']\n",
      "Train Epoch: 137   Avg_Loss: 1.77245 ['MAELoss: 1.79185']\n",
      "Train Epoch: 138   Avg_Loss: 1.76904 ['MAELoss: 1.78996']\n",
      "Train Epoch: 139   Avg_Loss: 1.76556 ['MAELoss: 1.78801']\n",
      "Train Epoch: 140   Avg_Loss: 1.76240 ['MAELoss: 1.78622']\n",
      "Train Epoch: 141   Avg_Loss: 1.75912 ['MAELoss: 1.78438']\n",
      "Train Epoch: 142   Avg_Loss: 1.75586 ['MAELoss: 1.78256']\n",
      "Train Epoch: 143   Avg_Loss: 1.75282 ['MAELoss: 1.78080']\n",
      "Train Epoch: 144   Avg_Loss: 1.74970 ['MAELoss: 1.77899']\n",
      "Train Epoch: 145   Avg_Loss: 1.74662 ['MAELoss: 1.77721']\n",
      "Train Epoch: 146   Avg_Loss: 1.74365 ['MAELoss: 1.77546']\n",
      "Train Epoch: 147   Avg_Loss: 1.74059 ['MAELoss: 1.77372']\n",
      "Train Epoch: 148   Avg_Loss: 1.73782 ['MAELoss: 1.77211']\n",
      "Train Epoch: 149   Avg_Loss: 1.73484 ['MAELoss: 1.77034']\n",
      "Train Epoch: 150   Avg_Loss: 1.73203 ['MAELoss: 1.76869']\n",
      "Train Epoch: 151   Avg_Loss: 1.72919 ['MAELoss: 1.76706']\n",
      "Train Epoch: 152   Avg_Loss: 1.72650 ['MAELoss: 1.76550']\n",
      "Train Epoch: 153   Avg_Loss: 1.72368 ['MAELoss: 1.76384']\n",
      "Train Epoch: 154   Avg_Loss: 1.72109 ['MAELoss: 1.76231']\n",
      "Train Epoch: 155   Avg_Loss: 1.71834 ['MAELoss: 1.76074']\n",
      "Train Epoch: 156   Avg_Loss: 1.71581 ['MAELoss: 1.75923']\n",
      "Train Epoch: 157   Avg_Loss: 1.71317 ['MAELoss: 1.75765']\n",
      "Train Epoch: 158   Avg_Loss: 1.71062 ['MAELoss: 1.75616']\n",
      "Train Epoch: 159   Avg_Loss: 1.70804 ['MAELoss: 1.75458']\n",
      "Train Epoch: 160   Avg_Loss: 1.70559 ['MAELoss: 1.75311']\n",
      "Train Epoch: 161   Avg_Loss: 1.70306 ['MAELoss: 1.75160']\n",
      "Train Epoch: 162   Avg_Loss: 1.70063 ['MAELoss: 1.75016']\n",
      "Train Epoch: 163   Avg_Loss: 1.69818 ['MAELoss: 1.74863']\n",
      "Train Epoch: 164   Avg_Loss: 1.69581 ['MAELoss: 1.74723']\n",
      "Train Epoch: 165   Avg_Loss: 1.69338 ['MAELoss: 1.74575']\n",
      "Train Epoch: 166   Avg_Loss: 1.69101 ['MAELoss: 1.74429']\n",
      "Train Epoch: 167   Avg_Loss: 1.68872 ['MAELoss: 1.74292']\n",
      "Train Epoch: 168   Avg_Loss: 1.68649 ['MAELoss: 1.74149']\n",
      "Train Epoch: 169   Avg_Loss: 1.68414 ['MAELoss: 1.74005']\n",
      "Train Epoch: 170   Avg_Loss: 1.68186 ['MAELoss: 1.73864']\n",
      "Train Epoch: 171   Avg_Loss: 1.67972 ['MAELoss: 1.73730']\n",
      "Train Epoch: 172   Avg_Loss: 1.67753 ['MAELoss: 1.73593']\n",
      "Train Epoch: 173   Avg_Loss: 1.67535 ['MAELoss: 1.73457']\n",
      "Train Epoch: 174   Avg_Loss: 1.67320 ['MAELoss: 1.73320']\n",
      "Train Epoch: 175   Avg_Loss: 1.67107 ['MAELoss: 1.73190']\n",
      "Train Epoch: 176   Avg_Loss: 1.66902 ['MAELoss: 1.73051']\n",
      "Train Epoch: 177   Avg_Loss: 1.66690 ['MAELoss: 1.72920']\n",
      "Train Epoch: 178   Avg_Loss: 1.66482 ['MAELoss: 1.72791']\n",
      "Train Epoch: 179   Avg_Loss: 1.66277 ['MAELoss: 1.72667']\n",
      "Train Epoch: 180   Avg_Loss: 1.66071 ['MAELoss: 1.72530']\n",
      "Train Epoch: 181   Avg_Loss: 1.65879 ['MAELoss: 1.72405']\n",
      "Train Epoch: 182   Avg_Loss: 1.65686 ['MAELoss: 1.72282']\n",
      "Train Epoch: 183   Avg_Loss: 1.65479 ['MAELoss: 1.72147']\n",
      "Train Epoch: 184   Avg_Loss: 1.65280 ['MAELoss: 1.72020']\n",
      "Train Epoch: 185   Avg_Loss: 1.65095 ['MAELoss: 1.71902']\n",
      "Train Epoch: 186   Avg_Loss: 1.64911 ['MAELoss: 1.71780']\n",
      "Train Epoch: 187   Avg_Loss: 1.64704 ['MAELoss: 1.71650']\n",
      "Train Epoch: 188   Avg_Loss: 1.64514 ['MAELoss: 1.71528']\n",
      "Train Epoch: 189   Avg_Loss: 1.64341 ['MAELoss: 1.71412']\n",
      "Train Epoch: 190   Avg_Loss: 1.64155 ['MAELoss: 1.71287']\n",
      "Train Epoch: 191   Avg_Loss: 1.63967 ['MAELoss: 1.71173']\n",
      "Train Epoch: 192   Avg_Loss: 1.63778 ['MAELoss: 1.71050']\n",
      "Train Epoch: 193   Avg_Loss: 1.63623 ['MAELoss: 1.70942']\n",
      "Train Epoch: 194   Avg_Loss: 1.63427 ['MAELoss: 1.70819']\n",
      "Train Epoch: 195   Avg_Loss: 1.63264 ['MAELoss: 1.70712']\n",
      "Train Epoch: 196   Avg_Loss: 1.63081 ['MAELoss: 1.70591']\n",
      "Train Epoch: 197   Avg_Loss: 1.62920 ['MAELoss: 1.70486']\n",
      "Train Epoch: 198   Avg_Loss: 1.62735 ['MAELoss: 1.70365']\n",
      "Train Epoch: 199   Avg_Loss: 1.62572 ['MAELoss: 1.70258']\n",
      "saving model...\n",
      "evaluating model...\n",
      "Config name :  DEEP_SMALL_A1\n",
      "loading data...\n",
      "defining model...\n",
      "training...\n",
      "Train Epoch: 0   Avg_Loss: 6.29010 ['MAELoss: 4.29609']\n",
      "Train Epoch: 1   Avg_Loss: 6.16263 ['MAELoss: 4.15610']\n",
      "Train Epoch: 2   Avg_Loss: 6.03868 ['MAELoss: 4.03101']\n",
      "Train Epoch: 3   Avg_Loss: 5.90731 ['MAELoss: 3.91397']\n",
      "Train Epoch: 4   Avg_Loss: 5.77031 ['MAELoss: 3.80223']\n",
      "Train Epoch: 5   Avg_Loss: 5.63396 ['MAELoss: 3.69532']\n",
      "Train Epoch: 6   Avg_Loss: 5.50561 ['MAELoss: 3.60284']\n",
      "Train Epoch: 7   Avg_Loss: 5.39216 ['MAELoss: 3.53154']\n",
      "Train Epoch: 8   Avg_Loss: 5.29534 ['MAELoss: 3.48054']\n",
      "Train Epoch: 9   Avg_Loss: 5.21048 ['MAELoss: 3.44232']\n",
      "Train Epoch: 10   Avg_Loss: 5.13371 ['MAELoss: 3.41229']\n",
      "Train Epoch: 11   Avg_Loss: 5.06202 ['MAELoss: 3.38745']\n",
      "Train Epoch: 12   Avg_Loss: 4.99252 ['MAELoss: 3.36520']\n",
      "Train Epoch: 13   Avg_Loss: 4.92370 ['MAELoss: 3.34332']\n",
      "Train Epoch: 14   Avg_Loss: 4.85814 ['MAELoss: 3.32321']\n",
      "Train Epoch: 15   Avg_Loss: 4.79476 ['MAELoss: 3.30337']\n",
      "Train Epoch: 16   Avg_Loss: 4.73305 ['MAELoss: 3.28515']\n",
      "Train Epoch: 17   Avg_Loss: 4.67338 ['MAELoss: 3.26668']\n",
      "Train Epoch: 18   Avg_Loss: 4.61617 ['MAELoss: 3.24862']\n",
      "Train Epoch: 19   Avg_Loss: 4.56140 ['MAELoss: 3.23144']\n",
      "Train Epoch: 20   Avg_Loss: 4.50854 ['MAELoss: 3.21353']\n",
      "Train Epoch: 21   Avg_Loss: 4.45788 ['MAELoss: 3.19555']\n",
      "Train Epoch: 22   Avg_Loss: 4.41017 ['MAELoss: 3.17746']\n",
      "Train Epoch: 23   Avg_Loss: 4.36529 ['MAELoss: 3.16053']\n",
      "Train Epoch: 24   Avg_Loss: 4.32294 ['MAELoss: 3.14423']\n",
      "Train Epoch: 25   Avg_Loss: 4.28300 ['MAELoss: 3.12836']\n",
      "Train Epoch: 26   Avg_Loss: 4.24523 ['MAELoss: 3.11217']\n",
      "Train Epoch: 27   Avg_Loss: 4.20915 ['MAELoss: 3.09622']\n",
      "Train Epoch: 28   Avg_Loss: 4.17477 ['MAELoss: 3.08040']\n",
      "Train Epoch: 29   Avg_Loss: 4.14172 ['MAELoss: 3.06498']\n",
      "Train Epoch: 30   Avg_Loss: 4.10990 ['MAELoss: 3.04941']\n",
      "Train Epoch: 31   Avg_Loss: 4.07914 ['MAELoss: 3.03411']\n",
      "Train Epoch: 32   Avg_Loss: 4.04932 ['MAELoss: 3.01891']\n",
      "Train Epoch: 33   Avg_Loss: 4.02046 ['MAELoss: 3.00368']\n",
      "Train Epoch: 34   Avg_Loss: 3.99223 ['MAELoss: 2.98876']\n",
      "Train Epoch: 35   Avg_Loss: 3.96466 ['MAELoss: 2.97390']\n",
      "Train Epoch: 36   Avg_Loss: 3.93780 ['MAELoss: 2.95943']\n",
      "Train Epoch: 37   Avg_Loss: 3.91160 ['MAELoss: 2.94523']\n",
      "Train Epoch: 38   Avg_Loss: 3.88607 ['MAELoss: 2.93137']\n",
      "Train Epoch: 39   Avg_Loss: 3.86110 ['MAELoss: 2.91784']\n",
      "Train Epoch: 40   Avg_Loss: 3.83659 ['MAELoss: 2.90479']\n",
      "Train Epoch: 41   Avg_Loss: 3.81267 ['MAELoss: 2.89197']\n",
      "Train Epoch: 42   Avg_Loss: 3.78930 ['MAELoss: 2.87971']\n",
      "Train Epoch: 43   Avg_Loss: 3.76648 ['MAELoss: 2.86772']\n",
      "Train Epoch: 44   Avg_Loss: 3.74421 ['MAELoss: 2.85602']\n",
      "Train Epoch: 45   Avg_Loss: 3.72239 ['MAELoss: 2.84458']\n",
      "Train Epoch: 46   Avg_Loss: 3.70096 ['MAELoss: 2.83333']\n",
      "Train Epoch: 47   Avg_Loss: 3.67990 ['MAELoss: 2.82228']\n",
      "Train Epoch: 48   Avg_Loss: 3.65914 ['MAELoss: 2.81154']\n",
      "Train Epoch: 49   Avg_Loss: 3.63872 ['MAELoss: 2.80103']\n",
      "Train Epoch: 50   Avg_Loss: 3.61870 ['MAELoss: 2.79079']\n",
      "Train Epoch: 51   Avg_Loss: 3.59904 ['MAELoss: 2.78088']\n",
      "Train Epoch: 52   Avg_Loss: 3.57969 ['MAELoss: 2.77122']\n",
      "Train Epoch: 53   Avg_Loss: 3.56062 ['MAELoss: 2.76182']\n",
      "Train Epoch: 54   Avg_Loss: 3.54178 ['MAELoss: 2.75263']\n",
      "Train Epoch: 55   Avg_Loss: 3.52315 ['MAELoss: 2.74370']\n",
      "Train Epoch: 56   Avg_Loss: 3.50495 ['MAELoss: 2.73502']\n",
      "Train Epoch: 57   Avg_Loss: 3.48711 ['MAELoss: 2.72664']\n",
      "Train Epoch: 58   Avg_Loss: 3.46957 ['MAELoss: 2.71847']\n",
      "Train Epoch: 59   Avg_Loss: 3.45239 ['MAELoss: 2.71055']\n",
      "Train Epoch: 60   Avg_Loss: 3.43548 ['MAELoss: 2.70281']\n",
      "Train Epoch: 61   Avg_Loss: 3.41887 ['MAELoss: 2.69527']\n",
      "Train Epoch: 62   Avg_Loss: 3.40249 ['MAELoss: 2.68786']\n",
      "Train Epoch: 63   Avg_Loss: 3.38637 ['MAELoss: 2.68057']\n",
      "Train Epoch: 64   Avg_Loss: 3.37049 ['MAELoss: 2.67337']\n",
      "Train Epoch: 65   Avg_Loss: 3.35487 ['MAELoss: 2.66625']\n",
      "Train Epoch: 66   Avg_Loss: 3.33947 ['MAELoss: 2.65921']\n",
      "Train Epoch: 67   Avg_Loss: 3.32433 ['MAELoss: 2.65227']\n",
      "Train Epoch: 68   Avg_Loss: 3.30938 ['MAELoss: 2.64539']\n",
      "Train Epoch: 69   Avg_Loss: 3.29468 ['MAELoss: 2.63858']\n",
      "Train Epoch: 70   Avg_Loss: 3.28015 ['MAELoss: 2.63181']\n",
      "Train Epoch: 71   Avg_Loss: 3.26585 ['MAELoss: 2.62513']\n",
      "Train Epoch: 72   Avg_Loss: 3.25173 ['MAELoss: 2.61851']\n",
      "Train Epoch: 73   Avg_Loss: 3.23780 ['MAELoss: 2.61198']\n",
      "Train Epoch: 74   Avg_Loss: 3.22405 ['MAELoss: 2.60553']\n",
      "Train Epoch: 75   Avg_Loss: 3.21050 ['MAELoss: 2.59913']\n",
      "Train Epoch: 76   Avg_Loss: 3.19714 ['MAELoss: 2.59281']\n",
      "Train Epoch: 77   Avg_Loss: 3.18394 ['MAELoss: 2.58656']\n",
      "Train Epoch: 78   Avg_Loss: 3.17090 ['MAELoss: 2.58033']\n",
      "Train Epoch: 79   Avg_Loss: 3.15802 ['MAELoss: 2.57416']\n",
      "Train Epoch: 80   Avg_Loss: 3.14527 ['MAELoss: 2.56805']\n",
      "Train Epoch: 81   Avg_Loss: 3.13271 ['MAELoss: 2.56230']\n",
      "Train Epoch: 82   Avg_Loss: 3.12013 ['MAELoss: 2.55729']\n",
      "Train Epoch: 83   Avg_Loss: 3.10754 ['MAELoss: 2.55158']\n",
      "Train Epoch: 84   Avg_Loss: 3.09502 ['MAELoss: 2.54544']\n",
      "Train Epoch: 85   Avg_Loss: 3.08262 ['MAELoss: 2.53871']\n",
      "Train Epoch: 86   Avg_Loss: 3.07047 ['MAELoss: 2.53202']\n",
      "Train Epoch: 87   Avg_Loss: 3.05841 ['MAELoss: 2.52550']\n",
      "Train Epoch: 88   Avg_Loss: 3.04591 ['MAELoss: 2.51921']\n",
      "Train Epoch: 89   Avg_Loss: 3.03324 ['MAELoss: 2.51299']\n",
      "Train Epoch: 90   Avg_Loss: 3.02104 ['MAELoss: 2.50701']\n",
      "Train Epoch: 91   Avg_Loss: 3.00914 ['MAELoss: 2.50112']\n",
      "Train Epoch: 92   Avg_Loss: 2.99753 ['MAELoss: 2.49539']\n",
      "Train Epoch: 93   Avg_Loss: 2.98613 ['MAELoss: 2.48974']\n",
      "Train Epoch: 94   Avg_Loss: 2.97494 ['MAELoss: 2.48423']\n",
      "Train Epoch: 95   Avg_Loss: 2.96393 ['MAELoss: 2.47880']\n",
      "Train Epoch: 96   Avg_Loss: 2.95309 ['MAELoss: 2.47343']\n",
      "Train Epoch: 97   Avg_Loss: 2.94242 ['MAELoss: 2.46815']\n",
      "Train Epoch: 98   Avg_Loss: 2.93192 ['MAELoss: 2.46299']\n",
      "Train Epoch: 99   Avg_Loss: 2.92156 ['MAELoss: 2.45786']\n",
      "Train Epoch: 100   Avg_Loss: 2.91135 ['MAELoss: 2.45284']\n",
      "Train Epoch: 101   Avg_Loss: 2.90130 ['MAELoss: 2.44789']\n",
      "Train Epoch: 102   Avg_Loss: 2.89137 ['MAELoss: 2.44300']\n",
      "Train Epoch: 103   Avg_Loss: 2.88158 ['MAELoss: 2.43819']\n",
      "Train Epoch: 104   Avg_Loss: 2.87190 ['MAELoss: 2.43343']\n",
      "Train Epoch: 105   Avg_Loss: 2.86239 ['MAELoss: 2.42876']\n",
      "Train Epoch: 106   Avg_Loss: 2.85295 ['MAELoss: 2.42412']\n",
      "Train Epoch: 107   Avg_Loss: 2.84367 ['MAELoss: 2.41959']\n",
      "Train Epoch: 108   Avg_Loss: 2.83447 ['MAELoss: 2.41510']\n",
      "Train Epoch: 109   Avg_Loss: 2.82539 ['MAELoss: 2.41063']\n",
      "Train Epoch: 110   Avg_Loss: 2.81645 ['MAELoss: 2.40625']\n",
      "Train Epoch: 111   Avg_Loss: 2.80763 ['MAELoss: 2.40194']\n",
      "Train Epoch: 112   Avg_Loss: 2.79890 ['MAELoss: 2.39766']\n",
      "Train Epoch: 113   Avg_Loss: 2.79027 ['MAELoss: 2.39347']\n",
      "Train Epoch: 114   Avg_Loss: 2.78172 ['MAELoss: 2.38928']\n",
      "Train Epoch: 115   Avg_Loss: 2.77321 ['MAELoss: 2.38508']\n",
      "Train Epoch: 116   Avg_Loss: 2.76481 ['MAELoss: 2.38097']\n",
      "Train Epoch: 117   Avg_Loss: 2.75655 ['MAELoss: 2.37704']\n",
      "Train Epoch: 118   Avg_Loss: 2.74843 ['MAELoss: 2.37316']\n",
      "Train Epoch: 119   Avg_Loss: 2.74042 ['MAELoss: 2.36929']\n",
      "Train Epoch: 120   Avg_Loss: 2.73251 ['MAELoss: 2.36547']\n",
      "Train Epoch: 121   Avg_Loss: 2.72468 ['MAELoss: 2.36168']\n",
      "Train Epoch: 122   Avg_Loss: 2.71694 ['MAELoss: 2.35796']\n",
      "Train Epoch: 123   Avg_Loss: 2.70926 ['MAELoss: 2.35425']\n",
      "Train Epoch: 124   Avg_Loss: 2.70176 ['MAELoss: 2.35067']\n",
      "Train Epoch: 125   Avg_Loss: 2.69428 ['MAELoss: 2.34706']\n",
      "Train Epoch: 126   Avg_Loss: 2.68693 ['MAELoss: 2.34356']\n",
      "Train Epoch: 127   Avg_Loss: 2.67963 ['MAELoss: 2.34012']\n",
      "Train Epoch: 128   Avg_Loss: 2.67245 ['MAELoss: 2.33673']\n",
      "Train Epoch: 129   Avg_Loss: 2.66533 ['MAELoss: 2.33338']\n",
      "Train Epoch: 130   Avg_Loss: 2.65827 ['MAELoss: 2.33005']\n",
      "Train Epoch: 131   Avg_Loss: 2.65133 ['MAELoss: 2.32675']\n",
      "Train Epoch: 132   Avg_Loss: 2.64443 ['MAELoss: 2.32354']\n",
      "Train Epoch: 133   Avg_Loss: 2.63761 ['MAELoss: 2.32035']\n",
      "Train Epoch: 134   Avg_Loss: 2.63087 ['MAELoss: 2.31726']\n",
      "Train Epoch: 135   Avg_Loss: 2.62414 ['MAELoss: 2.31406']\n",
      "Train Epoch: 136   Avg_Loss: 2.61753 ['MAELoss: 2.31101']\n",
      "Train Epoch: 137   Avg_Loss: 2.61095 ['MAELoss: 2.30796']\n",
      "Train Epoch: 138   Avg_Loss: 2.60442 ['MAELoss: 2.30492']\n",
      "Train Epoch: 139   Avg_Loss: 2.59794 ['MAELoss: 2.30192']\n",
      "Train Epoch: 140   Avg_Loss: 2.59151 ['MAELoss: 2.29897']\n",
      "Train Epoch: 141   Avg_Loss: 2.58508 ['MAELoss: 2.29602']\n",
      "Train Epoch: 142   Avg_Loss: 2.57871 ['MAELoss: 2.29310']\n",
      "Train Epoch: 143   Avg_Loss: 2.57239 ['MAELoss: 2.29017']\n",
      "Train Epoch: 144   Avg_Loss: 2.56610 ['MAELoss: 2.28728']\n",
      "Train Epoch: 145   Avg_Loss: 2.55990 ['MAELoss: 2.28447']\n",
      "Train Epoch: 146   Avg_Loss: 2.55370 ['MAELoss: 2.28163']\n",
      "Train Epoch: 147   Avg_Loss: 2.54758 ['MAELoss: 2.27881']\n",
      "Train Epoch: 148   Avg_Loss: 2.54150 ['MAELoss: 2.27603']\n",
      "Train Epoch: 149   Avg_Loss: 2.53548 ['MAELoss: 2.27329']\n",
      "Train Epoch: 150   Avg_Loss: 2.52948 ['MAELoss: 2.27051']\n",
      "Train Epoch: 151   Avg_Loss: 2.52357 ['MAELoss: 2.26780']\n",
      "Train Epoch: 152   Avg_Loss: 2.51766 ['MAELoss: 2.26502']\n",
      "Train Epoch: 153   Avg_Loss: 2.51183 ['MAELoss: 2.26236']\n",
      "Train Epoch: 154   Avg_Loss: 2.50605 ['MAELoss: 2.25963']\n",
      "Train Epoch: 155   Avg_Loss: 2.50032 ['MAELoss: 2.25695']\n",
      "Train Epoch: 156   Avg_Loss: 2.49464 ['MAELoss: 2.25431']\n",
      "Train Epoch: 157   Avg_Loss: 2.48899 ['MAELoss: 2.25165']\n",
      "Train Epoch: 158   Avg_Loss: 2.48341 ['MAELoss: 2.24900']\n",
      "Train Epoch: 159   Avg_Loss: 2.47786 ['MAELoss: 2.24636']\n",
      "Train Epoch: 160   Avg_Loss: 2.47240 ['MAELoss: 2.24382']\n",
      "Train Epoch: 161   Avg_Loss: 2.46694 ['MAELoss: 2.24123']\n",
      "Train Epoch: 162   Avg_Loss: 2.46154 ['MAELoss: 2.23867']\n",
      "Train Epoch: 163   Avg_Loss: 2.45617 ['MAELoss: 2.23616']\n",
      "Train Epoch: 164   Avg_Loss: 2.45083 ['MAELoss: 2.23358']\n",
      "Train Epoch: 165   Avg_Loss: 2.44555 ['MAELoss: 2.23109']\n",
      "Train Epoch: 166   Avg_Loss: 2.44033 ['MAELoss: 2.22859']\n",
      "Train Epoch: 167   Avg_Loss: 2.43511 ['MAELoss: 2.22606']\n",
      "Train Epoch: 168   Avg_Loss: 2.42995 ['MAELoss: 2.22362']\n",
      "Train Epoch: 169   Avg_Loss: 2.42485 ['MAELoss: 2.22121']\n",
      "Train Epoch: 170   Avg_Loss: 2.41980 ['MAELoss: 2.21887']\n",
      "Train Epoch: 171   Avg_Loss: 2.41478 ['MAELoss: 2.21643']\n",
      "Train Epoch: 172   Avg_Loss: 2.40977 ['MAELoss: 2.21403']\n",
      "Train Epoch: 173   Avg_Loss: 2.40480 ['MAELoss: 2.21162']\n",
      "Train Epoch: 174   Avg_Loss: 2.39993 ['MAELoss: 2.20934']\n",
      "Train Epoch: 175   Avg_Loss: 2.39508 ['MAELoss: 2.20701']\n",
      "Train Epoch: 176   Avg_Loss: 2.39028 ['MAELoss: 2.20470']\n",
      "Train Epoch: 177   Avg_Loss: 2.38551 ['MAELoss: 2.20238']\n",
      "Train Epoch: 178   Avg_Loss: 2.38074 ['MAELoss: 2.20007']\n",
      "Train Epoch: 179   Avg_Loss: 2.37612 ['MAELoss: 2.19794']\n",
      "Train Epoch: 180   Avg_Loss: 2.37147 ['MAELoss: 2.19570']\n",
      "Train Epoch: 181   Avg_Loss: 2.36690 ['MAELoss: 2.19351']\n",
      "Train Epoch: 182   Avg_Loss: 2.36236 ['MAELoss: 2.19132']\n",
      "Train Epoch: 183   Avg_Loss: 2.35790 ['MAELoss: 2.18923']\n",
      "Train Epoch: 184   Avg_Loss: 2.35346 ['MAELoss: 2.18706']\n",
      "Train Epoch: 185   Avg_Loss: 2.34909 ['MAELoss: 2.18497']\n",
      "Train Epoch: 186   Avg_Loss: 2.34476 ['MAELoss: 2.18286']\n",
      "Train Epoch: 187   Avg_Loss: 2.34050 ['MAELoss: 2.18084']\n",
      "Train Epoch: 188   Avg_Loss: 2.33623 ['MAELoss: 2.17875']\n",
      "Train Epoch: 189   Avg_Loss: 2.33204 ['MAELoss: 2.17670']\n",
      "Train Epoch: 190   Avg_Loss: 2.32792 ['MAELoss: 2.17482']\n",
      "Train Epoch: 191   Avg_Loss: 2.32386 ['MAELoss: 2.17285']\n",
      "Train Epoch: 192   Avg_Loss: 2.31980 ['MAELoss: 2.17089']\n",
      "Train Epoch: 193   Avg_Loss: 2.31577 ['MAELoss: 2.16899']\n",
      "Train Epoch: 194   Avg_Loss: 2.31177 ['MAELoss: 2.16703']\n",
      "Train Epoch: 195   Avg_Loss: 2.30785 ['MAELoss: 2.16520']\n",
      "Train Epoch: 196   Avg_Loss: 2.30395 ['MAELoss: 2.16326']\n",
      "Train Epoch: 197   Avg_Loss: 2.30010 ['MAELoss: 2.16143']\n",
      "Train Epoch: 198   Avg_Loss: 2.29631 ['MAELoss: 2.15960']\n",
      "Train Epoch: 199   Avg_Loss: 2.29255 ['MAELoss: 2.15779']\n",
      "saving model...\n",
      "evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Anthony : 0\n",
    "    Anton   : 1\n",
    "\"\"\"\n",
    "\n",
    "partition = 1\n",
    "\n",
    "if partition == 0:  config_names = [\"DEEP_SMALL_A4_DECAY_DROPOUT\", \"DEEP_SMALL_A6_DECAY_DROPOUT\", \"DEEP_SMALL_G2_DECAY\", \"DEEP_SMALL_G4_DECAY\"]\n",
    "else:               config_names = [\"DEEP_SMALL_A2_DECAY\", \"DEEP_SMALL_A4_DECAY\", \"DEEP_SMALL_A6_DECAY\"]\n",
    "\n",
    "simulate(config_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-18T13:41:28.823375300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config name :  DEEP_SMALL_3\n",
      "loading data...\n",
      "defining model...\n",
      "training...\n",
      "Train Epoch: 0   Avg_Loss: 5.86416 ['MAELoss: 3.81465']\n",
      "Train Epoch: 1   Avg_Loss: 5.43878 ['MAELoss: 3.46463']\n",
      "Train Epoch: 2   Avg_Loss: 5.04451 ['MAELoss: 3.29999']\n",
      "Train Epoch: 3   Avg_Loss: 4.70953 ['MAELoss: 3.23064']\n",
      "Train Epoch: 4   Avg_Loss: 4.44560 ['MAELoss: 3.16900']\n",
      "Train Epoch: 5   Avg_Loss: 4.23493 ['MAELoss: 3.09504']\n",
      "Train Epoch: 6   Avg_Loss: 4.07055 ['MAELoss: 3.02052']\n",
      "Train Epoch: 7   Avg_Loss: 3.93675 ['MAELoss: 2.94812']\n",
      "Train Epoch: 8   Avg_Loss: 3.82260 ['MAELoss: 2.88436']\n",
      "Train Epoch: 9   Avg_Loss: 3.72014 ['MAELoss: 2.82495']\n",
      "Train Epoch: 10   Avg_Loss: 3.62594 ['MAELoss: 2.76979']\n",
      "Train Epoch: 11   Avg_Loss: 3.53574 ['MAELoss: 2.71655']\n",
      "Train Epoch: 12   Avg_Loss: 3.44907 ['MAELoss: 2.66550']\n",
      "Train Epoch: 13   Avg_Loss: 3.36603 ['MAELoss: 2.61660']\n",
      "Train Epoch: 14   Avg_Loss: 3.28656 ['MAELoss: 2.56992']\n",
      "Train Epoch: 15   Avg_Loss: 3.21109 ['MAELoss: 2.52562']\n",
      "Train Epoch: 16   Avg_Loss: 3.13896 ['MAELoss: 2.48311']\n",
      "Train Epoch: 17   Avg_Loss: 3.07000 ['MAELoss: 2.44220']\n",
      "Train Epoch: 18   Avg_Loss: 3.00375 ['MAELoss: 2.40217']\n",
      "Train Epoch: 19   Avg_Loss: 2.94028 ['MAELoss: 2.36445']\n",
      "Train Epoch: 20   Avg_Loss: 2.88040 ['MAELoss: 2.32932']\n",
      "Train Epoch: 21   Avg_Loss: 2.82381 ['MAELoss: 2.29675']\n",
      "Train Epoch: 22   Avg_Loss: 2.76999 ['MAELoss: 2.26583']\n",
      "Train Epoch: 23   Avg_Loss: 2.71874 ['MAELoss: 2.23674']\n",
      "Train Epoch: 24   Avg_Loss: 2.66984 ['MAELoss: 2.20904']\n",
      "Train Epoch: 25   Avg_Loss: 2.62315 ['MAELoss: 2.18283']\n",
      "Train Epoch: 26   Avg_Loss: 2.57892 ['MAELoss: 2.15789']\n",
      "Train Epoch: 27   Avg_Loss: 2.53698 ['MAELoss: 2.13436']\n",
      "Train Epoch: 28   Avg_Loss: 2.49713 ['MAELoss: 2.11214']\n",
      "Train Epoch: 29   Avg_Loss: 2.45943 ['MAELoss: 2.09142']\n",
      "Train Epoch: 30   Avg_Loss: 2.42363 ['MAELoss: 2.07182']\n",
      "Train Epoch: 31   Avg_Loss: 2.38953 ['MAELoss: 2.05345']\n",
      "Train Epoch: 32   Avg_Loss: 2.35728 ['MAELoss: 2.03632']\n",
      "Train Epoch: 33   Avg_Loss: 2.32647 ['MAELoss: 2.02024']\n",
      "Train Epoch: 34   Avg_Loss: 2.29694 ['MAELoss: 2.00494']\n",
      "Train Epoch: 35   Avg_Loss: 2.26893 ['MAELoss: 1.99050']\n",
      "Train Epoch: 36   Avg_Loss: 2.24204 ['MAELoss: 1.97673']\n",
      "Train Epoch: 37   Avg_Loss: 2.21626 ['MAELoss: 1.96342']\n",
      "Train Epoch: 38   Avg_Loss: 2.19162 ['MAELoss: 1.95076']\n",
      "Train Epoch: 39   Avg_Loss: 2.16786 ['MAELoss: 1.93842']\n",
      "Train Epoch: 40   Avg_Loss: 2.14526 ['MAELoss: 1.92670']\n",
      "Train Epoch: 41   Avg_Loss: 2.12341 ['MAELoss: 1.91534']\n",
      "Train Epoch: 42   Avg_Loss: 2.10245 ['MAELoss: 1.90448']\n",
      "Train Epoch: 43   Avg_Loss: 2.08245 ['MAELoss: 1.89397']\n",
      "Train Epoch: 44   Avg_Loss: 2.06318 ['MAELoss: 1.88388']\n",
      "Train Epoch: 45   Avg_Loss: 2.04469 ['MAELoss: 1.87408']\n",
      "Train Epoch: 46   Avg_Loss: 2.02685 ['MAELoss: 1.86458']\n",
      "Train Epoch: 47   Avg_Loss: 2.00958 ['MAELoss: 1.85550']\n",
      "Train Epoch: 48   Avg_Loss: 1.99307 ['MAELoss: 1.84671']\n",
      "Train Epoch: 49   Avg_Loss: 1.97707 ['MAELoss: 1.83823']\n",
      "Train Epoch: 50   Avg_Loss: 1.96189 ['MAELoss: 1.83025']\n",
      "Train Epoch: 51   Avg_Loss: 1.94706 ['MAELoss: 1.82247']\n",
      "Train Epoch: 52   Avg_Loss: 1.93304 ['MAELoss: 1.81510']\n",
      "Train Epoch: 53   Avg_Loss: 1.91922 ['MAELoss: 1.80774']\n",
      "Train Epoch: 54   Avg_Loss: 1.90627 ['MAELoss: 1.80095']\n",
      "Train Epoch: 55   Avg_Loss: 1.89349 ['MAELoss: 1.79415']\n",
      "Train Epoch: 56   Avg_Loss: 1.88138 ['MAELoss: 1.78781']\n",
      "Train Epoch: 57   Avg_Loss: 1.86955 ['MAELoss: 1.78156']\n",
      "Train Epoch: 58   Avg_Loss: 1.85824 ['MAELoss: 1.77564']\n",
      "Train Epoch: 59   Avg_Loss: 1.84711 ['MAELoss: 1.76977']\n",
      "Train Epoch: 60   Avg_Loss: 1.83653 ['MAELoss: 1.76427']\n",
      "Train Epoch: 61   Avg_Loss: 1.82619 ['MAELoss: 1.75880']\n",
      "Train Epoch: 62   Avg_Loss: 1.81627 ['MAELoss: 1.75362']\n",
      "Train Epoch: 63   Avg_Loss: 1.80655 ['MAELoss: 1.74843']\n",
      "Train Epoch: 64   Avg_Loss: 1.79728 ['MAELoss: 1.74355']\n",
      "Train Epoch: 65   Avg_Loss: 1.78817 ['MAELoss: 1.73870']\n",
      "Train Epoch: 66   Avg_Loss: 1.77936 ['MAELoss: 1.73405']\n",
      "Train Epoch: 67   Avg_Loss: 1.77071 ['MAELoss: 1.72944']\n",
      "Train Epoch: 68   Avg_Loss: 1.76251 ['MAELoss: 1.72509']\n",
      "Train Epoch: 69   Avg_Loss: 1.75447 ['MAELoss: 1.72077']\n",
      "Train Epoch: 70   Avg_Loss: 1.74672 ['MAELoss: 1.71668']\n",
      "Train Epoch: 71   Avg_Loss: 1.73925 ['MAELoss: 1.71267']\n",
      "Train Epoch: 72   Avg_Loss: 1.73189 ['MAELoss: 1.70868']\n",
      "Train Epoch: 73   Avg_Loss: 1.72493 ['MAELoss: 1.70492']\n",
      "Train Epoch: 74   Avg_Loss: 1.71794 ['MAELoss: 1.70115']\n",
      "Train Epoch: 75   Avg_Loss: 1.71142 ['MAELoss: 1.69752']\n",
      "Train Epoch: 76   Avg_Loss: 1.70487 ['MAELoss: 1.69394']\n",
      "Train Epoch: 77   Avg_Loss: 1.69872 ['MAELoss: 1.69051']\n",
      "Train Epoch: 78   Avg_Loss: 1.69252 ['MAELoss: 1.68706']\n",
      "Train Epoch: 79   Avg_Loss: 1.68663 ['MAELoss: 1.68375']\n",
      "Train Epoch: 80   Avg_Loss: 1.68073 ['MAELoss: 1.68042']\n",
      "Train Epoch: 81   Avg_Loss: 1.67524 ['MAELoss: 1.67714']\n",
      "Train Epoch: 82   Avg_Loss: 1.66962 ['MAELoss: 1.67395']\n",
      "Train Epoch: 83   Avg_Loss: 1.66422 ['MAELoss: 1.67074']\n",
      "Train Epoch: 84   Avg_Loss: 1.65899 ['MAELoss: 1.66767']\n",
      "Train Epoch: 85   Avg_Loss: 1.65384 ['MAELoss: 1.66462']\n",
      "Train Epoch: 86   Avg_Loss: 1.64883 ['MAELoss: 1.66164']\n",
      "Train Epoch: 87   Avg_Loss: 1.64403 ['MAELoss: 1.65870']\n",
      "Train Epoch: 88   Avg_Loss: 1.63917 ['MAELoss: 1.65577']\n",
      "Train Epoch: 89   Avg_Loss: 1.63459 ['MAELoss: 1.65293']\n",
      "Train Epoch: 90   Avg_Loss: 1.62997 ['MAELoss: 1.65012']\n",
      "Train Epoch: 91   Avg_Loss: 1.62559 ['MAELoss: 1.64736']\n",
      "Train Epoch: 92   Avg_Loss: 1.62115 ['MAELoss: 1.64467']\n",
      "Train Epoch: 93   Avg_Loss: 1.61685 ['MAELoss: 1.64190']\n",
      "Train Epoch: 94   Avg_Loss: 1.61260 ['MAELoss: 1.63933']\n",
      "Train Epoch: 95   Avg_Loss: 1.60853 ['MAELoss: 1.63674']\n",
      "Train Epoch: 96   Avg_Loss: 1.60475 ['MAELoss: 1.63434']\n",
      "Train Epoch: 97   Avg_Loss: 1.60060 ['MAELoss: 1.63176']\n",
      "Train Epoch: 98   Avg_Loss: 1.59698 ['MAELoss: 1.62940']\n",
      "Train Epoch: 99   Avg_Loss: 1.59300 ['MAELoss: 1.62688']\n",
      "Train Epoch: 100   Avg_Loss: 1.58951 ['MAELoss: 1.62462']\n",
      "Train Epoch: 101   Avg_Loss: 1.58584 ['MAELoss: 1.62225']\n",
      "Train Epoch: 102   Avg_Loss: 1.58231 ['MAELoss: 1.62000']\n",
      "Train Epoch: 103   Avg_Loss: 1.57869 ['MAELoss: 1.61769']\n",
      "Train Epoch: 104   Avg_Loss: 1.57537 ['MAELoss: 1.61558']\n",
      "Train Epoch: 105   Avg_Loss: 1.57209 ['MAELoss: 1.61343']\n",
      "Train Epoch: 106   Avg_Loss: 1.56867 ['MAELoss: 1.61129']\n",
      "Train Epoch: 107   Avg_Loss: 1.56551 ['MAELoss: 1.60922']\n",
      "Train Epoch: 108   Avg_Loss: 1.56236 ['MAELoss: 1.60721']\n",
      "Train Epoch: 109   Avg_Loss: 1.55916 ['MAELoss: 1.60511']\n",
      "Train Epoch: 110   Avg_Loss: 1.55612 ['MAELoss: 1.60317']\n",
      "Train Epoch: 111   Avg_Loss: 1.55309 ['MAELoss: 1.60120']\n",
      "Train Epoch: 112   Avg_Loss: 1.55012 ['MAELoss: 1.59932']\n",
      "Train Epoch: 113   Avg_Loss: 1.54725 ['MAELoss: 1.59740']\n",
      "Train Epoch: 114   Avg_Loss: 1.54438 ['MAELoss: 1.59557']\n",
      "Train Epoch: 115   Avg_Loss: 1.54155 ['MAELoss: 1.59369']\n",
      "Train Epoch: 116   Avg_Loss: 1.53871 ['MAELoss: 1.59187']\n",
      "Train Epoch: 117   Avg_Loss: 1.53612 ['MAELoss: 1.59013']\n",
      "Train Epoch: 118   Avg_Loss: 1.53330 ['MAELoss: 1.58832']\n",
      "Train Epoch: 119   Avg_Loss: 1.53073 ['MAELoss: 1.58663']\n",
      "Train Epoch: 120   Avg_Loss: 1.52805 ['MAELoss: 1.58488']\n",
      "Train Epoch: 121   Avg_Loss: 1.52553 ['MAELoss: 1.58320']\n",
      "Train Epoch: 122   Avg_Loss: 1.52301 ['MAELoss: 1.58154']\n",
      "Train Epoch: 123   Avg_Loss: 1.52052 ['MAELoss: 1.57991']\n",
      "Train Epoch: 124   Avg_Loss: 1.51796 ['MAELoss: 1.57823']\n",
      "Train Epoch: 125   Avg_Loss: 1.51566 ['MAELoss: 1.57665']\n",
      "Train Epoch: 126   Avg_Loss: 1.51304 ['MAELoss: 1.57493']\n",
      "Train Epoch: 127   Avg_Loss: 1.51087 ['MAELoss: 1.57347']\n",
      "Train Epoch: 128   Avg_Loss: 1.50836 ['MAELoss: 1.57185']\n",
      "Train Epoch: 129   Avg_Loss: 1.50604 ['MAELoss: 1.57034']\n",
      "Train Epoch: 130   Avg_Loss: 1.50389 ['MAELoss: 1.56885']\n",
      "Train Epoch: 131   Avg_Loss: 1.50148 ['MAELoss: 1.56728']\n",
      "Train Epoch: 132   Avg_Loss: 1.49920 ['MAELoss: 1.56581']\n",
      "Train Epoch: 133   Avg_Loss: 1.49715 ['MAELoss: 1.56431']\n",
      "Train Epoch: 134   Avg_Loss: 1.49477 ['MAELoss: 1.56274']\n",
      "Train Epoch: 135   Avg_Loss: 1.49261 ['MAELoss: 1.56134']\n",
      "Train Epoch: 136   Avg_Loss: 1.49045 ['MAELoss: 1.55982']\n",
      "Train Epoch: 137   Avg_Loss: 1.48832 ['MAELoss: 1.55835']\n",
      "Train Epoch: 138   Avg_Loss: 1.48613 ['MAELoss: 1.55687']\n",
      "Train Epoch: 139   Avg_Loss: 1.48404 ['MAELoss: 1.55548']\n",
      "Train Epoch: 140   Avg_Loss: 1.48188 ['MAELoss: 1.55395']\n",
      "Train Epoch: 141   Avg_Loss: 1.47998 ['MAELoss: 1.55265']\n",
      "Train Epoch: 142   Avg_Loss: 1.47777 ['MAELoss: 1.55113']\n",
      "Train Epoch: 143   Avg_Loss: 1.47592 ['MAELoss: 1.54980']\n",
      "Train Epoch: 144   Avg_Loss: 1.47385 ['MAELoss: 1.54838']\n",
      "Train Epoch: 145   Avg_Loss: 1.47194 ['MAELoss: 1.54706']\n",
      "Train Epoch: 146   Avg_Loss: 1.46985 ['MAELoss: 1.54563']\n",
      "Train Epoch: 147   Avg_Loss: 1.46813 ['MAELoss: 1.54444']\n",
      "Train Epoch: 148   Avg_Loss: 1.46604 ['MAELoss: 1.54294']\n",
      "Train Epoch: 149   Avg_Loss: 1.46412 ['MAELoss: 1.54163']\n",
      "Train Epoch: 150   Avg_Loss: 1.46227 ['MAELoss: 1.54029']\n",
      "Train Epoch: 151   Avg_Loss: 1.46035 ['MAELoss: 1.53900']\n",
      "Train Epoch: 152   Avg_Loss: 1.45851 ['MAELoss: 1.53762']\n",
      "Train Epoch: 153   Avg_Loss: 1.45668 ['MAELoss: 1.53632']\n",
      "Train Epoch: 154   Avg_Loss: 1.45476 ['MAELoss: 1.53498']\n",
      "Train Epoch: 155   Avg_Loss: 1.45302 ['MAELoss: 1.53373']\n",
      "Train Epoch: 156   Avg_Loss: 1.45134 ['MAELoss: 1.53246']\n",
      "Train Epoch: 157   Avg_Loss: 1.44944 ['MAELoss: 1.53120']\n",
      "Train Epoch: 158   Avg_Loss: 1.44770 ['MAELoss: 1.52986']\n",
      "Train Epoch: 159   Avg_Loss: 1.44590 ['MAELoss: 1.52861']\n",
      "Train Epoch: 160   Avg_Loss: 1.44421 ['MAELoss: 1.52735']\n",
      "Train Epoch: 161   Avg_Loss: 1.44248 ['MAELoss: 1.52609']\n",
      "Train Epoch: 162   Avg_Loss: 1.44085 ['MAELoss: 1.52484']\n",
      "Train Epoch: 163   Avg_Loss: 1.43904 ['MAELoss: 1.52360']\n",
      "Train Epoch: 164   Avg_Loss: 1.43760 ['MAELoss: 1.52241']\n",
      "Train Epoch: 165   Avg_Loss: 1.43566 ['MAELoss: 1.52117']\n",
      "Train Epoch: 166   Avg_Loss: 1.43411 ['MAELoss: 1.51983']\n",
      "Train Epoch: 167   Avg_Loss: 1.43242 ['MAELoss: 1.51879']\n",
      "Train Epoch: 168   Avg_Loss: 1.43073 ['MAELoss: 1.51738']\n",
      "Train Epoch: 169   Avg_Loss: 1.42932 ['MAELoss: 1.51645']\n",
      "Train Epoch: 170   Avg_Loss: 1.42754 ['MAELoss: 1.51502']\n",
      "Train Epoch: 171   Avg_Loss: 1.42615 ['MAELoss: 1.51412']\n",
      "Train Epoch: 172   Avg_Loss: 1.42441 ['MAELoss: 1.51282']\n",
      "Train Epoch: 173   Avg_Loss: 1.42308 ['MAELoss: 1.51185']\n",
      "Train Epoch: 174   Avg_Loss: 1.42131 ['MAELoss: 1.51049']\n",
      "Train Epoch: 175   Avg_Loss: 1.41994 ['MAELoss: 1.50956']\n",
      "Train Epoch: 176   Avg_Loss: 1.41836 ['MAELoss: 1.50831']\n",
      "Train Epoch: 177   Avg_Loss: 1.41696 ['MAELoss: 1.50743']\n",
      "Train Epoch: 178   Avg_Loss: 1.41529 ['MAELoss: 1.50617']\n",
      "Train Epoch: 179   Avg_Loss: 1.41402 ['MAELoss: 1.50523']\n",
      "Train Epoch: 180   Avg_Loss: 1.41242 ['MAELoss: 1.50416']\n",
      "Train Epoch: 181   Avg_Loss: 1.41099 ['MAELoss: 1.50312']\n",
      "Train Epoch: 182   Avg_Loss: 1.40959 ['MAELoss: 1.50213']\n",
      "Train Epoch: 183   Avg_Loss: 1.40813 ['MAELoss: 1.50116']\n",
      "Train Epoch: 184   Avg_Loss: 1.40683 ['MAELoss: 1.50006']\n",
      "Train Epoch: 185   Avg_Loss: 1.40523 ['MAELoss: 1.49909']\n",
      "Train Epoch: 186   Avg_Loss: 1.40403 ['MAELoss: 1.49810']\n",
      "Train Epoch: 187   Avg_Loss: 1.40275 ['MAELoss: 1.49719']\n",
      "Train Epoch: 188   Avg_Loss: 1.40117 ['MAELoss: 1.49615']\n",
      "Train Epoch: 189   Avg_Loss: 1.39983 ['MAELoss: 1.49520']\n",
      "Train Epoch: 190   Avg_Loss: 1.39849 ['MAELoss: 1.49413']\n",
      "Train Epoch: 191   Avg_Loss: 1.39716 ['MAELoss: 1.49323']\n"
     ]
    }
   ],
   "source": [
    "# simulate a particular set of configurations\n",
    "simulate([\"DEEP_SMALL_3\", \"DEEP_SMALL_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model and plot training curve\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('models/packed_mlp_SMOOTH_G4/losses.json', 'r') as f:\n",
    "    losses = json.load(f)[\"train_losses\"]\n",
    "\n",
    "plt.plot(losses)\n",
    "benchmark = load_dataset()\n",
    "chunk_sizes=benchmark.train_dataset.get_simulations_sizes()\n",
    "no_norm_x=benchmark.train_dataset.get_no_normalization_axis_indices()\n",
    "scalerParams={\"chunk_sizes\":chunk_sizes,\"no_norm_x\":no_norm_x}\n",
    "\n",
    "torch_sim = TorchSimulator(name=\"packed_mlp\",\n",
    "                           model=PackedMLP,\n",
    "                           scaler=StandardScalerIterative,\n",
    "                           scalerParams=scalerParams,\n",
    "                           log_path=None,\n",
    "                           device=\"cuda:0\",\n",
    "                           seed=42,\n",
    "                           bench_config_path=BENCH_CONFIG_PATH,\n",
    "                           bench_config_name=\"Benchmark1\",\n",
    "                           sim_config_path=SIM_CONFIG_PATH,\n",
    "                           sim_config_name=\"SMOOTH_G4\",\n",
    "                          )\n",
    "\n",
    "torch_sim.restore(epoch=99, path=\"./models\")\n",
    "torch_sim.train(benchmark.train_dataset,\n",
    "                epochs=20,\n",
    "                save_path=None,\n",
    "                pin_memory=True, \n",
    "                non_blocking=True, \n",
    "                num_workers=6\n",
    "                )\n",
    "torch_sim.visualize_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T16:57:30.842892Z",
     "start_time": "2023-12-16T16:55:11.660562700Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_sim.train(benchmark.train_dataset, \n",
    "                save_path=None, \n",
    "                epochs=3, \n",
    "                train_batch_size=128000,\n",
    "                pin_memory=True, \n",
    "                non_blocking=True, \n",
    "                num_workers=6\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T18:01:57.080670600Z",
     "start_time": "2023-12-16T16:57:30.837383800Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_sim_metrics = benchmark.evaluate_simulator(augmented_simulator=torch_sim,\n",
    "                                                  eval_batch_size=256000,\n",
    "                                                  dataset=\"all\",\n",
    "                                                  shuffle=False,\n",
    "                                                  save_path=\".\",\n",
    "                                                  save_predictions=True\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T18:01:57.092737400Z",
     "start_time": "2023-12-16T18:01:57.088673100Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_sim_metrics[\"test\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
