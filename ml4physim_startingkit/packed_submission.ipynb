{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packed ensemble submission process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:53.997221600Z",
     "start_time": "2023-12-19T12:25:48.211731900Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lips import get_root_path\n",
    "from lips.dataset.scaler.standard_scaler import StandardScaler\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "from lips.dataset.airfransDataSet import download_data\n",
    "from lips.augmented_simulators.torch_simulator import TorchSimulator\n",
    "from lips.dataset.scaler.standard_scaler_iterative import StandardScalerIterative\n",
    "\n",
    "#from my_custom_packed_ensemble import *\n",
    "#from my_packed_cv import *\n",
    "from my_augmented_simulator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Step (Load the required data) <a id='generic_step'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:54.013022400Z",
     "start_time": "2023-12-19T12:25:53.998103300Z"
    }
   },
   "outputs": [],
   "source": [
    "# indicate required paths\n",
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = '../ml4physim_startingkit/Dataset'\n",
    "BENCHMARK_NAME = \"Case1\"\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the configuration files path, that aim to describe specific caracteristics of the use case or the augmented simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:54.033534200Z",
     "start_time": "2023-12-19T12:25:54.014022300Z"
    }
   },
   "outputs": [],
   "source": [
    "BENCH_CONFIG_PATH = os.path.join(\"airfoilConfigurations\", \"benchmarks\",\n",
    "                                 \"confAirfoil.ini\")  #Configuration file related to the benchmark\n",
    "SIM_CONFIG_PATH = r\"config.ini\"  #Configuration file re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:54.046533700Z",
     "start_time": "2023-12-19T12:25:54.029532800Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(DIRECTORY_NAME):\n",
    "    download_data(root_path=\".\", directory_name=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset using the dedicated class used by LIPS platform offers a list of advantages:\n",
    "\n",
    "1. Ease the importing of datasets\n",
    "1. A set of functions to organize the `inputs` and `outputs` required by augmented simulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:54.065533700Z",
     "start_time": "2023-12-19T12:25:54.045532700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the required benchmark datasets\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load the airfrans dataset as a benchmark object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    benchmark : AirfRANSBenchmark\n",
    "        The airfrans benchmark object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('benchmark.pkl', 'rb') as f:\n",
    "            benchmark = pickle.load(f)\n",
    "    except:\n",
    "        benchmark = AirfRANSBenchmark(benchmark_path=DIRECTORY_NAME,\n",
    "                                    config_path=BENCH_CONFIG_PATH,\n",
    "                                    benchmark_name=BENCHMARK_NAME,\n",
    "                                    log_path=LOG_PATH)\n",
    "        benchmark.load(path=DIRECTORY_NAME)\n",
    "        with open('benchmark.pkl', 'wb') as f:\n",
    "            pickle.dump(benchmark, f)\n",
    "    \n",
    "    return benchmark\n",
    "\n",
    "#benchmark = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:54.075538100Z",
     "start_time": "2023-12-19T12:25:54.064533700Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate(config_names):\n",
    "    \"\"\"\n",
    "    Creates a packed MLP model, trains it and evaluates it on the test dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config_names : list\n",
    "        List of the names of the configurations to be used for training the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        0 if we have successfully trained and evaluated the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    for config_name in config_names:\n",
    "        print(\"Config name : \", config_name)\n",
    "        print(\"loading data...\")\n",
    "        benchmark = load_dataset()\n",
    "\n",
    "        chunk_sizes=benchmark.train_dataset.get_simulations_sizes()\n",
    "        no_norm_x=benchmark.train_dataset.get_no_normalization_axis_indices()\n",
    "        scalerParams={\"chunk_sizes\":chunk_sizes,\"no_norm_x\":no_norm_x}\n",
    "\n",
    "        name = \"packed_mlp\"\n",
    "\n",
    "        print(\"defining model...\")\n",
    "        # PackedMLP model definition \n",
    "        torch_sim = TorchSimulator(name=name,\n",
    "                           model=PackedMLP,\n",
    "                           scaler=StandardScalerIterative,\n",
    "                           scalerParams=scalerParams,\n",
    "                           log_path=None,\n",
    "                           device=\"cuda:0\",\n",
    "                           seed=42,\n",
    "                           bench_config_path=BENCH_CONFIG_PATH,\n",
    "                           bench_config_name=\"Benchmark1\",\n",
    "                           sim_config_path=SIM_CONFIG_PATH,\n",
    "                           sim_config_name=config_name,\n",
    "                          )\n",
    "        \n",
    "        print(\"training...\")\n",
    "        # model training \n",
    "        start = time.perf_counter()\n",
    "        torch_sim.train(benchmark.train_dataset, \n",
    "                save_path=None,\n",
    "                pin_memory=True, \n",
    "                non_blocking=True, \n",
    "                num_workers=6\n",
    "                )\n",
    "        end = time.perf_counter()\n",
    "        train_time = end-start\n",
    "        \n",
    "        print(\"saving model...\")\n",
    "        # saving the model \n",
    "        torch_sim.save(path=\"./models\")\n",
    "\n",
    "        print(\"evaluating model...\")\n",
    "        # evaluating the model \n",
    "        start = time.perf_counter()\n",
    "        torch_sim_metrics = benchmark.evaluate_simulator(augmented_simulator=torch_sim,\n",
    "                                                  eval_batch_size=256000,\n",
    "                                                  dataset=\"all\",\n",
    "                                                  shuffle=False,\n",
    "                                                  save_path=\"./evaluations\",\n",
    "                                                  save_predictions=True\n",
    "                                                 )\n",
    "        end = time.perf_counter()\n",
    "        evaluation_time = end-start\n",
    "        \n",
    "        # save the evaluation time to file\n",
    "        with open(f\"evaluations/{name}_{config_name}/time.txt\", \"a\") as f:\n",
    "            f.write(f\"Training took {train_time:.2f} seconds\\n\")\n",
    "            f.write(f\"Evaluation took {evaluation_time:.2f} seconds\")\n",
    "        \n",
    "        del benchmark\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T06:15:36.977830800Z",
     "start_time": "2023-12-19T00:18:45.964659200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config name :  DEEP_SMALL_A2_DECAY\n",
      "loading data...\n",
      "defining model...\n",
      "training...\n",
      "Train Epoch: 0   Avg_Loss: 6.07788 ['MAELoss: 4.11132']\n",
      "Train Epoch: 1   Avg_Loss: 5.85506 ['MAELoss: 3.86446']\n",
      "Train Epoch: 2   Avg_Loss: 5.62319 ['MAELoss: 3.65643']\n",
      "Train Epoch: 3   Avg_Loss: 5.37931 ['MAELoss: 3.49173']\n",
      "Train Epoch: 4   Avg_Loss: 5.14252 ['MAELoss: 3.39630']\n",
      "Train Epoch: 5   Avg_Loss: 4.92519 ['MAELoss: 3.34471']\n",
      "Train Epoch: 6   Avg_Loss: 4.74278 ['MAELoss: 3.30988']\n",
      "Train Epoch: 7   Avg_Loss: 4.59088 ['MAELoss: 3.27538']\n",
      "Train Epoch: 8   Avg_Loss: 4.46004 ['MAELoss: 3.23514']\n",
      "Train Epoch: 9   Avg_Loss: 4.34338 ['MAELoss: 3.19031']\n",
      "Train Epoch: 10   Avg_Loss: 4.23959 ['MAELoss: 3.14386']\n",
      "Train Epoch: 11   Avg_Loss: 4.14803 ['MAELoss: 3.09807']\n",
      "Train Epoch: 12   Avg_Loss: 4.06598 ['MAELoss: 3.05447']\n",
      "Train Epoch: 13   Avg_Loss: 3.99140 ['MAELoss: 3.01162']\n",
      "Train Epoch: 14   Avg_Loss: 3.92237 ['MAELoss: 2.97031']\n",
      "Train Epoch: 15   Avg_Loss: 3.85785 ['MAELoss: 2.93131']\n",
      "Train Epoch: 16   Avg_Loss: 3.79740 ['MAELoss: 2.89401']\n",
      "Train Epoch: 17   Avg_Loss: 3.74004 ['MAELoss: 2.85869']\n",
      "Train Epoch: 18   Avg_Loss: 3.68540 ['MAELoss: 2.82562']\n",
      "Train Epoch: 19   Avg_Loss: 3.63320 ['MAELoss: 2.79497']\n",
      "Train Epoch: 20   Avg_Loss: 3.58298 ['MAELoss: 2.76589']\n",
      "Train Epoch: 21   Avg_Loss: 3.53425 ['MAELoss: 2.73813']\n",
      "Train Epoch: 22   Avg_Loss: 3.48711 ['MAELoss: 2.71167']\n",
      "Train Epoch: 23   Avg_Loss: 3.44138 ['MAELoss: 2.68644']\n",
      "Train Epoch: 24   Avg_Loss: 3.39700 ['MAELoss: 2.66255']\n",
      "Train Epoch: 25   Avg_Loss: 3.35377 ['MAELoss: 2.63940']\n",
      "Train Epoch: 26   Avg_Loss: 3.31172 ['MAELoss: 2.61694']\n",
      "Train Epoch: 27   Avg_Loss: 3.27089 ['MAELoss: 2.59524']\n",
      "Train Epoch: 28   Avg_Loss: 3.23121 ['MAELoss: 2.57421']\n",
      "Train Epoch: 29   Avg_Loss: 3.19261 ['MAELoss: 2.55369']\n",
      "Train Epoch: 30   Avg_Loss: 3.15508 ['MAELoss: 2.53370']\n",
      "Train Epoch: 31   Avg_Loss: 3.11848 ['MAELoss: 2.51405']\n",
      "Train Epoch: 32   Avg_Loss: 3.08282 ['MAELoss: 2.49495']\n",
      "Train Epoch: 33   Avg_Loss: 3.04801 ['MAELoss: 2.47610']\n",
      "Train Epoch: 34   Avg_Loss: 3.01404 ['MAELoss: 2.45760']\n",
      "Train Epoch: 35   Avg_Loss: 2.98096 ['MAELoss: 2.43947']\n",
      "Train Epoch: 36   Avg_Loss: 2.94878 ['MAELoss: 2.42185']\n",
      "Train Epoch: 37   Avg_Loss: 2.91711 ['MAELoss: 2.40439']\n",
      "Train Epoch: 38   Avg_Loss: 2.88602 ['MAELoss: 2.38741']\n",
      "Train Epoch: 39   Avg_Loss: 2.85590 ['MAELoss: 2.37095']\n",
      "Train Epoch: 40   Avg_Loss: 2.82671 ['MAELoss: 2.35511']\n",
      "Train Epoch: 41   Avg_Loss: 2.79839 ['MAELoss: 2.33962']\n",
      "Train Epoch: 42   Avg_Loss: 2.77083 ['MAELoss: 2.32457']\n",
      "Train Epoch: 43   Avg_Loss: 2.74403 ['MAELoss: 2.30994']\n",
      "Train Epoch: 44   Avg_Loss: 2.71802 ['MAELoss: 2.29571']\n",
      "Train Epoch: 45   Avg_Loss: 2.69272 ['MAELoss: 2.28187']\n",
      "Train Epoch: 46   Avg_Loss: 2.66813 ['MAELoss: 2.26837']\n",
      "Train Epoch: 47   Avg_Loss: 2.64420 ['MAELoss: 2.25520']\n",
      "Train Epoch: 48   Avg_Loss: 2.62092 ['MAELoss: 2.24232']\n",
      "Train Epoch: 49   Avg_Loss: 2.59822 ['MAELoss: 2.22986']\n",
      "Train Epoch: 50   Avg_Loss: 2.57610 ['MAELoss: 2.21764']\n",
      "Train Epoch: 51   Avg_Loss: 2.55455 ['MAELoss: 2.20581']\n",
      "Train Epoch: 52   Avg_Loss: 2.53359 ['MAELoss: 2.19432']\n",
      "Train Epoch: 53   Avg_Loss: 2.51316 ['MAELoss: 2.18305']\n",
      "Train Epoch: 54   Avg_Loss: 2.49326 ['MAELoss: 2.17215']\n",
      "Train Epoch: 55   Avg_Loss: 2.47382 ['MAELoss: 2.16140']\n",
      "Train Epoch: 56   Avg_Loss: 2.45488 ['MAELoss: 2.15095']\n",
      "Train Epoch: 57   Avg_Loss: 2.43642 ['MAELoss: 2.14071']\n",
      "Train Epoch: 58   Avg_Loss: 2.41838 ['MAELoss: 2.13073']\n",
      "Train Epoch: 59   Avg_Loss: 2.40080 ['MAELoss: 2.12093']\n",
      "Train Epoch: 60   Avg_Loss: 2.38370 ['MAELoss: 2.11147']\n",
      "Train Epoch: 61   Avg_Loss: 2.36708 ['MAELoss: 2.10225']\n",
      "Train Epoch: 62   Avg_Loss: 2.35088 ['MAELoss: 2.09333']\n",
      "Train Epoch: 63   Avg_Loss: 2.33506 ['MAELoss: 2.08458']\n",
      "Train Epoch: 64   Avg_Loss: 2.31963 ['MAELoss: 2.07609']\n",
      "Train Epoch: 65   Avg_Loss: 2.30450 ['MAELoss: 2.06778']\n",
      "Train Epoch: 66   Avg_Loss: 2.28978 ['MAELoss: 2.05973']\n",
      "Train Epoch: 67   Avg_Loss: 2.27538 ['MAELoss: 2.05188']\n",
      "Train Epoch: 68   Avg_Loss: 2.26133 ['MAELoss: 2.04428']\n",
      "Train Epoch: 69   Avg_Loss: 2.24757 ['MAELoss: 2.03688']\n",
      "Train Epoch: 70   Avg_Loss: 2.23415 ['MAELoss: 2.02973']\n",
      "Train Epoch: 71   Avg_Loss: 2.22103 ['MAELoss: 2.02278']\n",
      "Train Epoch: 72   Avg_Loss: 2.20822 ['MAELoss: 2.01599']\n",
      "Train Epoch: 73   Avg_Loss: 2.19571 ['MAELoss: 2.00933']\n",
      "Train Epoch: 74   Avg_Loss: 2.18357 ['MAELoss: 2.00289']\n",
      "Train Epoch: 75   Avg_Loss: 2.17168 ['MAELoss: 1.99658']\n",
      "Train Epoch: 76   Avg_Loss: 2.16009 ['MAELoss: 1.99048']\n",
      "Train Epoch: 77   Avg_Loss: 2.14879 ['MAELoss: 1.98455']\n",
      "Train Epoch: 78   Avg_Loss: 2.13775 ['MAELoss: 1.97879']\n",
      "Train Epoch: 79   Avg_Loss: 2.12695 ['MAELoss: 1.97317']\n",
      "Train Epoch: 80   Avg_Loss: 2.11641 ['MAELoss: 1.96768']\n",
      "Train Epoch: 81   Avg_Loss: 2.10613 ['MAELoss: 1.96232']\n",
      "Train Epoch: 82   Avg_Loss: 2.09607 ['MAELoss: 1.95710']\n",
      "Train Epoch: 83   Avg_Loss: 2.08623 ['MAELoss: 1.95204']\n",
      "Train Epoch: 84   Avg_Loss: 2.07670 ['MAELoss: 1.94710']\n",
      "Train Epoch: 85   Avg_Loss: 2.06736 ['MAELoss: 1.94226']\n",
      "Train Epoch: 86   Avg_Loss: 2.05819 ['MAELoss: 1.93755']\n",
      "Train Epoch: 87   Avg_Loss: 2.04917 ['MAELoss: 1.93294']\n",
      "Train Epoch: 88   Avg_Loss: 2.04047 ['MAELoss: 1.92844']\n",
      "Train Epoch: 89   Avg_Loss: 2.03197 ['MAELoss: 1.92405']\n",
      "Train Epoch: 90   Avg_Loss: 2.02361 ['MAELoss: 1.91975']\n",
      "Train Epoch: 91   Avg_Loss: 2.01537 ['MAELoss: 1.91550']\n",
      "Train Epoch: 92   Avg_Loss: 2.00739 ['MAELoss: 1.91144']\n",
      "Train Epoch: 93   Avg_Loss: 1.99959 ['MAELoss: 1.90737']\n",
      "Train Epoch: 94   Avg_Loss: 1.99196 ['MAELoss: 1.90345']\n",
      "Train Epoch: 95   Avg_Loss: 1.98442 ['MAELoss: 1.89952']\n",
      "Train Epoch: 96   Avg_Loss: 1.97701 ['MAELoss: 1.89576']\n",
      "Train Epoch: 97   Avg_Loss: 1.96985 ['MAELoss: 1.89199']\n",
      "Train Epoch: 98   Avg_Loss: 1.96279 ['MAELoss: 1.88836']\n",
      "Train Epoch: 99   Avg_Loss: 1.95585 ['MAELoss: 1.88481']\n",
      "Train Epoch: 100   Avg_Loss: 1.94901 ['MAELoss: 1.88132']\n",
      "Train Epoch: 101   Avg_Loss: 1.94245 ['MAELoss: 1.87790']\n",
      "Train Epoch: 102   Avg_Loss: 1.93594 ['MAELoss: 1.87459']\n",
      "Train Epoch: 103   Avg_Loss: 1.92967 ['MAELoss: 1.87136']\n",
      "Train Epoch: 104   Avg_Loss: 1.92324 ['MAELoss: 1.86810']\n",
      "Train Epoch: 105   Avg_Loss: 1.91722 ['MAELoss: 1.86504']\n",
      "Train Epoch: 106   Avg_Loss: 1.91109 ['MAELoss: 1.86202']\n",
      "Train Epoch: 107   Avg_Loss: 1.90522 ['MAELoss: 1.85898']\n",
      "Train Epoch: 108   Avg_Loss: 1.89930 ['MAELoss: 1.85605']\n",
      "Train Epoch: 109   Avg_Loss: 1.89353 ['MAELoss: 1.85309']\n",
      "Train Epoch: 110   Avg_Loss: 1.88780 ['MAELoss: 1.85016']\n",
      "Train Epoch: 111   Avg_Loss: 1.88223 ['MAELoss: 1.84722']\n",
      "Train Epoch: 112   Avg_Loss: 1.87675 ['MAELoss: 1.84439']\n",
      "Train Epoch: 113   Avg_Loss: 1.87151 ['MAELoss: 1.84160']\n",
      "Train Epoch: 114   Avg_Loss: 1.86637 ['MAELoss: 1.83894']\n",
      "Train Epoch: 115   Avg_Loss: 1.86126 ['MAELoss: 1.83625']\n",
      "Train Epoch: 116   Avg_Loss: 1.85648 ['MAELoss: 1.83376']\n",
      "Train Epoch: 117   Avg_Loss: 1.85153 ['MAELoss: 1.83121']\n",
      "Train Epoch: 118   Avg_Loss: 1.84685 ['MAELoss: 1.82882']\n",
      "Train Epoch: 119   Avg_Loss: 1.84218 ['MAELoss: 1.82631']\n",
      "Train Epoch: 120   Avg_Loss: 1.83769 ['MAELoss: 1.82400']\n",
      "Train Epoch: 121   Avg_Loss: 1.83311 ['MAELoss: 1.82157']\n",
      "Train Epoch: 122   Avg_Loss: 1.82889 ['MAELoss: 1.81936']\n",
      "Train Epoch: 123   Avg_Loss: 1.82447 ['MAELoss: 1.81703']\n",
      "Train Epoch: 124   Avg_Loss: 1.82034 ['MAELoss: 1.81487']\n",
      "Train Epoch: 125   Avg_Loss: 1.81618 ['MAELoss: 1.81266']\n",
      "Train Epoch: 126   Avg_Loss: 1.81216 ['MAELoss: 1.81052']\n",
      "Train Epoch: 127   Avg_Loss: 1.80811 ['MAELoss: 1.80844']\n",
      "Train Epoch: 128   Avg_Loss: 1.80429 ['MAELoss: 1.80637']\n",
      "Train Epoch: 129   Avg_Loss: 1.80038 ['MAELoss: 1.80434']\n",
      "Train Epoch: 130   Avg_Loss: 1.79660 ['MAELoss: 1.80232']\n",
      "Train Epoch: 131   Avg_Loss: 1.79291 ['MAELoss: 1.80038']\n",
      "Train Epoch: 132   Avg_Loss: 1.78927 ['MAELoss: 1.79840']\n",
      "Train Epoch: 133   Avg_Loss: 1.78562 ['MAELoss: 1.79644']\n",
      "Train Epoch: 134   Avg_Loss: 1.78213 ['MAELoss: 1.79450']\n",
      "Train Epoch: 135   Avg_Loss: 1.77857 ['MAELoss: 1.79255']\n",
      "Train Epoch: 136   Avg_Loss: 1.77520 ['MAELoss: 1.79076']\n",
      "Train Epoch: 137   Avg_Loss: 1.77179 ['MAELoss: 1.78892']\n",
      "Train Epoch: 138   Avg_Loss: 1.76851 ['MAELoss: 1.78710']\n",
      "Train Epoch: 139   Avg_Loss: 1.76516 ['MAELoss: 1.78518']\n",
      "Train Epoch: 140   Avg_Loss: 1.76198 ['MAELoss: 1.78349']\n",
      "Train Epoch: 141   Avg_Loss: 1.75869 ['MAELoss: 1.78163']\n",
      "Train Epoch: 142   Avg_Loss: 1.75564 ['MAELoss: 1.77987']\n",
      "Train Epoch: 143   Avg_Loss: 1.75240 ['MAELoss: 1.77803']\n",
      "Train Epoch: 144   Avg_Loss: 1.74935 ['MAELoss: 1.77625']\n",
      "Train Epoch: 145   Avg_Loss: 1.74635 ['MAELoss: 1.77451']\n",
      "Train Epoch: 146   Avg_Loss: 1.74330 ['MAELoss: 1.77285']\n",
      "Train Epoch: 147   Avg_Loss: 1.74042 ['MAELoss: 1.77121']\n",
      "Train Epoch: 148   Avg_Loss: 1.73753 ['MAELoss: 1.76964']\n",
      "Train Epoch: 149   Avg_Loss: 1.73458 ['MAELoss: 1.76803']\n",
      "Train Epoch: 150   Avg_Loss: 1.73192 ['MAELoss: 1.76654']\n",
      "Train Epoch: 151   Avg_Loss: 1.72903 ['MAELoss: 1.76486']\n",
      "Train Epoch: 152   Avg_Loss: 1.72635 ['MAELoss: 1.76344']\n",
      "Train Epoch: 153   Avg_Loss: 1.72365 ['MAELoss: 1.76181']\n",
      "Train Epoch: 154   Avg_Loss: 1.72106 ['MAELoss: 1.76046']\n",
      "Train Epoch: 155   Avg_Loss: 1.71834 ['MAELoss: 1.75882']\n",
      "Train Epoch: 156   Avg_Loss: 1.71589 ['MAELoss: 1.75755']\n",
      "Train Epoch: 157   Avg_Loss: 1.71326 ['MAELoss: 1.75592']\n",
      "Train Epoch: 158   Avg_Loss: 1.71086 ['MAELoss: 1.75466']\n",
      "Train Epoch: 159   Avg_Loss: 1.70830 ['MAELoss: 1.75314']\n",
      "Train Epoch: 160   Avg_Loss: 1.70597 ['MAELoss: 1.75180']\n",
      "Train Epoch: 161   Avg_Loss: 1.70344 ['MAELoss: 1.75027']\n",
      "Train Epoch: 162   Avg_Loss: 1.70118 ['MAELoss: 1.74900']\n",
      "Train Epoch: 163   Avg_Loss: 1.69878 ['MAELoss: 1.74759']\n",
      "Train Epoch: 164   Avg_Loss: 1.69655 ['MAELoss: 1.74634']\n",
      "Train Epoch: 165   Avg_Loss: 1.69415 ['MAELoss: 1.74487']\n",
      "Train Epoch: 166   Avg_Loss: 1.69199 ['MAELoss: 1.74362']\n",
      "Train Epoch: 167   Avg_Loss: 1.68969 ['MAELoss: 1.74222']\n",
      "Train Epoch: 168   Avg_Loss: 1.68759 ['MAELoss: 1.74100']\n",
      "Train Epoch: 169   Avg_Loss: 1.68535 ['MAELoss: 1.73965']\n",
      "Train Epoch: 170   Avg_Loss: 1.68331 ['MAELoss: 1.73844']\n",
      "Train Epoch: 171   Avg_Loss: 1.68109 ['MAELoss: 1.73702']\n",
      "Train Epoch: 172   Avg_Loss: 1.67909 ['MAELoss: 1.73585']\n",
      "Train Epoch: 173   Avg_Loss: 1.67695 ['MAELoss: 1.73454']\n",
      "Train Epoch: 174   Avg_Loss: 1.67498 ['MAELoss: 1.73335']\n",
      "Train Epoch: 175   Avg_Loss: 1.67290 ['MAELoss: 1.73209']\n",
      "Train Epoch: 176   Avg_Loss: 1.67101 ['MAELoss: 1.73092']\n",
      "Train Epoch: 177   Avg_Loss: 1.66896 ['MAELoss: 1.72966']\n",
      "Train Epoch: 178   Avg_Loss: 1.66708 ['MAELoss: 1.72853']\n",
      "Train Epoch: 179   Avg_Loss: 1.66516 ['MAELoss: 1.72740']\n",
      "Train Epoch: 180   Avg_Loss: 1.66316 ['MAELoss: 1.72615']\n",
      "Train Epoch: 181   Avg_Loss: 1.66126 ['MAELoss: 1.72495']\n",
      "Train Epoch: 182   Avg_Loss: 1.65930 ['MAELoss: 1.72382']\n",
      "Train Epoch: 183   Avg_Loss: 1.65750 ['MAELoss: 1.72271']\n",
      "Train Epoch: 184   Avg_Loss: 1.65566 ['MAELoss: 1.72163']\n",
      "Train Epoch: 185   Avg_Loss: 1.65378 ['MAELoss: 1.72047']\n",
      "Train Epoch: 186   Avg_Loss: 1.65191 ['MAELoss: 1.71927']\n",
      "Train Epoch: 187   Avg_Loss: 1.65013 ['MAELoss: 1.71819']\n",
      "Train Epoch: 188   Avg_Loss: 1.64838 ['MAELoss: 1.71708']\n",
      "Train Epoch: 189   Avg_Loss: 1.64654 ['MAELoss: 1.71610']\n",
      "Train Epoch: 190   Avg_Loss: 1.64479 ['MAELoss: 1.71492']\n",
      "Train Epoch: 191   Avg_Loss: 1.64293 ['MAELoss: 1.71376']\n",
      "Train Epoch: 192   Avg_Loss: 1.64126 ['MAELoss: 1.71265']\n",
      "Train Epoch: 193   Avg_Loss: 1.63948 ['MAELoss: 1.71150']\n",
      "Train Epoch: 194   Avg_Loss: 1.63782 ['MAELoss: 1.71044']\n",
      "Train Epoch: 195   Avg_Loss: 1.63609 ['MAELoss: 1.70928']\n",
      "Train Epoch: 196   Avg_Loss: 1.63437 ['MAELoss: 1.70821']\n",
      "Train Epoch: 197   Avg_Loss: 1.63273 ['MAELoss: 1.70712']\n",
      "Train Epoch: 198   Avg_Loss: 1.63109 ['MAELoss: 1.70601']\n",
      "Train Epoch: 199   Avg_Loss: 1.62942 ['MAELoss: 1.70488']\n",
      "saving model...\n",
      "evaluating model...\n",
      "Config name :  DEEP_SMALL_A4_DECAY\n",
      "loading data...\n",
      "defining model...\n",
      "training...\n",
      "Train Epoch: 0   Avg_Loss: 5.86616 ['MAELoss: 3.81490']\n",
      "Train Epoch: 1   Avg_Loss: 5.44931 ['MAELoss: 3.46688']\n",
      "Train Epoch: 2   Avg_Loss: 5.05934 ['MAELoss: 3.29739']\n",
      "Train Epoch: 3   Avg_Loss: 4.72091 ['MAELoss: 3.23048']\n",
      "Train Epoch: 4   Avg_Loss: 4.45650 ['MAELoss: 3.17420']\n",
      "Train Epoch: 5   Avg_Loss: 4.25023 ['MAELoss: 3.10534']\n",
      "Train Epoch: 6   Avg_Loss: 4.08886 ['MAELoss: 3.03276']\n",
      "Train Epoch: 7   Avg_Loss: 3.95888 ['MAELoss: 2.96226']\n",
      "Train Epoch: 8   Avg_Loss: 3.84765 ['MAELoss: 2.89897']\n",
      "Train Epoch: 9   Avg_Loss: 3.74815 ['MAELoss: 2.84004']\n",
      "Train Epoch: 10   Avg_Loss: 3.65562 ['MAELoss: 2.78465']\n",
      "Train Epoch: 11   Avg_Loss: 3.56740 ['MAELoss: 2.73092']\n",
      "Train Epoch: 12   Avg_Loss: 3.48217 ['MAELoss: 2.67940']\n",
      "Train Epoch: 13   Avg_Loss: 3.39965 ['MAELoss: 2.63084']\n",
      "Train Epoch: 14   Avg_Loss: 3.32000 ['MAELoss: 2.58444']\n",
      "Train Epoch: 15   Avg_Loss: 3.24388 ['MAELoss: 2.53976']\n",
      "Train Epoch: 16   Avg_Loss: 3.17066 ['MAELoss: 2.49619']\n",
      "Train Epoch: 17   Avg_Loss: 3.10053 ['MAELoss: 2.45474']\n",
      "Train Epoch: 18   Avg_Loss: 3.03362 ['MAELoss: 2.41525']\n",
      "Train Epoch: 19   Avg_Loss: 2.96969 ['MAELoss: 2.37816']\n",
      "Train Epoch: 20   Avg_Loss: 2.90882 ['MAELoss: 2.34295']\n",
      "Train Epoch: 21   Avg_Loss: 2.85091 ['MAELoss: 2.30967']\n",
      "Train Epoch: 22   Avg_Loss: 2.79608 ['MAELoss: 2.27823']\n",
      "Train Epoch: 23   Avg_Loss: 2.74427 ['MAELoss: 2.24861']\n",
      "Train Epoch: 24   Avg_Loss: 2.69494 ['MAELoss: 2.22064']\n",
      "Train Epoch: 25   Avg_Loss: 2.64824 ['MAELoss: 2.19429']\n",
      "Train Epoch: 26   Avg_Loss: 2.60389 ['MAELoss: 2.16923']\n",
      "Train Epoch: 27   Avg_Loss: 2.56184 ['MAELoss: 2.14567']\n",
      "Train Epoch: 28   Avg_Loss: 2.52202 ['MAELoss: 2.12337']\n",
      "Train Epoch: 29   Avg_Loss: 2.48420 ['MAELoss: 2.10241']\n",
      "Train Epoch: 30   Avg_Loss: 2.44826 ['MAELoss: 2.08243']\n",
      "Train Epoch: 31   Avg_Loss: 2.41404 ['MAELoss: 2.06389']\n",
      "Train Epoch: 32   Avg_Loss: 2.38133 ['MAELoss: 2.04602']\n",
      "Train Epoch: 33   Avg_Loss: 2.35032 ['MAELoss: 2.02932']\n",
      "Train Epoch: 34   Avg_Loss: 2.32068 ['MAELoss: 2.01335']\n",
      "Train Epoch: 35   Avg_Loss: 2.29237 ['MAELoss: 1.99834']\n",
      "Train Epoch: 36   Avg_Loss: 2.26524 ['MAELoss: 1.98390']\n",
      "Train Epoch: 37   Avg_Loss: 2.23937 ['MAELoss: 1.97043']\n",
      "Train Epoch: 38   Avg_Loss: 2.21448 ['MAELoss: 1.95740']\n",
      "Train Epoch: 39   Avg_Loss: 2.19060 ['MAELoss: 1.94499']\n",
      "Train Epoch: 40   Avg_Loss: 2.16778 ['MAELoss: 1.93298']\n",
      "Train Epoch: 41   Avg_Loss: 2.14579 ['MAELoss: 1.92148']\n",
      "Train Epoch: 42   Avg_Loss: 2.12465 ['MAELoss: 1.91040']\n",
      "Train Epoch: 43   Avg_Loss: 2.10446 ['MAELoss: 1.89982']\n",
      "Train Epoch: 44   Avg_Loss: 2.08494 ['MAELoss: 1.88944']\n",
      "Train Epoch: 45   Avg_Loss: 2.06615 ['MAELoss: 1.87943']\n",
      "Train Epoch: 46   Avg_Loss: 2.04808 ['MAELoss: 1.86974']\n",
      "Train Epoch: 47   Avg_Loss: 2.03054 ['MAELoss: 1.86042']\n",
      "Train Epoch: 48   Avg_Loss: 2.01369 ['MAELoss: 1.85142']\n",
      "Train Epoch: 49   Avg_Loss: 1.99739 ['MAELoss: 1.84272']\n",
      "Train Epoch: 50   Avg_Loss: 1.98167 ['MAELoss: 1.83434']\n",
      "Train Epoch: 51   Avg_Loss: 1.96648 ['MAELoss: 1.82621']\n",
      "Train Epoch: 52   Avg_Loss: 1.95198 ['MAELoss: 1.81861']\n",
      "Train Epoch: 53   Avg_Loss: 1.93784 ['MAELoss: 1.81115']\n",
      "Train Epoch: 54   Avg_Loss: 1.92436 ['MAELoss: 1.80408']\n",
      "Train Epoch: 55   Avg_Loss: 1.91114 ['MAELoss: 1.79703']\n",
      "Train Epoch: 56   Avg_Loss: 1.89855 ['MAELoss: 1.79040']\n",
      "Train Epoch: 57   Avg_Loss: 1.88627 ['MAELoss: 1.78389']\n",
      "Train Epoch: 58   Avg_Loss: 1.87444 ['MAELoss: 1.77762']\n",
      "Train Epoch: 59   Avg_Loss: 1.86301 ['MAELoss: 1.77148']\n",
      "Train Epoch: 60   Avg_Loss: 1.85189 ['MAELoss: 1.76564']\n",
      "Train Epoch: 61   Avg_Loss: 1.84120 ['MAELoss: 1.75990']\n",
      "Train Epoch: 62   Avg_Loss: 1.83078 ['MAELoss: 1.75442']\n",
      "Train Epoch: 63   Avg_Loss: 1.82061 ['MAELoss: 1.74897']\n",
      "Train Epoch: 64   Avg_Loss: 1.81098 ['MAELoss: 1.74389']\n",
      "Train Epoch: 65   Avg_Loss: 1.80149 ['MAELoss: 1.73875']\n",
      "Train Epoch: 66   Avg_Loss: 1.79254 ['MAELoss: 1.73398']\n",
      "Train Epoch: 67   Avg_Loss: 1.78356 ['MAELoss: 1.72905']\n",
      "Train Epoch: 68   Avg_Loss: 1.77512 ['MAELoss: 1.72448']\n",
      "Train Epoch: 69   Avg_Loss: 1.76678 ['MAELoss: 1.71985']\n",
      "Train Epoch: 70   Avg_Loss: 1.75871 ['MAELoss: 1.71540']\n",
      "Train Epoch: 71   Avg_Loss: 1.75078 ['MAELoss: 1.71106']\n",
      "Train Epoch: 72   Avg_Loss: 1.74338 ['MAELoss: 1.70687']\n",
      "Train Epoch: 73   Avg_Loss: 1.73580 ['MAELoss: 1.70272']\n",
      "Train Epoch: 74   Avg_Loss: 1.72867 ['MAELoss: 1.69872']\n",
      "Train Epoch: 75   Avg_Loss: 1.72168 ['MAELoss: 1.69479']\n",
      "Train Epoch: 76   Avg_Loss: 1.71492 ['MAELoss: 1.69101']\n",
      "Train Epoch: 77   Avg_Loss: 1.70841 ['MAELoss: 1.68729']\n",
      "Train Epoch: 78   Avg_Loss: 1.70192 ['MAELoss: 1.68355']\n",
      "Train Epoch: 79   Avg_Loss: 1.69572 ['MAELoss: 1.67997']\n",
      "Train Epoch: 80   Avg_Loss: 1.68951 ['MAELoss: 1.67627']\n",
      "Train Epoch: 81   Avg_Loss: 1.68364 ['MAELoss: 1.67278']\n",
      "Train Epoch: 82   Avg_Loss: 1.67780 ['MAELoss: 1.66931']\n",
      "Train Epoch: 83   Avg_Loss: 1.67231 ['MAELoss: 1.66605']\n",
      "Train Epoch: 84   Avg_Loss: 1.66672 ['MAELoss: 1.66282']\n",
      "Train Epoch: 85   Avg_Loss: 1.66142 ['MAELoss: 1.65969']\n",
      "Train Epoch: 86   Avg_Loss: 1.65625 ['MAELoss: 1.65666']\n",
      "Train Epoch: 87   Avg_Loss: 1.65108 ['MAELoss: 1.65364']\n",
      "Train Epoch: 88   Avg_Loss: 1.64622 ['MAELoss: 1.65071']\n",
      "Train Epoch: 89   Avg_Loss: 1.64135 ['MAELoss: 1.64781']\n",
      "Train Epoch: 90   Avg_Loss: 1.63658 ['MAELoss: 1.64495']\n",
      "Train Epoch: 91   Avg_Loss: 1.63201 ['MAELoss: 1.64221']\n",
      "Train Epoch: 92   Avg_Loss: 1.62754 ['MAELoss: 1.63952']\n",
      "Train Epoch: 93   Avg_Loss: 1.62306 ['MAELoss: 1.63682']\n",
      "Train Epoch: 94   Avg_Loss: 1.61883 ['MAELoss: 1.63422']\n",
      "Train Epoch: 95   Avg_Loss: 1.61456 ['MAELoss: 1.63164']\n",
      "Train Epoch: 96   Avg_Loss: 1.61059 ['MAELoss: 1.62919']\n",
      "Train Epoch: 97   Avg_Loss: 1.60643 ['MAELoss: 1.62664']\n",
      "Train Epoch: 98   Avg_Loss: 1.60265 ['MAELoss: 1.62423']\n",
      "Train Epoch: 99   Avg_Loss: 1.59877 ['MAELoss: 1.62186']\n",
      "Train Epoch: 100   Avg_Loss: 1.59496 ['MAELoss: 1.61947']\n",
      "Train Epoch: 101   Avg_Loss: 1.59135 ['MAELoss: 1.61717']\n",
      "Train Epoch: 102   Avg_Loss: 1.58759 ['MAELoss: 1.61484']\n",
      "Train Epoch: 103   Avg_Loss: 1.58411 ['MAELoss: 1.61265']\n",
      "Train Epoch: 104   Avg_Loss: 1.58069 ['MAELoss: 1.61038']\n",
      "Train Epoch: 105   Avg_Loss: 1.57718 ['MAELoss: 1.60817']\n",
      "Train Epoch: 106   Avg_Loss: 1.57392 ['MAELoss: 1.60598']\n",
      "Train Epoch: 107   Avg_Loss: 1.57061 ['MAELoss: 1.60386']\n",
      "Train Epoch: 108   Avg_Loss: 1.56730 ['MAELoss: 1.60167']\n",
      "Train Epoch: 109   Avg_Loss: 1.56401 ['MAELoss: 1.59957']\n",
      "Train Epoch: 110   Avg_Loss: 1.56106 ['MAELoss: 1.59746']\n",
      "Train Epoch: 111   Avg_Loss: 1.55783 ['MAELoss: 1.59540']\n",
      "Train Epoch: 112   Avg_Loss: 1.55480 ['MAELoss: 1.59335']\n",
      "Train Epoch: 113   Avg_Loss: 1.55166 ['MAELoss: 1.59132']\n",
      "Train Epoch: 114   Avg_Loss: 1.54894 ['MAELoss: 1.58943']\n",
      "Train Epoch: 115   Avg_Loss: 1.54582 ['MAELoss: 1.58741']\n",
      "Train Epoch: 116   Avg_Loss: 1.54308 ['MAELoss: 1.58552']\n",
      "Train Epoch: 117   Avg_Loss: 1.54019 ['MAELoss: 1.58358']\n",
      "Train Epoch: 118   Avg_Loss: 1.53762 ['MAELoss: 1.58175']\n",
      "Train Epoch: 119   Avg_Loss: 1.53475 ['MAELoss: 1.57988']\n",
      "Train Epoch: 120   Avg_Loss: 1.53217 ['MAELoss: 1.57810']\n",
      "Train Epoch: 121   Avg_Loss: 1.52941 ['MAELoss: 1.57627']\n",
      "Train Epoch: 122   Avg_Loss: 1.52715 ['MAELoss: 1.57452']\n",
      "Train Epoch: 123   Avg_Loss: 1.52445 ['MAELoss: 1.57277']\n",
      "Train Epoch: 124   Avg_Loss: 1.52195 ['MAELoss: 1.57103']\n",
      "Train Epoch: 125   Avg_Loss: 1.51942 ['MAELoss: 1.56926']\n",
      "Train Epoch: 126   Avg_Loss: 1.51697 ['MAELoss: 1.56747']\n",
      "Train Epoch: 127   Avg_Loss: 1.51444 ['MAELoss: 1.56579']\n",
      "Train Epoch: 128   Avg_Loss: 1.51217 ['MAELoss: 1.56412']\n",
      "Train Epoch: 129   Avg_Loss: 1.50966 ['MAELoss: 1.56246']\n",
      "Train Epoch: 130   Avg_Loss: 1.50751 ['MAELoss: 1.56087']\n",
      "Train Epoch: 131   Avg_Loss: 1.50504 ['MAELoss: 1.55922']\n",
      "Train Epoch: 132   Avg_Loss: 1.50289 ['MAELoss: 1.55764']\n",
      "Train Epoch: 133   Avg_Loss: 1.50056 ['MAELoss: 1.55607']\n",
      "Train Epoch: 134   Avg_Loss: 1.49850 ['MAELoss: 1.55455']\n",
      "Train Epoch: 135   Avg_Loss: 1.49611 ['MAELoss: 1.55296']\n",
      "Train Epoch: 136   Avg_Loss: 1.49415 ['MAELoss: 1.55143']\n",
      "Train Epoch: 137   Avg_Loss: 1.49185 ['MAELoss: 1.54997']\n",
      "Train Epoch: 138   Avg_Loss: 1.48980 ['MAELoss: 1.54848']\n",
      "Train Epoch: 139   Avg_Loss: 1.48756 ['MAELoss: 1.54696']\n",
      "Train Epoch: 140   Avg_Loss: 1.48560 ['MAELoss: 1.54551']\n",
      "Train Epoch: 141   Avg_Loss: 1.48339 ['MAELoss: 1.54401']\n",
      "Train Epoch: 142   Avg_Loss: 1.48153 ['MAELoss: 1.54263']\n",
      "Train Epoch: 143   Avg_Loss: 1.47932 ['MAELoss: 1.54115']\n",
      "Train Epoch: 144   Avg_Loss: 1.47746 ['MAELoss: 1.53974']\n",
      "Train Epoch: 145   Avg_Loss: 1.47537 ['MAELoss: 1.53834']\n",
      "Train Epoch: 146   Avg_Loss: 1.47354 ['MAELoss: 1.53693']\n",
      "Train Epoch: 147   Avg_Loss: 1.47147 ['MAELoss: 1.53556']\n",
      "Train Epoch: 148   Avg_Loss: 1.46967 ['MAELoss: 1.53421']\n",
      "Train Epoch: 149   Avg_Loss: 1.46764 ['MAELoss: 1.53284']\n",
      "Train Epoch: 150   Avg_Loss: 1.46587 ['MAELoss: 1.53153']\n",
      "Train Epoch: 151   Avg_Loss: 1.46394 ['MAELoss: 1.53017']\n",
      "Train Epoch: 152   Avg_Loss: 1.46219 ['MAELoss: 1.52888']\n",
      "Train Epoch: 153   Avg_Loss: 1.46032 ['MAELoss: 1.52753']\n",
      "Train Epoch: 154   Avg_Loss: 1.45855 ['MAELoss: 1.52625']\n",
      "Train Epoch: 155   Avg_Loss: 1.45678 ['MAELoss: 1.52499']\n",
      "Train Epoch: 156   Avg_Loss: 1.45498 ['MAELoss: 1.52372']\n",
      "Train Epoch: 157   Avg_Loss: 1.45325 ['MAELoss: 1.52239']\n",
      "Train Epoch: 158   Avg_Loss: 1.45151 ['MAELoss: 1.52111']\n",
      "Train Epoch: 159   Avg_Loss: 1.44972 ['MAELoss: 1.51985']\n",
      "Train Epoch: 160   Avg_Loss: 1.44807 ['MAELoss: 1.51865']\n",
      "Train Epoch: 161   Avg_Loss: 1.44631 ['MAELoss: 1.51735']\n",
      "Train Epoch: 162   Avg_Loss: 1.44468 ['MAELoss: 1.51611']\n",
      "Train Epoch: 163   Avg_Loss: 1.44295 ['MAELoss: 1.51491']\n",
      "Train Epoch: 164   Avg_Loss: 1.44139 ['MAELoss: 1.51366']\n",
      "Train Epoch: 165   Avg_Loss: 1.43967 ['MAELoss: 1.51245']\n",
      "Train Epoch: 166   Avg_Loss: 1.43808 ['MAELoss: 1.51121']\n",
      "Train Epoch: 167   Avg_Loss: 1.43648 ['MAELoss: 1.51010']\n",
      "Train Epoch: 168   Avg_Loss: 1.43490 ['MAELoss: 1.50887']\n",
      "Train Epoch: 169   Avg_Loss: 1.43325 ['MAELoss: 1.50771']\n",
      "Train Epoch: 170   Avg_Loss: 1.43175 ['MAELoss: 1.50653']\n",
      "Train Epoch: 171   Avg_Loss: 1.43016 ['MAELoss: 1.50545']\n",
      "Train Epoch: 172   Avg_Loss: 1.42876 ['MAELoss: 1.50426']\n",
      "Train Epoch: 173   Avg_Loss: 1.42710 ['MAELoss: 1.50308']\n",
      "Train Epoch: 174   Avg_Loss: 1.42556 ['MAELoss: 1.50196']\n",
      "Train Epoch: 175   Avg_Loss: 1.42405 ['MAELoss: 1.50084']\n",
      "Train Epoch: 176   Avg_Loss: 1.42257 ['MAELoss: 1.49972']\n",
      "Train Epoch: 177   Avg_Loss: 1.42113 ['MAELoss: 1.49862']\n",
      "Train Epoch: 178   Avg_Loss: 1.41963 ['MAELoss: 1.49751']\n",
      "Train Epoch: 179   Avg_Loss: 1.41816 ['MAELoss: 1.49646']\n",
      "Train Epoch: 180   Avg_Loss: 1.41683 ['MAELoss: 1.49535']\n",
      "Train Epoch: 181   Avg_Loss: 1.41521 ['MAELoss: 1.49429']\n",
      "Train Epoch: 182   Avg_Loss: 1.41393 ['MAELoss: 1.49321']\n",
      "Train Epoch: 183   Avg_Loss: 1.41238 ['MAELoss: 1.49214']\n",
      "Train Epoch: 184   Avg_Loss: 1.41090 ['MAELoss: 1.49103']\n",
      "Train Epoch: 185   Avg_Loss: 1.40962 ['MAELoss: 1.49004']\n",
      "Train Epoch: 186   Avg_Loss: 1.40816 ['MAELoss: 1.48890']\n",
      "Train Epoch: 187   Avg_Loss: 1.40681 ['MAELoss: 1.48794']\n",
      "Train Epoch: 188   Avg_Loss: 1.40547 ['MAELoss: 1.48688']\n",
      "Train Epoch: 189   Avg_Loss: 1.40414 ['MAELoss: 1.48587']\n",
      "Train Epoch: 190   Avg_Loss: 1.40274 ['MAELoss: 1.48477']\n",
      "Train Epoch: 191   Avg_Loss: 1.40144 ['MAELoss: 1.48384']\n",
      "Train Epoch: 192   Avg_Loss: 1.40014 ['MAELoss: 1.48275']\n",
      "Train Epoch: 193   Avg_Loss: 1.39876 ['MAELoss: 1.48179']\n",
      "Train Epoch: 194   Avg_Loss: 1.39744 ['MAELoss: 1.48079']\n",
      "Train Epoch: 195   Avg_Loss: 1.39621 ['MAELoss: 1.47980']\n",
      "Train Epoch: 196   Avg_Loss: 1.39471 ['MAELoss: 1.47873']\n",
      "Train Epoch: 197   Avg_Loss: 1.39352 ['MAELoss: 1.47785']\n",
      "Train Epoch: 198   Avg_Loss: 1.39222 ['MAELoss: 1.47685']\n",
      "Train Epoch: 199   Avg_Loss: 1.39098 ['MAELoss: 1.47591']\n",
      "saving model...\n",
      "evaluating model...\n",
      "Config name :  DEEP_SMALL_A6_DECAY\n",
      "loading data...\n",
      "defining model...\n",
      "training...\n",
      "Train Epoch: 0   Avg_Loss: 5.71919 ['MAELoss: 3.69757']\n",
      "Train Epoch: 1   Avg_Loss: 5.09886 ['MAELoss: 3.33554']\n",
      "Train Epoch: 2   Avg_Loss: 4.59900 ['MAELoss: 3.21313']\n",
      "Train Epoch: 3   Avg_Loss: 4.24961 ['MAELoss: 3.09473']\n",
      "Train Epoch: 4   Avg_Loss: 3.99923 ['MAELoss: 2.96561']\n",
      "Train Epoch: 5   Avg_Loss: 3.80438 ['MAELoss: 2.85156']\n",
      "Train Epoch: 6   Avg_Loss: 3.63967 ['MAELoss: 2.75263']\n",
      "Train Epoch: 7   Avg_Loss: 3.49355 ['MAELoss: 2.66578']\n",
      "Train Epoch: 8   Avg_Loss: 3.36077 ['MAELoss: 2.58785']\n",
      "Train Epoch: 9   Avg_Loss: 3.23889 ['MAELoss: 2.51762']\n",
      "Train Epoch: 10   Avg_Loss: 3.12632 ['MAELoss: 2.45343']\n",
      "Train Epoch: 11   Avg_Loss: 3.02255 ['MAELoss: 2.39452']\n",
      "Train Epoch: 12   Avg_Loss: 2.92731 ['MAELoss: 2.34061']\n",
      "Train Epoch: 13   Avg_Loss: 2.83975 ['MAELoss: 2.29099']\n",
      "Train Epoch: 14   Avg_Loss: 2.75925 ['MAELoss: 2.24534']\n",
      "Train Epoch: 15   Avg_Loss: 2.68476 ['MAELoss: 2.20326']\n",
      "Train Epoch: 16   Avg_Loss: 2.61645 ['MAELoss: 2.16453']\n",
      "Train Epoch: 17   Avg_Loss: 2.55273 ['MAELoss: 2.12863']\n",
      "Train Epoch: 18   Avg_Loss: 2.49388 ['MAELoss: 2.09569']\n",
      "Train Epoch: 19   Avg_Loss: 2.43913 ['MAELoss: 2.06506']\n",
      "Train Epoch: 20   Avg_Loss: 2.38815 ['MAELoss: 2.03670']\n",
      "Train Epoch: 21   Avg_Loss: 2.34037 ['MAELoss: 2.01029']\n",
      "Train Epoch: 22   Avg_Loss: 2.29607 ['MAELoss: 1.98586']\n",
      "Train Epoch: 23   Avg_Loss: 2.25476 ['MAELoss: 1.96328']\n",
      "Train Epoch: 24   Avg_Loss: 2.21623 ['MAELoss: 1.94222']\n",
      "Train Epoch: 25   Avg_Loss: 2.17998 ['MAELoss: 1.92272']\n",
      "Train Epoch: 26   Avg_Loss: 2.14607 ['MAELoss: 1.90433']\n",
      "Train Epoch: 27   Avg_Loss: 2.11442 ['MAELoss: 1.88731']\n",
      "Train Epoch: 28   Avg_Loss: 2.08450 ['MAELoss: 1.87127']\n",
      "Train Epoch: 29   Avg_Loss: 2.05637 ['MAELoss: 1.85645']\n",
      "Train Epoch: 30   Avg_Loss: 2.02966 ['MAELoss: 1.84245']\n",
      "Train Epoch: 31   Avg_Loss: 2.00465 ['MAELoss: 1.82926']\n",
      "Train Epoch: 32   Avg_Loss: 1.98084 ['MAELoss: 1.81651']\n",
      "Train Epoch: 33   Avg_Loss: 1.95846 ['MAELoss: 1.80461']\n",
      "Train Epoch: 34   Avg_Loss: 1.93767 ['MAELoss: 1.79341']\n",
      "Train Epoch: 35   Avg_Loss: 1.91782 ['MAELoss: 1.78287']\n",
      "Train Epoch: 36   Avg_Loss: 1.89899 ['MAELoss: 1.77270']\n",
      "Train Epoch: 37   Avg_Loss: 1.88091 ['MAELoss: 1.76323']\n",
      "Train Epoch: 38   Avg_Loss: 1.86402 ['MAELoss: 1.75408']\n",
      "Train Epoch: 39   Avg_Loss: 1.84774 ['MAELoss: 1.74529']\n",
      "Train Epoch: 40   Avg_Loss: 1.83263 ['MAELoss: 1.73702']\n",
      "Train Epoch: 41   Avg_Loss: 1.81776 ['MAELoss: 1.72884']\n",
      "Train Epoch: 42   Avg_Loss: 1.80391 ['MAELoss: 1.72123']\n",
      "Train Epoch: 43   Avg_Loss: 1.79082 ['MAELoss: 1.71388']\n",
      "Train Epoch: 44   Avg_Loss: 1.77827 ['MAELoss: 1.70684']\n",
      "Train Epoch: 45   Avg_Loss: 1.76623 ['MAELoss: 1.70010']\n",
      "Train Epoch: 46   Avg_Loss: 1.75471 ['MAELoss: 1.69352']\n",
      "Train Epoch: 47   Avg_Loss: 1.74366 ['MAELoss: 1.68729']\n",
      "Train Epoch: 48   Avg_Loss: 1.73310 ['MAELoss: 1.68117']\n",
      "Train Epoch: 49   Avg_Loss: 1.72291 ['MAELoss: 1.67543']\n",
      "Train Epoch: 50   Avg_Loss: 1.71306 ['MAELoss: 1.66967']\n",
      "Train Epoch: 51   Avg_Loss: 1.70408 ['MAELoss: 1.66444']\n",
      "Train Epoch: 52   Avg_Loss: 1.69481 ['MAELoss: 1.65914']\n",
      "Train Epoch: 53   Avg_Loss: 1.68623 ['MAELoss: 1.65441']\n",
      "Train Epoch: 54   Avg_Loss: 1.67792 ['MAELoss: 1.64955']\n",
      "Train Epoch: 55   Avg_Loss: 1.67015 ['MAELoss: 1.64493']\n",
      "Train Epoch: 56   Avg_Loss: 1.66236 ['MAELoss: 1.64059']\n",
      "Train Epoch: 57   Avg_Loss: 1.65481 ['MAELoss: 1.63619']\n",
      "Train Epoch: 58   Avg_Loss: 1.64754 ['MAELoss: 1.63197']\n",
      "Train Epoch: 59   Avg_Loss: 1.64087 ['MAELoss: 1.62784']\n",
      "Train Epoch: 60   Avg_Loss: 1.63407 ['MAELoss: 1.62386']\n",
      "Train Epoch: 61   Avg_Loss: 1.62736 ['MAELoss: 1.61991']\n",
      "Train Epoch: 62   Avg_Loss: 1.62120 ['MAELoss: 1.61612']\n",
      "Train Epoch: 63   Avg_Loss: 1.61516 ['MAELoss: 1.61238']\n",
      "Train Epoch: 64   Avg_Loss: 1.60926 ['MAELoss: 1.60869']\n",
      "Train Epoch: 65   Avg_Loss: 1.60351 ['MAELoss: 1.60521']\n",
      "Train Epoch: 66   Avg_Loss: 1.59804 ['MAELoss: 1.60185']\n",
      "Train Epoch: 67   Avg_Loss: 1.59251 ['MAELoss: 1.59853']\n",
      "Train Epoch: 68   Avg_Loss: 1.58742 ['MAELoss: 1.59529']\n",
      "Train Epoch: 69   Avg_Loss: 1.58229 ['MAELoss: 1.59215']\n",
      "Train Epoch: 70   Avg_Loss: 1.57751 ['MAELoss: 1.58913']\n",
      "Train Epoch: 71   Avg_Loss: 1.57269 ['MAELoss: 1.58601']\n",
      "Train Epoch: 72   Avg_Loss: 1.56798 ['MAELoss: 1.58309']\n",
      "Train Epoch: 73   Avg_Loss: 1.56320 ['MAELoss: 1.58024']\n",
      "Train Epoch: 74   Avg_Loss: 1.55915 ['MAELoss: 1.57741']\n",
      "Train Epoch: 75   Avg_Loss: 1.55478 ['MAELoss: 1.57464']\n",
      "Train Epoch: 76   Avg_Loss: 1.55040 ['MAELoss: 1.57189']\n",
      "Train Epoch: 77   Avg_Loss: 1.54637 ['MAELoss: 1.56930']\n",
      "Train Epoch: 78   Avg_Loss: 1.54237 ['MAELoss: 1.56650']\n",
      "Train Epoch: 79   Avg_Loss: 1.53838 ['MAELoss: 1.56410']\n",
      "Train Epoch: 80   Avg_Loss: 1.53460 ['MAELoss: 1.56150']\n",
      "Train Epoch: 81   Avg_Loss: 1.53092 ['MAELoss: 1.55909']\n",
      "Train Epoch: 82   Avg_Loss: 1.52721 ['MAELoss: 1.55654']\n",
      "Train Epoch: 83   Avg_Loss: 1.52362 ['MAELoss: 1.55416']\n",
      "Train Epoch: 84   Avg_Loss: 1.52015 ['MAELoss: 1.55182']\n",
      "Train Epoch: 85   Avg_Loss: 1.51660 ['MAELoss: 1.54945']\n",
      "Train Epoch: 86   Avg_Loss: 1.51354 ['MAELoss: 1.54717']\n",
      "Train Epoch: 87   Avg_Loss: 1.50991 ['MAELoss: 1.54493']\n",
      "Train Epoch: 88   Avg_Loss: 1.50670 ['MAELoss: 1.54266']\n",
      "Train Epoch: 89   Avg_Loss: 1.50368 ['MAELoss: 1.54060']\n",
      "Train Epoch: 90   Avg_Loss: 1.50045 ['MAELoss: 1.53840']\n",
      "Train Epoch: 91   Avg_Loss: 1.49744 ['MAELoss: 1.53629']\n",
      "Train Epoch: 92   Avg_Loss: 1.49423 ['MAELoss: 1.53417']\n",
      "Train Epoch: 93   Avg_Loss: 1.49136 ['MAELoss: 1.53231']\n",
      "Train Epoch: 94   Avg_Loss: 1.48829 ['MAELoss: 1.53016']\n",
      "Train Epoch: 95   Avg_Loss: 1.48570 ['MAELoss: 1.52822']\n",
      "Train Epoch: 96   Avg_Loss: 1.48259 ['MAELoss: 1.52618']\n",
      "Train Epoch: 97   Avg_Loss: 1.47998 ['MAELoss: 1.52432']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m partition \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:  config_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEEP_SMALL_A4_DECAY_DROPOUT\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEEP_SMALL_A6_DECAY_DROPOUT\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEEP_SMALL_G2_DECAY\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEEP_SMALL_G4_DECAY\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:               config_names \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEEP_SMALL_A2_DECAY\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEEP_SMALL_A4_DECAY\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEEP_SMALL_A6_DECAY\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m---> 11\u001B[0m \u001B[43msimulate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig_names\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 45\u001B[0m, in \u001B[0;36msimulate\u001B[1;34m(config_names)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# model training \u001B[39;00m\n\u001B[0;32m     44\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m---> 45\u001B[0m \u001B[43mtorch_sim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbenchmark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpin_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m6\u001B[39;49m\n\u001B[0;32m     50\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[0;32m     52\u001B[0m train_time \u001B[38;5;241m=\u001B[39m end\u001B[38;5;241m-\u001B[39mstart\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml4science\\lib\\site-packages\\lips\\augmented_simulators\\torch_simulator.py:148\u001B[0m, in \u001B[0;36mTorchSimulator.train\u001B[1;34m(self, train_dataset, val_dataset, save_path, **kwargs)\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;66;03m#losses, elapsed_time = train_model(self.model, data_loaders=data)\u001B[39;00m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;66;03m#pbar = tqdm(range(1, self.params[\"epochs\"]+1))\u001B[39;00m\n\u001B[0;32m    145\u001B[0m \u001B[38;5;66;03m#for epoch in pbar:\u001B[39;00m\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n\u001B[0;32m    147\u001B[0m     \u001B[38;5;66;03m#pbar.set_description(\"Epoch %s\" % str(epoch))\u001B[39;00m\n\u001B[1;32m--> 148\u001B[0m     train_loss_epoch, train_metrics_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_one_epoch(epoch, train_loader, optimizer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    149\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_losses\u001B[38;5;241m.\u001B[39mappend(train_loss_epoch)\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m nm_, arr_ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_metrics\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml4science\\lib\\site-packages\\lips\\augmented_simulators\\torch_simulator.py:196\u001B[0m, in \u001B[0;36mTorchSimulator._train_one_epoch\u001B[1;34m(self, epoch, train_loader, optimizer, **kwargs)\u001B[0m\n\u001B[0;32m    194\u001B[0m         metric_func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mget_loss_func(loss_name\u001B[38;5;241m=\u001B[39mmetric, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    195\u001B[0m         metric_value \u001B[38;5;241m=\u001B[39m metric_func(prediction, target)\n\u001B[1;32m--> 196\u001B[0m         metric_value \u001B[38;5;241m=\u001B[39m \u001B[43mmetric_value\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlen\u001B[39m(target)\n\u001B[0;32m    197\u001B[0m         metric_dict[metric] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m metric_value\n\u001B[0;32m    199\u001B[0m mean_loss \u001B[38;5;241m=\u001B[39m total_loss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_loader\u001B[38;5;241m.\u001B[39mdataset)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Anthony : 0\n",
    "    Anton   : 1\n",
    "\"\"\"\n",
    "\n",
    "partition = 1\n",
    "\n",
    "if partition == 0:  config_names = [\"DEEP_SMALL_A4_DECAY_DROPOUT\", \"DEEP_SMALL_A6_DECAY_DROPOUT\", \"DEEP_SMALL_G2_DECAY\", \"DEEP_SMALL_G4_DECAY\"]\n",
    "else:               config_names = [\"DEEP_SMALL_A2_DECAY\", \"DEEP_SMALL_A4_DECAY\", \"DEEP_SMALL_A6_DECAY\"]\n",
    "\n",
    "simulate(config_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-19T12:26:16.219297600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config name :  DEEP_SMALL_A6_DECAY\n",
      "loading data...\n",
      "defining model...\n",
      "training...\n",
      "Train Epoch: 0   Avg_Loss: 5.71919 ['MAELoss: 3.69757']\n",
      "Train Epoch: 1   Avg_Loss: 5.09886 ['MAELoss: 3.33554']\n",
      "Train Epoch: 2   Avg_Loss: 4.59900 ['MAELoss: 3.21313']\n",
      "Train Epoch: 3   Avg_Loss: 4.24961 ['MAELoss: 3.09473']\n",
      "Train Epoch: 4   Avg_Loss: 3.99923 ['MAELoss: 2.96561']\n",
      "Train Epoch: 5   Avg_Loss: 3.80438 ['MAELoss: 2.85156']\n",
      "Train Epoch: 6   Avg_Loss: 3.63967 ['MAELoss: 2.75263']\n",
      "Train Epoch: 7   Avg_Loss: 3.49355 ['MAELoss: 2.66578']\n",
      "Train Epoch: 8   Avg_Loss: 3.36077 ['MAELoss: 2.58785']\n",
      "Train Epoch: 9   Avg_Loss: 3.23889 ['MAELoss: 2.51762']\n",
      "Train Epoch: 10   Avg_Loss: 3.12632 ['MAELoss: 2.45343']\n",
      "Train Epoch: 11   Avg_Loss: 3.02255 ['MAELoss: 2.39452']\n",
      "Train Epoch: 12   Avg_Loss: 2.92731 ['MAELoss: 2.34061']\n",
      "Train Epoch: 13   Avg_Loss: 2.83975 ['MAELoss: 2.29099']\n",
      "Train Epoch: 14   Avg_Loss: 2.75925 ['MAELoss: 2.24534']\n",
      "Train Epoch: 15   Avg_Loss: 2.68476 ['MAELoss: 2.20326']\n",
      "Train Epoch: 16   Avg_Loss: 2.61645 ['MAELoss: 2.16453']\n",
      "Train Epoch: 17   Avg_Loss: 2.55273 ['MAELoss: 2.12863']\n",
      "Train Epoch: 18   Avg_Loss: 2.49388 ['MAELoss: 2.09569']\n",
      "Train Epoch: 19   Avg_Loss: 2.43913 ['MAELoss: 2.06506']\n",
      "Train Epoch: 20   Avg_Loss: 2.38815 ['MAELoss: 2.03670']\n",
      "Train Epoch: 21   Avg_Loss: 2.34037 ['MAELoss: 2.01029']\n",
      "Train Epoch: 22   Avg_Loss: 2.29607 ['MAELoss: 1.98586']\n",
      "Train Epoch: 23   Avg_Loss: 2.25476 ['MAELoss: 1.96328']\n",
      "Train Epoch: 24   Avg_Loss: 2.21623 ['MAELoss: 1.94222']\n",
      "Train Epoch: 25   Avg_Loss: 2.17998 ['MAELoss: 1.92272']\n",
      "Train Epoch: 26   Avg_Loss: 2.14607 ['MAELoss: 1.90433']\n",
      "Train Epoch: 27   Avg_Loss: 2.11442 ['MAELoss: 1.88731']\n",
      "Train Epoch: 28   Avg_Loss: 2.08450 ['MAELoss: 1.87127']\n",
      "Train Epoch: 29   Avg_Loss: 2.05637 ['MAELoss: 1.85645']\n",
      "Train Epoch: 30   Avg_Loss: 2.02966 ['MAELoss: 1.84245']\n",
      "Train Epoch: 31   Avg_Loss: 2.00465 ['MAELoss: 1.82926']\n"
     ]
    }
   ],
   "source": [
    "# simulate a particular set of configurations\n",
    "simulate([\"DEEP_SMALL_A6_DECAY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model and plot training curve\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('models/packed_mlp_SMOOTH_G4/losses.json', 'r') as f:\n",
    "    losses = json.load(f)[\"train_losses\"]\n",
    "\n",
    "plt.plot(losses)\n",
    "benchmark = load_dataset()\n",
    "chunk_sizes=benchmark.train_dataset.get_simulations_sizes()\n",
    "no_norm_x=benchmark.train_dataset.get_no_normalization_axis_indices()\n",
    "scalerParams={\"chunk_sizes\":chunk_sizes,\"no_norm_x\":no_norm_x}\n",
    "\n",
    "torch_sim = TorchSimulator(name=\"packed_mlp\",\n",
    "                           model=PackedMLP,\n",
    "                           scaler=StandardScalerIterative,\n",
    "                           scalerParams=scalerParams,\n",
    "                           log_path=None,\n",
    "                           device=\"cuda:0\",\n",
    "                           seed=42,\n",
    "                           bench_config_path=BENCH_CONFIG_PATH,\n",
    "                           bench_config_name=\"Benchmark1\",\n",
    "                           sim_config_path=SIM_CONFIG_PATH,\n",
    "                           sim_config_name=\"SMOOTH_G4\",\n",
    "                          )\n",
    "\n",
    "torch_sim.restore(epoch=99, path=\"./models\")\n",
    "torch_sim.train(benchmark.train_dataset,\n",
    "                epochs=20,\n",
    "                save_path=None,\n",
    "                pin_memory=True, \n",
    "                non_blocking=True, \n",
    "                num_workers=6\n",
    "                )\n",
    "torch_sim.visualize_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T16:57:30.842892Z",
     "start_time": "2023-12-16T16:55:11.660562700Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_sim.train(benchmark.train_dataset, \n",
    "                save_path=None, \n",
    "                epochs=3, \n",
    "                train_batch_size=128000,\n",
    "                pin_memory=True, \n",
    "                non_blocking=True, \n",
    "                num_workers=6\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T18:01:57.080670600Z",
     "start_time": "2023-12-16T16:57:30.837383800Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_sim_metrics = benchmark.evaluate_simulator(augmented_simulator=torch_sim,\n",
    "                                                  eval_batch_size=256000,\n",
    "                                                  dataset=\"all\",\n",
    "                                                  shuffle=False,\n",
    "                                                  save_path=\".\",\n",
    "                                                  save_predictions=True\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T18:01:57.092737400Z",
     "start_time": "2023-12-16T18:01:57.088673100Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_sim_metrics[\"test\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
