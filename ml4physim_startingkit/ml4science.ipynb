{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2228,"status":"ok","timestamp":1701505614095,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"0rUcVZoTmI7A","outputId":"25da29da-fbb4-40e5-b2a7-1e0c04edd420"},"outputs":[],"source":["colab=False\n","if colab:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    !source /content/drive/MyDrive/my_colab_env/bin/activate\n","    import sys\n","    import os\n","    sys.path.append(\"/content/drive/MyDrive/my_colab_env/lib/python3.10/site-packages\")\n","    os.chdir(\"/content/drive/MyDrive/ml4science/ml4physim_startingkit\")"]},{"cell_type":"markdown","metadata":{"id":"EsOvkQ_CmG6z"},"source":["# Packed Ensemble Application to the AirfRANS dataset"]},{"cell_type":"markdown","metadata":{"id":"KQWKckDomG65"},"source":["### Generic Step (Load the required data) <a id='generic_step'></a>"]},{"cell_type":"markdown","metadata":{"id":"XrkPT3UcmG66"},"source":["Install the LIPS framework if it is not already done. For more information look at the LIPS framework [Github repository](https://github.com/IRT-SystemX/LIPS)"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2023-11-29T19:12:04.318752700Z","start_time":"2023-11-29T19:12:04.196773800Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701505615102,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"g54uEia8mG67"},"outputs":[],"source":["# !pip install -r requirements.txt\n","# or\n","# !pip install -U ."]},{"cell_type":"markdown","metadata":{"id":"PD9DFGIamG69"},"source":["\n","Install the AirfRANS package"]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"end_time":"2023-11-29T19:12:04.318752700Z","start_time":"2023-11-29T19:12:04.210800600Z"},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1701505621668,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"DUEm2XlhmG69"},"outputs":[],"source":["# !pip install airfrans"]},{"cell_type":"markdown","metadata":{"id":"WDNZhB1tmG6-"},"source":["### Generic Step (Load the required data) <a id='generic_step'></a>"]},{"cell_type":"code","execution_count":4,"metadata":{"ExecuteTime":{"end_time":"2023-11-29T19:12:04.319753800Z","start_time":"2023-11-29T19:12:04.222321Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701505622187,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"J8e2OwYLmG6-"},"outputs":[],"source":["import math\n","import os\n","from lips import get_root_path"]},{"cell_type":"code","execution_count":5,"metadata":{"ExecuteTime":{"end_time":"2023-11-29T19:12:04.319753800Z","start_time":"2023-11-29T19:12:04.231836400Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701505622617,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"23XBnSjnmG6_"},"outputs":[],"source":["# indicate required paths\n","LIPS_PATH = get_root_path()\n","DIRECTORY_NAME = '../ml4physim_startingkit/Dataset'\n","BENCHMARK_NAME = \"Case1\"\n","LOG_PATH = LIPS_PATH + \"lips_logs.log\""]},{"cell_type":"markdown","metadata":{"id":"YzJsnBWRmG6_"},"source":["Define the configuration files path, that aim to describe specific caracteristics of the use case or the augmented simulator."]},{"cell_type":"code","execution_count":6,"metadata":{"ExecuteTime":{"end_time":"2023-11-29T19:12:04.319753800Z","start_time":"2023-11-29T19:12:04.243872100Z"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701505623020,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"fZ9nrutLmG7A"},"outputs":[],"source":["BENCH_CONFIG_PATH = os.path.join(\"airfoilConfigurations\", \"benchmarks\",\n","                                 \"confAirfoil.ini\")  #Configuration file related to the benchmark\n","SIM_CONFIG_PATH = os.path.join(\"airfoilConfigurations\", \"simulators\", \"torch_fc.ini\")  #Configuration file re"]},{"cell_type":"markdown","metadata":{"id":"qovzIDjOmG7A"},"source":["Download the data"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1701505624431,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"JdfkvIIimluX","outputId":"4c52f233-08fd-4085-c636-86c2946d9233"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["not os.path.isdir(DIRECTORY_NAME)"]},{"cell_type":"code","execution_count":8,"metadata":{"ExecuteTime":{"end_time":"2023-11-29T19:12:06.043774500Z","start_time":"2023-11-29T19:12:04.255905100Z"},"executionInfo":{"elapsed":2186,"status":"ok","timestamp":1701505627135,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"TIo_MeNVmG7B"},"outputs":[],"source":["from lips.dataset.airfransDataSet import download_data\n","\n","if not os.path.isdir(DIRECTORY_NAME):\n","    download_data(root_path=\".\", directory_name=DIRECTORY_NAME)"]},{"cell_type":"markdown","metadata":{"id":"4tWfcbpNmG7B"},"source":["Loading the dataset using the dedicated class used by LIPS platform offers a list of advantages:\n","\n","1. Ease the importing of datasets\n","1. A set of functions to organize the `inputs` and `outputs` required by augmented simulators\n"]},{"cell_type":"code","execution_count":9,"metadata":{"ExecuteTime":{"start_time":"2023-11-29T19:12:06.047785600Z"},"executionInfo":{"elapsed":20655,"status":"ok","timestamp":1701505649736,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"cZujz-mpmG7B","is_executing":true},"outputs":[],"source":["# Load the required benchmark datasets\n","from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n","import pickle\n","\n","try:\n","    with open('benchmark.pkl', 'rb') as f:\n","        benchmark = pickle.load(f)\n","except:\n","    benchmark = AirfRANSBenchmark(benchmark_path=DIRECTORY_NAME,\n","                                config_path=BENCH_CONFIG_PATH,\n","                                benchmark_name=BENCHMARK_NAME,\n","                                log_path=LOG_PATH)\n","    benchmark.load(path=DIRECTORY_NAME)\n","    with open('benchmark.pkl', 'wb') as f:\n","        pickle.dump(benchmark, f)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"0zh_rdsXmG7C"},"source":["# Model selection (Cross validation)"]},{"cell_type":"markdown","metadata":{"id":"UmjFPEN9mG7C"},"source":["Importing the necessary dependencies, as well as the `packed_ensemble` methods"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701505651793,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"6SZsz0RdyK8k"},"outputs":[],"source":["if colab:\n","    sys.path.append(os.getcwd())"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2058,"status":"ok","timestamp":1701505651792,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"haMKF-humG7C"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","from tqdm import tqdm\n","import itertools as it"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2333,"status":"ok","timestamp":1701505691343,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"yJwmOMo4oLxW"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\antho\\miniconda3\\envs\\ml4science\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from my_packed_ensemble import *"]},{"cell_type":"code","execution_count":13,"metadata":{"ExecuteTime":{"end_time":"2023-11-29T15:24:54.482164600Z","start_time":"2023-11-29T15:24:54.466167Z"},"executionInfo":{"elapsed":485,"status":"ok","timestamp":1701505697714,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"uk8MmZ6hmG7D"},"outputs":[],"source":["def build_k_indices(num_row, k_fold, seed):\n","    \"\"\"build k indices for k-fold.\n","\n","    Parameters\n","    ----------\n","    num_row : int\n","        Number of rows in the dataset.\n","    k_fold : int\n","        Number of folds\n","    seed : int\n","        Seed for random generator\n","\n","    Returns\n","    -------\n","    k_indices : np.array\n","        Array of indices for each fold\"\"\"\n","    interval = int(num_row / k_fold)\n","    np.random.seed(seed)\n","    indices = np.random.permutation(num_row)\n","    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n","    return np.array(k_indices)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"p6ABwWCxmG7D"},"source":["Create cross validation on hyperparameters of the model"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1701506164093,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"riiN62xNq5tB"},"outputs":[],"source":["#from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","from lips.dataset.scaler.standard_scaler import StandardScaler"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"QVFd98OY2Xm1"},"outputs":[],"source":["def save_training_validation_losses_plot(train_losses_list: list, val_losses_list: list,\n","                                         hyperparam_dict: dict, folder: str, plot_name: str):\n","    \"\"\"\n","    Saves the training and validation losses plot.\n","\n","    Parameters\n","    ----------\n","    train_losses_list : list\n","        List containing the training losses.\n","    val_losses_list : list\n","        List containing the validation losses.\n","    \"\"\"\n","\n","    # create folder if it does not exist\n","    if not os.path.isdir(folder):\n","        os.makedirs(folder)\n","    \n","    # clear previous plot\n","    plt.clf()\n","\n","    plt.plot(train_losses_list, label='Training loss', color='blue')\n","    plt.plot(val_losses_list, label='Validation loss',color='red')\n","\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    \n","    plt.title(f'Losses for hidden_sizes={hyperparam_dict[\"hidden_sizes\"]}, dropout={hyperparam_dict[\"dropout\"]}, M={hyperparam_dict[\"M\"]}, \\n alpha={hyperparam_dict[\"alpha\"]}, gamma={hyperparam_dict[\"gamma\"]}, lr={hyperparam_dict[\"lr\"]}')\n","    plt.legend()\n","    plt.savefig(folder + \"/\" + plot_name)"]},{"cell_type":"code","execution_count":16,"metadata":{"ExecuteTime":{"end_time":"2023-11-29T15:43:26.527546400Z","start_time":"2023-11-29T15:43:26.493488100Z"},"executionInfo":{"elapsed":427,"status":"ok","timestamp":1701508470240,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"8yJvr53SmG7E"},"outputs":[],"source":["def hyperparameters_tuning(param_grid: dict, k_folds: int, num_epochs: int, batch_size: int = 128000,\n","                            shuffle: bool = False, n_workers: int = 0, seed: int=42, scaler: Scaler=None):\n","    \"\"\"\n","    Performs hyperparameter tuning using K-fold cross validation.\n","\n","    Parameters\n","    ----------\n","    param_grid : dict\n","        Dictionary containing the values for each hyperparameter to be tested.\n","    k_folds : int\n","        Number of folds to be used in the cross validation.\n","    num_epochs : int\n","        Number of epochs to be used in the training.\n","    batch_size : int\n","        Batch size to be used in the training.\n","    shuffle : bool\n","        Whether to shuffle the training dataset.\n","    n_workers : int\n","        Number of workers to be used in the training.\n","    seed : int\n","        Random seed to be used in the training.\n","\n","    Returns\n","    -------\n","    results_df : pd.DataFrame\n","        DataFrame containing the results of the hyperparameter tuning.\n","    \"\"\"\n","\n","    # generate all combinations of parameter values\n","    combinations = it.product(*(param_grid[key] for key in param_grid))\n","\n","    # create a new dictionary with keys as hyperparameter names and values as lists of combinations\n","    hyperparameter_dict = {key: [] for key in param_grid}\n","\n","    # fill in the values for each key in the new dictionary\n","    for combo in combinations:\n","        for i, key in enumerate(param_grid):\n","            hyperparameter_dict[key].append(combo[i])\n","\n","    hyperparameters_size = len(hyperparameter_dict[list(hyperparameter_dict.keys())[0]])\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    print(device)\n","    torch.manual_seed(seed)\n","    dataset = benchmark.train_dataset\n","    input_size, output_size = infer_input_output_size(dataset)\n","\n","    extract_x, extract_y = dataset.extract_data()\n","    results_df = pd.DataFrame(columns=[*param_grid.keys(), \"mean_loss\"])\n","\n","    for i in tqdm(range(hyperparameters_size)):\n","        param_dict = {\n","            'hidden_sizes': hyperparameter_dict[\"hidden_sizes\"][i],\n","            'dropout': hyperparameter_dict[\"dropout\"][i],\n","            'M': hyperparameter_dict[\"M\"][i],\n","            'alpha': hyperparameter_dict[\"alpha\"][i],\n","            'gamma': hyperparameter_dict[\"gamma\"][i],\n","            'lr': hyperparameter_dict[\"lr\"][i]\n","        }\n","\n","        print(f'Hyperparameters: {i}/hidden_sizes={hyperparameter_dict[\"hidden_sizes\"][i]}, \\\n","              dropout={hyperparameter_dict[\"dropout\"][i]}, M={hyperparameter_dict[\"M\"][i]}, alpha={hyperparameter_dict[\"alpha\"][i]}, \\\n","              gamma={hyperparameter_dict[\"gamma\"][i]}, lr={hyperparameter_dict[\"lr\"][i]}')\n","\n","        # define the K-fold Cross Validator\n","        k_indices = build_k_indices(extract_y.shape[0], k_folds, seed=seed)\n","        summed_total_loss = 0\n","\n","        # k-fold Cross Validation model evaluation\n","        for fold in range(k_folds):\n","            print(f\"fold: {fold}\")\n","            \n","            # initialize the Packed MLP model\n","            model = PackedMLP(\n","                input_size=input_size,\n","                output_size=output_size,\n","                hidden_sizes=hyperparameter_dict[\"hidden_sizes\"][i],\n","                activation=F.relu,\n","                device=device,\n","                dropout=hyperparameter_dict[\"dropout\"][i],\n","                M=hyperparameter_dict[\"M\"][i],\n","                alpha=hyperparameter_dict[\"alpha\"][i],\n","                gamma=hyperparameter_dict[\"gamma\"][i],\n","                scaler=scaler\n","            )\n","            model.to(device)\n","\n","            val_ids = k_indices[fold]\n","            train_ids = k_indices[~(np.arange(k_indices.shape[0]) == fold)]\n","\n","            train_x = extract_x[train_ids]\n","            train_y = extract_y[train_ids]\n","\n","            train_x = train_x.reshape(train_x.shape[0] * train_x.shape[1], -1)\n","            train_y = train_y.reshape(train_y.shape[0] * train_y.shape[1], -1)\n","\n","            val_x = extract_x[val_ids]\n","            val_y = extract_y[val_ids]\n","\n","            trainloader = model.process_dataset(data=(train_x, train_y), training=True, batch_size=batch_size, shuffle=shuffle, n_workers=n_workers)\n","            validateloader = model.process_dataset(data=(val_x, val_y), training=False, batch_size=batch_size, shuffle=shuffle, n_workers=n_workers)\n","\n","            model, train_losses, val_losses = train(model=model, train_loader=trainloader, val_loader=validateloader, epochs=num_epochs, device=device, lr=hyperparameter_dict[\"lr\"][i], verbose=True)\n","\n","            summed_total_loss += torch.mean(val_losses)\n","\n","            # saving the curve\n","            save_training_validation_losses_plot(train_losses_list=train_losses, val_losses_list=val_losses,\n","                                                 hyperparam_dict=param_dict, folder=\"CV_plots\", plot_name=f'hyperparameters_{i}_fold_{fold}.png')\n","\n","        mean_total_loss = summed_total_loss / k_folds\n","        # print fold results\n","        print(f'FOLD {fold} RESULTS FOR {i}th HYPERPARAMETERS')\n","        print(f'Average validation loss: {mean_total_loss}')\n","        print('--------------------------------')\n","\n","        param_dict.update({'mean_val_loss': mean_total_loss})\n","        results_df.loc[len(results_df)] = param_dict\n","\n","    return results_df"]},{"cell_type":"code","execution_count":17,"metadata":{"ExecuteTime":{"end_time":"2023-11-29T15:39:10.229068Z","start_time":"2023-11-29T15:39:10.214595Z"},"executionInfo":{"elapsed":412,"status":"ok","timestamp":1701507256997,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"ae107VZzmG7E"},"outputs":[],"source":["param_grid = {\n","    'hidden_sizes': [(48, 128, 48), (128, 256, 128), (256, 512, 256)],\n","    'dropout': [True, False],\n","    \"alpha\": [2, 4],\n","    \"gamma\": [1, 2, 4],\n","    \"M\": [4],\n","    'lr': [3e-4,1e-2,1e-3]\n","}"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701505700914,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"HRm2njGrmG7E"},"outputs":[],"source":["param_grid = {\n","    'hidden_sizes': [(48, 128, 48)],\n","    'dropout': [True],\n","    \"alpha\": [2],\n","    \"gamma\": [1],\n","    \"M\": [4],\n","    'lr': [3e-4],\n","}"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1701505701899,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"5g06af2oug-v","outputId":"57fc0852-2843-448b-e18d-fe734928e87b"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":21,"metadata":{"ExecuteTime":{"end_time":"2023-11-29T15:46:02.828363500Z","start_time":"2023-11-29T15:45:32.271700800Z"},"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":111517,"status":"error","timestamp":1701508586332,"user":{"displayName":"Alexi Semiz","userId":"03762052284520431505"},"user_tz":-60},"id":"u_I6yZB6mG7F","outputId":"a4b627fc-77c3-44b2-b6ad-cb027833b802"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Hyperparameters: 0/hidden_sizes=(48, 128, 48),               dropout=True, M=4, alpha=2,               gamma=1, lr=0.0003\n","fold: 0\n"]},{"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0/11 [02:02<?, ?it/s]\n","Epochs:   0%|          | 0/2 [02:02<?, ?it/s]\n","  0%|          | 0/1 [02:05<?, ?it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\antho\\Desktop\\EPFL\\ML4Science\\ml4physim_startingkit\\ml4science.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/antho/Desktop/EPFL/ML4Science/ml4physim_startingkit/ml4science.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results_df \u001b[39m=\u001b[39m hyperparameters_tuning(param_grid, k_folds\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1280000\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, n_workers\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, scaler\u001b[39m=\u001b[39;49mStandardScaler())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/antho/Desktop/EPFL/ML4Science/ml4physim_startingkit/ml4science.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#results_df.to_csv(\"results.csv\", index=False)\u001b[39;00m\n","\u001b[1;32mc:\\Users\\antho\\Desktop\\EPFL\\ML4Science\\ml4physim_startingkit\\ml4science.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/antho/Desktop/EPFL/ML4Science/ml4physim_startingkit/ml4science.ipynb#X42sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m trainloader \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mprocess_dataset(data\u001b[39m=\u001b[39m(train_x, train_y), training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39mshuffle, n_workers\u001b[39m=\u001b[39mn_workers)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/antho/Desktop/EPFL/ML4Science/ml4physim_startingkit/ml4science.ipynb#X42sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m validateloader \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mprocess_dataset(data\u001b[39m=\u001b[39m(val_x, val_y), training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39mshuffle, n_workers\u001b[39m=\u001b[39mn_workers)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/antho/Desktop/EPFL/ML4Science/ml4physim_startingkit/ml4science.ipynb#X42sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m model, train_losses, val_losses \u001b[39m=\u001b[39m train(model\u001b[39m=\u001b[39;49mmodel, train_loader\u001b[39m=\u001b[39;49mtrainloader, val_loader\u001b[39m=\u001b[39;49mvalidateloader, epochs\u001b[39m=\u001b[39;49mnum_epochs, device\u001b[39m=\u001b[39;49mdevice, lr\u001b[39m=\u001b[39;49mhyperparameter_dict[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m][i], verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/antho/Desktop/EPFL/ML4Science/ml4physim_startingkit/ml4science.ipynb#X42sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m summed_total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(val_losses)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/antho/Desktop/EPFL/ML4Science/ml4physim_startingkit/ml4science.ipynb#X42sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39m# saving the curve\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\antho\\Desktop\\EPFL\\ML4Science\\ml4physim_startingkit\\my_packed_ensemble.py:320\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, epochs, lr, device, tolerance, min_rate, verbose)\u001b[0m\n\u001b[0;32m    317\u001b[0m loss \u001b[39m=\u001b[39m loss_function(prediction, target\u001b[39m.\u001b[39mrepeat(model\u001b[39m.\u001b[39mnum_estimators, \u001b[39m1\u001b[39m))\n\u001b[0;32m    319\u001b[0m \u001b[39m# compute the gradient (backward pass of back propagation algorithm)\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    322\u001b[0m \u001b[39m# update the parameters of your model\u001b[39;00m\n\u001b[0;32m    323\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n","File \u001b[1;32mc:\\Users\\antho\\miniconda3\\envs\\ml4science\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n","File \u001b[1;32mc:\\Users\\antho\\miniconda3\\envs\\ml4science\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["results_df = hyperparameters_tuning(param_grid, k_folds=4, num_epochs=2, batch_size=1280000, shuffle=True, n_workers=6, scaler=StandardScaler())\n","#results_df.to_csv(\"results.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CV_RESULTS_FOLDER = \"CV/results\"\n","\n","# create folder if it does not exist\n","if not os.path.isdir(CV_RESULTS_FOLDER):\n","    os.makedirs(CV_RESULTS_FOLDER)\n","\n","results_df.to_csv(CV_RESULTS_FOLDER + \"/results.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"cse1Puv6mG7F"},"source":["# Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-11-28T21:22:24.132223400Z","start_time":"2023-11-28T21:22:23.218956800Z"},"id":"U-hVXoVPmG7F"},"outputs":[],"source":["train_loader = process_dataset(benchmark.train_dataset, training=True, n_workers=6)\n","input_size, output_size = infer_input_output_size(benchmark.train_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-11-28T21:22:26.041921400Z","start_time":"2023-11-28T21:22:25.989340700Z"},"id":"M6TL7FmFmG7F"},"outputs":[],"source":["# device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = PackedMLP(input_size=input_size,\n","                  output_size=output_size,\n","                  hidden_sizes=(50, 100, 50),\n","                  activation=F.relu,\n","                  device=device,\n","                  dropout=True,\n","                  )\n","model.to(device)\n","model.device"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-11-28T21:22:31.445847900Z","start_time":"2023-11-28T21:22:31.411243900Z"},"id":"TOtho6vhmG7F"},"outputs":[],"source":["print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-11-28T21:23:04.571539100Z","start_time":"2023-11-28T21:22:46.174380400Z"},"id":"_-dtcqk8mG7G"},"outputs":[],"source":["model, train_losses, _ = train(model, train_loader, epochs=1, device=device, lr=3e-4)"]},{"cell_type":"markdown","metadata":{"id":"5_ctGjrHmG7G"},"source":["##### prediction on `test_dataset`\n","This dataset has the same distribution as the training set"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-11-28T15:25:11.456699Z","start_time":"2023-11-28T15:25:11.456699Z"},"id":"-fssWxBHmG7G"},"outputs":[],"source":["predictions, observations = predict(model, benchmark._test_dataset, device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"start_time":"2023-11-28T15:25:11.457701800Z"},"id":"sNjmwJbfmG7G"},"outputs":[],"source":["print(\"Prediction dimensions: \", predictions[\"x-velocity\"].shape, predictions[\"y-velocity\"].shape,\n","      predictions[\"pressure\"].shape, predictions[\"turbulent_viscosity\"].shape)\n","print(\"Observation dimensions:\", observations[\"x-velocity\"].shape, observations[\"y-velocity\"].shape,\n","      observations[\"pressure\"].shape, observations[\"turbulent_viscosity\"].shape)\n","print(\"We have good dimensions!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"start_time":"2023-11-28T15:25:11.459698900Z"},"id":"TFRk7TdYmG7G"},"outputs":[],"source":["from lips.evaluation.airfrans_evaluation import AirfRANSEvaluation\n","\n","evaluator = AirfRANSEvaluation(config_path=BENCH_CONFIG_PATH,\n","                               scenario=BENCHMARK_NAME,\n","                               data_path=DIRECTORY_NAME,\n","                               log_path=LOG_PATH)\n","\n","observation_metadata = benchmark._test_dataset.extra_data\n","metrics = evaluator.evaluate(observations=observations,\n","                             predictions=predictions,\n","                             observation_metadata=observation_metadata)\n","print(metrics)"]},{"cell_type":"markdown","metadata":{"id":"w0VVE2XzmG7G"},"source":["##### Prediction on `test_ood_dataset`\n","This dataset has a different distribution in comparison to the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"start_time":"2023-11-28T15:25:11.460698700Z"},"id":"Q5SxkfOUmG7H"},"outputs":[],"source":["predictions, observations = predict(model, benchmark._test_ood_dataset, device=device)\n","evaluator = AirfRANSEvaluation(config_path=BENCH_CONFIG_PATH,\n","                               scenario=BENCHMARK_NAME,\n","                               data_path=DIRECTORY_NAME,\n","                               log_path=LOG_PATH)\n","\n","metrics = evaluator.evaluate(observations=observations,\n","                             predictions=predictions,\n","                             observation_metadata=observation_metadata)\n","print(metrics)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
