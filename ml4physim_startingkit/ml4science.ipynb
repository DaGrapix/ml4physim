{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packed Ensemble Application to the AirfRANS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Step (Load the required data) <a id='generic_step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the LIPS framework if it is not already done. For more information look at the LIPS framework [Github repository](https://github.com/IRT-SystemX/LIPS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:12:04.318752700Z",
     "start_time": "2023-11-29T19:12:04.196773800Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# or \n",
    "# !pip install -U ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the AirfRANS package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:12:04.318752700Z",
     "start_time": "2023-11-29T19:12:04.210800600Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install airfrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Step (Load the required data) <a id='generic_step'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:12:04.319753800Z",
     "start_time": "2023-11-29T19:12:04.222321Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from lips import get_root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:12:04.319753800Z",
     "start_time": "2023-11-29T19:12:04.231836400Z"
    }
   },
   "outputs": [],
   "source": [
    "# indicate required paths\n",
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = '../ml4physim_startingkit/Dataset'\n",
    "BENCHMARK_NAME = \"Case1\"\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the configuration files path, that aim to describe specific caracteristics of the use case or the augmented simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:12:04.319753800Z",
     "start_time": "2023-11-29T19:12:04.243872100Z"
    }
   },
   "outputs": [],
   "source": [
    "BENCH_CONFIG_PATH = os.path.join(\"airfoilConfigurations\", \"benchmarks\",\n",
    "                                 \"confAirfoil.ini\")  #Configuration file related to the benchmark\n",
    "SIM_CONFIG_PATH = os.path.join(\"airfoilConfigurations\", \"simulators\", \"torch_fc.ini\")  #Configuration file re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T19:12:06.043774500Z",
     "start_time": "2023-11-29T19:12:04.255905100Z"
    }
   },
   "outputs": [],
   "source": [
    "from lips.dataset.airfransDataSet import download_data\n",
    "\n",
    "if not os.path.isdir(DIRECTORY_NAME):\n",
    "    download_data(root_path=\".\", directory_name=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset using the dedicated class used by LIPS platform offers a list of advantages:\n",
    "\n",
    "1. Ease the importing of datasets\n",
    "1. A set of functions to organize the `inputs` and `outputs` required by augmented simulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-29T19:12:06.047785600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset (task: scarce, split: train): 100%|██████████| 200/200 [01:23<00:00,  2.39it/s]\n",
      "Loading dataset (task: full, split: test): 100%|██████████| 200/200 [01:15<00:00,  2.64it/s]\n",
      "Loading dataset (task: reynolds, split: test):  47%|████▋     | 231/496 [01:22<01:35,  2.76it/s]"
     ]
    }
   ],
   "source": [
    "# Load the required benchmark datasets\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "\n",
    "benchmark = AirfRANSBenchmark(benchmark_path=DIRECTORY_NAME,\n",
    "                              config_path=BENCH_CONFIG_PATH,\n",
    "                              benchmark_name=BENCHMARK_NAME,\n",
    "                              log_path=LOG_PATH)\n",
    "benchmark.load(path=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Packed MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 1: Architecture implementation using the ``torch-uncertainty`` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:24:54.416986800Z",
     "start_time": "2023-11-29T15:24:44.756112300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch_uncertainty.layers import PackedLinear\n",
    "\n",
    "\n",
    "class PackedMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple MLP with packed layers\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: str (default: \"PackedMLP\")\n",
    "        The name of the model\n",
    "    input_size: int (default: None)\n",
    "        The size of the input\n",
    "    output_size: int (default: None)\n",
    "        The size of the output\n",
    "    hidden_sizes: tuple (default: (100, 100,))\n",
    "        The sizes of the hidden layers\n",
    "    activation: torch.nn.functional (default: F.relu)\n",
    "        The activation function\n",
    "    dropout: bool (default: False)\n",
    "        Whether to use dropout\n",
    "    batch_normalization: bool (default: False)\n",
    "        Whether to use batch normalization\n",
    "    M: int (default: 4)\n",
    "        The number of estimators\n",
    "    alpha: int (default: 2)\n",
    "        The alpha parameter\n",
    "    gamma: int (default: 1)\n",
    "        The gamma parameter\n",
    "    device: str (default: \"cpu\")\n",
    "        The device to use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 name: str = \"PackedMLP\",\n",
    "                 input_size: int = None,\n",
    "                 output_size: int = None,\n",
    "                 hidden_sizes: tuple = (100, 100,),\n",
    "                 activation=F.relu,\n",
    "                 dropout: bool = False,\n",
    "                 M: int = 4,\n",
    "                 alpha: int = 2,\n",
    "                 gamma: int = 1,\n",
    "                 device: str = \"cpu\") -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # dropout\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=0.2)\n",
    "        else:\n",
    "            self.dropout = nn.Identity()\n",
    "\n",
    "        self.name = name\n",
    "        self.device = device\n",
    "\n",
    "        self.activation = activation\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "        self.num_estimators = M\n",
    "\n",
    "        self.input_layer = PackedLinear(self.input_size, self.hidden_sizes[0], alpha=alpha, num_estimators=M,\n",
    "                                        gamma=gamma, first=True,\n",
    "                                        device=device)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [PackedLinear(in_f, out_f, alpha=alpha, num_estimators=M, gamma=gamma, device=device) \\\n",
    "             for in_f, out_f in zip(self.hidden_sizes[:-1], self.hidden_sizes[1:])])\n",
    "        self.output_layer = PackedLinear(self.hidden_sizes[-1], self.output_size, alpha=alpha, num_estimators=M,\n",
    "                                         gamma=gamma, last=True,\n",
    "                                         device=device)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        The forward pass of the model\n",
    "        \"\"\"\n",
    "        out = self.input_layer(data)\n",
    "        for _, fc_ in enumerate(self.hidden_layers):\n",
    "            out = fc_(out)\n",
    "            out = self.activation(out)\n",
    "            out = self.dropout(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 2: Process the data to acquire the right Inputs and Outputs for the model alongside their dimensions\n",
    "This function uses a functionality offered by the Dataset class to extract the required inputs and outputs for the problem in hand, which facilitate the task. \n",
    "\n",
    "It also allows to create DataLoader from existing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:24:54.430636600Z",
     "start_time": "2023-11-29T15:24:54.419435700Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_dataset(dataset, batch_size: int = 128000, training: bool = False, shuffle: bool = False,\n",
    "                    n_workers: int = 0):\n",
    "    if training:\n",
    "        batch_size = batch_size\n",
    "        extract_x, extract_y = dataset.extract_data()\n",
    "    else:\n",
    "        batch_size = batch_size\n",
    "        extract_x, extract_y = dataset.extract_data()\n",
    "\n",
    "    torch_dataset = TensorDataset(torch.from_numpy(extract_x).float(), torch.from_numpy(extract_y).float())\n",
    "    data_loader = DataLoader(torch_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=n_workers)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def infer_input_output_size(dataset):\n",
    "    *dim_inputs, output_size = dataset.get_sizes()\n",
    "    input_size = np.sum(dim_inputs)\n",
    "    return input_size, output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 3: Implementation of the Training, Validation and Prediction functions\n",
    "\n",
    "**train.** This function allows to train (adjust the parameters of) your defined model using the provided datasets.\n",
    "\n",
    "**validate.** This function allows to validate your model on a validation dataset. The validation step is not mendatory and is used only to trace the model behavior (overfitting or not). \n",
    "\n",
    "**predict.** This function allows to predict using the trained model. The `DataSet` class provides a function `reconstruct_output` which allows to reshape the predictions in the correct form which will be comparable with ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T15:24:54.469196900Z",
     "start_time": "2023-11-29T15:24:54.443633Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader=None, epochs=100, lr=3e-4, device=\"cpu\", verbose=False):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # select your optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # select your loss function\n",
    "    loss_function = nn.MSELoss()\n",
    "    if verbose:\n",
    "        pbar = tqdm(range(epochs), desc=\"Epochs\")\n",
    "    else:\n",
    "        pbar = range(epochs)\n",
    "    for epoch in pbar:\n",
    "        # set your model for training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        # iterate over the batches of data\n",
    "        pbar_batch = tqdm(train_loader)\n",
    "        for batch in pbar_batch:\n",
    "            data, target = batch\n",
    "            # transfer your data on proper device. The model and your data should be on the same device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # reset the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # predict using your model on the current batch of data\n",
    "            prediction = model(data)\n",
    "            # compute the loss between prediction and real target, by repeating the target so it fits the different estimators \n",
    "            loss = loss_function(prediction, target.repeat(model.num_estimators, 1))\n",
    "            # compute the gradient (backward pass of back propagation algorithm)\n",
    "            loss.backward()\n",
    "            # update the parameters of your model\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * len(data)\n",
    "        # the validation step is optional\n",
    "        if val_loader is not None:\n",
    "            val_loss = validate(model, val_loader, device)\n",
    "            val_losses.append(val_loss)\n",
    "        mean_loss = total_loss / len(train_loader.dataset)\n",
    "        if verbose:\n",
    "            print(f\"Train Epoch: {epoch}   Avg_Loss: {mean_loss:.5f}\")\n",
    "        train_losses.append(mean_loss)\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "def validate(model, val_loader, device, verbose=False):\n",
    "    # set the model for evaluation (no update of the parameters)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    loss_function = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            data, target = batch\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            prediction = model(data)\n",
    "            loss = loss_function(prediction, target.repeat(model.num_estimators, 1))\n",
    "            total_loss += loss.item() * len(data)\n",
    "        mean_loss = total_loss / len(val_loader.dataset)\n",
    "        if verbose:\n",
    "            print(f\"Eval:   Avg_Loss: {mean_loss:.5f}\")\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def predict(model, dataset, device):\n",
    "    # set the model for the evaluation\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    observations = []\n",
    "    test_loader = process_dataset(dataset, training=False, shuffle=False)\n",
    "    # we dont require the computation of the gradient\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            data, target = batch\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            prediction = model(data)\n",
    "\n",
    "            #averaging the predictions of the different ensemble models\n",
    "            packed_split = rearrange(prediction, '(n b) m -> b n m', n=model.num_estimators)\n",
    "            packed_prediction = packed_split.mean(dim=1)\n",
    "\n",
    "            if device == torch.device(\"cpu\"):\n",
    "                predictions.append(packed_prediction.numpy())\n",
    "                observations.append(target.numpy())\n",
    "            else:\n",
    "                predictions.append(packed_prediction.cpu().data.numpy())\n",
    "                observations.append(target.cpu().data.numpy())\n",
    "    # reconstruct the prediction in the proper required shape of target variables\n",
    "    predictions = np.concatenate(predictions)\n",
    "    predictions = dataset.reconstruct_output(predictions)\n",
    "    # Do the same for the real observations\n",
    "    observations = np.concatenate(observations)\n",
    "    observations = dataset.reconstruct_output(observations)\n",
    "\n",
    "    return predictions, observations"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model selection (Cross validation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def build_k_indices(num_row, k_fold, seed):\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:24:54.482164600Z",
     "start_time": "2023-11-29T15:24:54.466167Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create cross validation on hyperparameters of the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def hyperparameters_tuning(\n",
    "        param_grid,\n",
    "        k_folds,\n",
    "        num_epochs,\n",
    "        batch_size: int = 128000,\n",
    "        shuffle: bool = False,\n",
    "        n_workers: int = 0,\n",
    "):\n",
    "    hyperparameter_grid = [\n",
    "        (hidden_layer_sizes, dropout, M, alpha, gamma, lr)\n",
    "        for hidden_layer_sizes in param_grid[\"hidden_sizes\"]\n",
    "        for dropout in param_grid[\"dropout\"]\n",
    "        for M in param_grid[\"M\"]\n",
    "        for alpha in param_grid[\"alpha\"]\n",
    "        for gamma in param_grid[\"gamma\"]\n",
    "        for lr in param_grid[\"lr\"]\n",
    "    ]\n",
    "\n",
    "    seed = 42\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = benchmark.train_dataset\n",
    "    input_size, output_size = infer_input_output_size(dataset)\n",
    "\n",
    "    results_df = pd.DataFrame(columns=param_grid.keys())\n",
    "    results = {}\n",
    "\n",
    "    for i, hyperparameter in enumerate(tqdm(hyperparameter_grid)):\n",
    "        extract_x, extract_y = dataset.extract_data()\n",
    "\n",
    "        # Define the K-fold Cross Validator\n",
    "        k_indices = build_k_indices(extract_y.shape[0], k_folds, seed=seed)\n",
    "        summed_total_loss = 0\n",
    "\n",
    "        # Start print\n",
    "        print('--------------------------------')\n",
    "\n",
    "        # K-fold Cross Validation model evaluation\n",
    "        for fold in range(k_folds):\n",
    "            val_ids = k_indices[fold]\n",
    "            train_ids = k_indices[~(np.arange(k_indices.shape[0]) == fold)]\n",
    "\n",
    "            train_x = extract_x[train_ids]\n",
    "            train_y = extract_y[train_ids]\n",
    "\n",
    "            train_x = train_x.reshape(train_x.shape[0] * train_x.shape[1], -1)\n",
    "            train_y = train_y.reshape(train_y.shape[0] * train_y.shape[1], -1)\n",
    "\n",
    "            val_x = extract_x[val_ids]\n",
    "            val_y = extract_y[val_ids]\n",
    "\n",
    "            train_dataset = TensorDataset(torch.from_numpy(train_x).float(), torch.from_numpy(train_y).float())\n",
    "            trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=n_workers)\n",
    "\n",
    "            val_dataset = TensorDataset(torch.from_numpy(val_x).float(), torch.from_numpy(val_y).float())\n",
    "            validateloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=n_workers)\n",
    "\n",
    "            # Init the neural network\n",
    "            model = PackedMLP(\n",
    "                input_size=input_size,\n",
    "                output_size=output_size,\n",
    "                hidden_sizes=hyperparameter[0],\n",
    "                activation=F.relu,\n",
    "                device=device,\n",
    "                dropout=hyperparameter[1],\n",
    "                M=hyperparameter[2],\n",
    "                alpha=hyperparameter[3],\n",
    "                gamma=hyperparameter[4],\n",
    "            )\n",
    "            model.to(device)\n",
    "\n",
    "            model, _, _ = train(model, trainloader, epochs=num_epochs, device=device, lr=hyperparameter[5])\n",
    "\n",
    "            mean_loss = validate(model, validateloader, device)\n",
    "\n",
    "            summed_total_loss += mean_loss\n",
    "\n",
    "        mean_total_loss = summed_total_loss / k_folds\n",
    "        # Print fold results\n",
    "        print(f'{k_folds}-FOLD CROSS VALIDATION RESULTS FOR {i}th HYPERPARAMETERS')\n",
    "        print('--------------------------------')\n",
    "        print(f'Average: {mean_total_loss}')\n",
    "\n",
    "        results.update({i: mean_total_loss})\n",
    "\n",
    "        new_row = {\n",
    "            'hidden_sizes': hyperparameter[0],\n",
    "            'dropout': hyperparameter[1],\n",
    "            'M': hyperparameter[2],\n",
    "            'alpha': hyperparameter[3],\n",
    "            'gamma': hyperparameter[4],\n",
    "            'lr': hyperparameter[5],\n",
    "        }\n",
    "        results_df.loc[len(results_df)] = new_row\n",
    "\n",
    "    return results_df, results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:43:26.527546400Z",
     "start_time": "2023-11-29T15:43:26.493488100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lr': [3e-4],\n",
    "    'hidden_sizes': [(50, 100, 50), (100, 200, 100), (200, 400, 200)],\n",
    "    'dropout': [True, False],\n",
    "    \"alpha\": [2, 3, 4],\n",
    "    \"gamma\": [1, 2, 4],\n",
    "    \"M\": [4],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:39:10.229068Z",
     "start_time": "2023-11-29T15:39:10.214595Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lr': [3e-4],\n",
    "    'hidden_sizes': [(48, 100, 48)],\n",
    "    'dropout': [True],\n",
    "    \"alpha\": [1],\n",
    "    \"gamma\": [1],\n",
    "    \"M\": [4],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:45:30.692789800Z",
     "start_time": "2023-11-29T15:45:30.652756100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/116 [00:00<?, ?it/s]\u001B[A\n",
      "  1%|          | 1/116 [00:12<24:09, 12.61s/it]\u001B[A\n",
      "  2%|▏         | 2/116 [00:23<22:45, 11.98s/it]\u001B[A\n",
      "  0%|          | 0/1 [00:30<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_csv, results = hyperparameters_tuning(param_grid, k_folds=5, num_epochs=1, batch_size=128000, shuffle=True,\n",
    "                                              n_workers=6)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T15:46:02.828363500Z",
     "start_time": "2023-11-29T15:45:32.271700800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Function to implement\"\"\"\n",
    "\n",
    "\n",
    "# Define the function to optimize the hyperparameters\n",
    "def optimize_hyperparameters(loss_fn, X_train, y_train, X_test, y_test, X_val, y_val, param_dist):\n",
    "    \"\"\"\n",
    "    Searches for the best hyperparameters, using a gridsearch approach\n",
    "    \"\"\"\n",
    "    best_validation_error = np.inf\n",
    "    best_hyperparameters = None\n",
    "\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = y_train.shape[1]\n",
    "\n",
    "    hyperparameter_grid = [(hidden_layer_sizes, activation, lr, batch_size, n_epochs)\n",
    "                           for hidden_layer_sizes in param_dist[\"hidden_layer_sizes\"]\n",
    "                           for activation in param_dist[\"activation\"]\n",
    "                           for lr in param_dist[\"lr\"]\n",
    "                           for batch_size in param_dist[\"batch_size\"]\n",
    "                           for n_epochs in param_dist[\"n_epochs\"]]\n",
    "\n",
    "    n_total = len(hyperparameter_grid)\n",
    "    counter = 1\n",
    "\n",
    "    for hidden_layer_sizes, activation, lr, batch_size, n_epochs in hyperparameter_grid:\n",
    "        print(f\"\\nstep: {counter}/{n_total}\")\n",
    "        print(\n",
    "            f\"hidden_layer_sizes: {hidden_layer_sizes}, activation:{activation}, lr{lr}, batch_size:{batch_size}, n_epochs:{n_epochs}\")\n",
    "\n",
    "        model = create_model(input_size, output_size, hidden_layer_sizes, activation)\n",
    "        train(model, X_train, X_test, y_train, y_test, lr, batch_size, n_epochs, loss_fn)\n",
    "\n",
    "        # Evaluate the best model on validation data\n",
    "        y_pred = model(X_val)\n",
    "        val_mse = loss_fn(y_val, y_pred)\n",
    "\n",
    "        if (val_mse < best_validation_error):\n",
    "            best_hyperparameters = [hidden_layer_sizes, activation, lr, batch_size, n_epochs]\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        training_error = loss_fn(model(X_train), y_train)\n",
    "        test_error = loss_fn(model(X_test), y_test)\n",
    "        val_error = loss_fn(model(X_val), y_val)\n",
    "\n",
    "        print(f\"training error: {training_error} , test error: {test_error} , val error: {val_error}\")\n",
    "        counter += 1\n",
    "\n",
    "    best_model = create_model(input_size, output_size, best_hyperparameters[0], best_hyperparameters[1])\n",
    "    return best_hyperparameters, best_model, best_validation_error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:22:24.132223400Z",
     "start_time": "2023-11-28T21:22:23.218956800Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = process_dataset(benchmark.train_dataset, training=True, n_workers=6)\n",
    "input_size, output_size = infer_input_output_size(benchmark.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:22:26.041921400Z",
     "start_time": "2023-11-28T21:22:25.989340700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PackedMLP(input_size=input_size,\n",
    "                  output_size=output_size,\n",
    "                  hidden_sizes=(50, 100, 50),\n",
    "                  activation=F.relu,\n",
    "                  device=device,\n",
    "                  dropout=True,\n",
    "                  )\n",
    "model.to(device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:22:31.445847900Z",
     "start_time": "2023-11-28T21:22:31.411243900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedMLP(\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (input_layer): PackedLinear(\n",
      "    (conv1x1): Conv1d(7, 100, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): PackedLinear(\n",
      "      (conv1x1): Conv1d(100, 200, kernel_size=(1,), stride=(1,), groups=4)\n",
      "    )\n",
      "    (1): PackedLinear(\n",
      "      (conv1x1): Conv1d(200, 100, kernel_size=(1,), stride=(1,), groups=4)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): PackedLinear(\n",
      "    (conv1x1): Conv1d(100, 16, kernel_size=(1,), stride=(1,), groups=4)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:23:04.571539100Z",
     "start_time": "2023-11-28T21:22:46.174380400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/145 [00:00<?, ?it/s]\u001B[A\n",
      "  1%|          | 1/145 [00:04<11:25,  4.76s/it]\u001B[A\n",
      "  1%|▏         | 2/145 [00:04<04:51,  2.04s/it]\u001B[A\n",
      "  2%|▏         | 3/145 [00:05<02:45,  1.16s/it]\u001B[A\n",
      "  3%|▎         | 4/145 [00:05<01:46,  1.32it/s]\u001B[A\n",
      "  3%|▎         | 5/145 [00:05<01:14,  1.89it/s]\u001B[A\n",
      "  4%|▍         | 6/145 [00:05<00:54,  2.56it/s]\u001B[A\n",
      "  5%|▍         | 7/145 [00:05<00:50,  2.74it/s]\u001B[A\n",
      "  6%|▌         | 8/145 [00:05<00:42,  3.20it/s]\u001B[A\n",
      "  6%|▌         | 9/145 [00:06<00:34,  3.91it/s]\u001B[A\n",
      "  7%|▋         | 10/145 [00:06<00:28,  4.68it/s]\u001B[A\n",
      "  8%|▊         | 11/145 [00:06<00:24,  5.36it/s]\u001B[A\n",
      "  8%|▊         | 12/145 [00:06<00:21,  6.09it/s]\u001B[A\n",
      "  9%|▉         | 13/145 [00:07<00:47,  2.76it/s]\u001B[A\n",
      " 10%|▉         | 14/145 [00:07<00:40,  3.20it/s]\u001B[A\n",
      " 10%|█         | 15/145 [00:07<00:32,  3.94it/s]\u001B[A\n",
      " 11%|█         | 16/145 [00:07<00:27,  4.65it/s]\u001B[A\n",
      " 12%|█▏        | 17/145 [00:07<00:24,  5.32it/s]\u001B[A\n",
      " 12%|█▏        | 18/145 [00:07<00:21,  6.00it/s]\u001B[A\n",
      " 13%|█▎        | 19/145 [00:08<00:43,  2.92it/s]\u001B[A\n",
      " 14%|█▍        | 20/145 [00:08<00:36,  3.39it/s]\u001B[A\n",
      " 14%|█▍        | 21/145 [00:08<00:30,  4.11it/s]\u001B[A\n",
      " 15%|█▌        | 22/145 [00:09<00:25,  4.88it/s]\u001B[A\n",
      " 16%|█▌        | 23/145 [00:09<00:21,  5.57it/s]\u001B[A\n",
      " 17%|█▋        | 24/145 [00:09<00:19,  6.18it/s]\u001B[A\n",
      " 17%|█▋        | 25/145 [00:10<00:38,  3.12it/s]\u001B[A\n",
      " 18%|█▊        | 26/145 [00:10<00:33,  3.57it/s]\u001B[A\n",
      " 19%|█▊        | 27/145 [00:10<00:27,  4.31it/s]\u001B[A\n",
      " 19%|█▉        | 28/145 [00:10<00:23,  4.98it/s]\u001B[A\n",
      " 20%|██        | 29/145 [00:10<00:20,  5.64it/s]\u001B[A\n",
      " 21%|██        | 30/145 [00:10<00:18,  6.26it/s]\u001B[A\n",
      " 21%|██▏       | 31/145 [00:11<00:39,  2.91it/s]\u001B[A\n",
      " 22%|██▏       | 32/145 [00:11<00:33,  3.39it/s]\u001B[A\n",
      " 23%|██▎       | 33/145 [00:11<00:27,  4.05it/s]\u001B[A\n",
      " 23%|██▎       | 34/145 [00:11<00:23,  4.78it/s]\u001B[A\n",
      " 24%|██▍       | 35/145 [00:12<00:20,  5.47it/s]\u001B[A\n",
      " 25%|██▍       | 36/145 [00:12<00:17,  6.09it/s]\u001B[A\n",
      " 26%|██▌       | 37/145 [00:12<00:32,  3.34it/s]\u001B[A\n",
      " 26%|██▌       | 38/145 [00:12<00:30,  3.51it/s]\u001B[A\n",
      " 27%|██▋       | 39/145 [00:13<00:24,  4.25it/s]\u001B[A\n",
      " 28%|██▊       | 40/145 [00:13<00:21,  4.99it/s]\u001B[A\n",
      " 28%|██▊       | 41/145 [00:13<00:18,  5.62it/s]\u001B[A\n",
      " 29%|██▉       | 42/145 [00:13<00:16,  6.16it/s]\u001B[A\n",
      " 30%|██▉       | 43/145 [00:14<00:33,  3.08it/s]\u001B[A\n",
      " 30%|███       | 44/145 [00:14<00:29,  3.37it/s]\u001B[A\n",
      " 31%|███       | 45/145 [00:14<00:24,  4.06it/s]\u001B[A\n",
      " 32%|███▏      | 46/145 [00:14<00:20,  4.72it/s]\u001B[A\n",
      " 32%|███▏      | 47/145 [00:14<00:17,  5.46it/s]\u001B[A\n",
      " 33%|███▎      | 48/145 [00:14<00:15,  6.13it/s]\u001B[A\n",
      " 34%|███▍      | 49/145 [00:15<00:30,  3.14it/s]\u001B[A\n",
      " 34%|███▍      | 50/145 [00:15<00:30,  3.13it/s]\u001B[A\n",
      " 35%|███▌      | 51/145 [00:16<00:24,  3.80it/s]\u001B[A\n",
      " 36%|███▌      | 52/145 [00:16<00:20,  4.53it/s]\u001B[A\n",
      " 37%|███▋      | 53/145 [00:16<00:17,  5.28it/s]\u001B[A\n",
      " 37%|███▋      | 54/145 [00:16<00:15,  5.84it/s]\u001B[AException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000217ACF3B8B0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\balanton\\anaconda3\\envs\\ml4science\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\balanton\\anaconda3\\envs\\ml4science\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\balanton\\anaconda3\\envs\\ml4science\\lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"C:\\Users\\balanton\\anaconda3\\envs\\ml4science\\lib\\multiprocessing\\popen_spawn_win32.py\", line 108, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "KeyboardInterrupt: \n",
      " 37%|███▋      | 54/145 [00:18<00:30,  2.94it/s]\n",
      "Epochs:   0%|          | 0/1 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[46], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model, train_losses, _ \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3e-4\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[19], line 34\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, train_loader, val_loader, epochs, lr, device)\u001B[0m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;66;03m# update the parameters of your model\u001B[39;00m\n\u001B[0;32m     33\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 34\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(data)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# the validation step is optional\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m val_loader \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model, train_losses, _ = train(model, train_loader, epochs=1, device=device, lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### prediction on `test_dataset`\n",
    "This dataset has the same distribution as the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T15:25:11.456699Z",
     "start_time": "2023-11-28T15:25:11.456699Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions, observations = predict(model, benchmark._test_dataset, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T15:25:11.457701800Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Prediction dimensions: \", predictions[\"x-velocity\"].shape, predictions[\"y-velocity\"].shape,\n",
    "      predictions[\"pressure\"].shape, predictions[\"turbulent_viscosity\"].shape)\n",
    "print(\"Observation dimensions:\", observations[\"x-velocity\"].shape, observations[\"y-velocity\"].shape,\n",
    "      observations[\"pressure\"].shape, observations[\"turbulent_viscosity\"].shape)\n",
    "print(\"We have good dimensions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T15:25:11.459698900Z"
    }
   },
   "outputs": [],
   "source": [
    "from lips.evaluation.airfrans_evaluation import AirfRANSEvaluation\n",
    "\n",
    "evaluator = AirfRANSEvaluation(config_path=BENCH_CONFIG_PATH,\n",
    "                               scenario=BENCHMARK_NAME,\n",
    "                               data_path=DIRECTORY_NAME,\n",
    "                               log_path=LOG_PATH)\n",
    "\n",
    "observation_metadata = benchmark._test_dataset.extra_data\n",
    "metrics = evaluator.evaluate(observations=observations,\n",
    "                             predictions=predictions,\n",
    "                             observation_metadata=observation_metadata)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction on `test_ood_dataset`\n",
    "This dataset has a different distribution in comparison to the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T15:25:11.460698700Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions, observations = predict(model, benchmark._test_ood_dataset, device=device)\n",
    "evaluator = AirfRANSEvaluation(config_path=BENCH_CONFIG_PATH,\n",
    "                               scenario=BENCHMARK_NAME,\n",
    "                               data_path=DIRECTORY_NAME,\n",
    "                               log_path=LOG_PATH)\n",
    "\n",
    "metrics = evaluator.evaluate(observations=observations,\n",
    "                             predictions=predictions,\n",
    "                             observation_metadata=observation_metadata)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
