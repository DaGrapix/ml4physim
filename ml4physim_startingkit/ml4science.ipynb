{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2228,
     "status": "ok",
     "timestamp": 1701505614095,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "0rUcVZoTmI7A",
    "outputId": "25da29da-fbb4-40e5-b2a7-1e0c04edd420",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.874572700Z",
     "start_time": "2023-12-03T17:42:05.617298100Z"
    }
   },
   "outputs": [],
   "source": [
    "colab = False\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    !source/content/drive/MyDrive/my_colab_env/bin/activate\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    sys.path.append(\"/content/drive/MyDrive/my_colab_env/lib/python3.10/site-packages\")\n",
    "    os.chdir(\"/content/drive/MyDrive/ml4science/ml4physim_startingkit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsOvkQ_CmG6z"
   },
   "source": [
    "# Packed Ensemble Application to the AirfRANS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQWKckDomG65"
   },
   "source": [
    "### Generic Step (Load the required data) <a id='generic_step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrkPT3UcmG66"
   },
   "source": [
    "Install the LIPS framework if it is not already done. For more information look at the LIPS framework [Github repository](https://github.com/IRT-SystemX/LIPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701505615102,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "g54uEia8mG67",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.879832900Z",
     "start_time": "2023-12-03T17:42:05.623864700Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# or\n",
    "# !pip install -U ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD9DFGIamG69"
   },
   "source": [
    "\n",
    "Install the AirfRANS package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1701505621668,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "DUEm2XlhmG69",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.881403Z",
     "start_time": "2023-12-03T17:42:05.632388500Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install airfrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDNZhB1tmG6-"
   },
   "source": [
    "### Generic Step (Load the required data) <a id='generic_step'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701505622187,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "J8e2OwYLmG6-",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.942558300Z",
     "start_time": "2023-12-03T17:42:05.639414700Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from lips import get_root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701505622617,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "23XBnSjnmG6_",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.957815900Z",
     "start_time": "2023-12-03T17:42:05.648478300Z"
    }
   },
   "outputs": [],
   "source": [
    "# indicate required paths\n",
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = '../ml4physim_startingkit/Dataset'\n",
    "BENCHMARK_NAME = \"Case1\"\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzJsnBWRmG6_"
   },
   "source": [
    "Define the configuration files path, that aim to describe specific caracteristics of the use case or the augmented simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701505623020,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "fZ9nrutLmG7A",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.958819800Z",
     "start_time": "2023-12-03T17:42:05.652823600Z"
    }
   },
   "outputs": [],
   "source": [
    "BENCH_CONFIG_PATH = os.path.join(\"airfoilConfigurations\", \"benchmarks\",\n",
    "                                 \"confAirfoil.ini\")  #Configuration file related to the benchmark\n",
    "SIM_CONFIG_PATH = os.path.join(\"airfoilConfigurations\", \"simulators\", \"torch_fc.ini\")  #Configuration file re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qovzIDjOmG7A"
   },
   "source": [
    "Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701505624431,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "JdfkvIIimluX",
    "outputId": "4c52f233-08fd-4085-c636-86c2946d9233",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.963513600Z",
     "start_time": "2023-12-03T17:42:05.662037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not os.path.isdir(DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2186,
     "status": "ok",
     "timestamp": 1701505627135,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "TIo_MeNVmG7B",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:07.631632200Z",
     "start_time": "2023-12-03T17:42:05.675721100Z"
    }
   },
   "outputs": [],
   "source": [
    "from lips.dataset.airfransDataSet import download_data\n",
    "\n",
    "if not os.path.isdir(DIRECTORY_NAME):\n",
    "    download_data(root_path=\".\", directory_name=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tWfcbpNmG7B"
   },
   "source": [
    "Loading the dataset using the dedicated class used by LIPS platform offers a list of advantages:\n",
    "\n",
    "1. Ease the importing of datasets\n",
    "1. A set of functions to organize the `inputs` and `outputs` required by augmented simulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 20655,
     "status": "ok",
     "timestamp": 1701505649736,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "cZujz-mpmG7B",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:53.607076500Z",
     "start_time": "2023-12-03T17:42:07.630121300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the required benchmark datasets\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    with open('benchmark.pkl', 'rb') as f:\n",
    "        benchmark = pickle.load(f)\n",
    "except:\n",
    "    benchmark = AirfRANSBenchmark(benchmark_path=DIRECTORY_NAME,\n",
    "                                  config_path=BENCH_CONFIG_PATH,\n",
    "                                  benchmark_name=BENCHMARK_NAME,\n",
    "                                  log_path=LOG_PATH)\n",
    "    benchmark.load(path=DIRECTORY_NAME)\n",
    "    with open('benchmark.pkl', 'wb') as f:\n",
    "        pickle.dump(benchmark, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0zh_rdsXmG7C"
   },
   "source": [
    "# Model selection (Cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmjFPEN9mG7C"
   },
   "source": [
    "Importing the necessary dependencies, as well as the `packed_ensemble` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701505651793,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "6SZsz0RdyK8k",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:53.619219600Z",
     "start_time": "2023-12-03T17:42:53.609076800Z"
    }
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2058,
     "status": "ok",
     "timestamp": 1701505651792,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "haMKF-humG7C",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:59.005101900Z",
     "start_time": "2023-12-03T17:42:53.614659800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2333,
     "status": "ok",
     "timestamp": 1701505691343,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "yJwmOMo4oLxW",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.354930300Z",
     "start_time": "2023-12-03T17:42:59.006100900Z"
    }
   },
   "outputs": [],
   "source": [
    "from my_packed_ensemble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1701505697714,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "uk8MmZ6hmG7D",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.368486500Z",
     "start_time": "2023-12-03T17:43:03.359876600Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_k_indices(num_row, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_row : int\n",
    "        Number of rows in the dataset.\n",
    "    k_fold : int\n",
    "        Number of folds\n",
    "    seed : int\n",
    "        Seed for random generator\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k_indices : np.array\n",
    "        Array of indices for each fold\"\"\"\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "p6ABwWCxmG7D"
   },
   "source": [
    "Create cross validation on hyperparameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701506164093,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "riiN62xNq5tB",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.393174100Z",
     "start_time": "2023-12-03T17:43:03.367506100Z"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from lips.dataset.scaler.standard_scaler import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def save_losses(train_losses_list: list, val_losses_list: list, folder: str, file_name: str):\n",
    "    \"\"\"\n",
    "    Saves the training and validation losses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_losses_list : list\n",
    "        List containing the training losses.\n",
    "    val_losses_list : list\n",
    "        List containing the validation losses.\n",
    "    folder : str\n",
    "        Folder where the losses will be saved.\n",
    "    file_name : str\n",
    "        Name of the file where the losses will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # create folder if it does not exist\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # save losses\n",
    "    df = pd.DataFrame({'train_loss': train_losses_list, 'val_loss': val_losses_list})\n",
    "    df.to_csv(folder + \"/\" + file_name, index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.408817100Z",
     "start_time": "2023-12-03T17:43:03.376748200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QVFd98OY2Xm1",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.409784900Z",
     "start_time": "2023-12-03T17:43:03.385490200Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_training_validation_losses_plot(train_losses_list: list, val_losses_list: list,\n",
    "                                         hyperparam_dict: dict, folder: str, plot_name: str):\n",
    "    \"\"\"\n",
    "    Saves the training and validation losses plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_losses_list : list\n",
    "        List containing the training losses.\n",
    "    val_losses_list : list\n",
    "        List containing the validation losses.\n",
    "    \"\"\"\n",
    "\n",
    "    # create folder if it does not exist\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # clear previous plot\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(train_losses_list, label='Training loss', color='blue')\n",
    "    plt.plot(val_losses_list, label='Validation loss', color='red')\n",
    "\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    plt.title(\n",
    "        f'Losses for hidden_sizes={hyperparam_dict[\"hidden_sizes\"]}, dropout={hyperparam_dict[\"dropout\"]}, M={hyperparam_dict[\"M\"]}, \\n alpha={hyperparam_dict[\"alpha\"]}, gamma={hyperparam_dict[\"gamma\"]}, lr={hyperparam_dict[\"lr\"]}')\n",
    "    plt.legend()\n",
    "    plt.savefig(folder + \"/\" + plot_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1701508470240,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "8yJvr53SmG7E",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:50:05.625075900Z",
     "start_time": "2023-12-03T17:50:05.614159Z"
    }
   },
   "outputs": [],
   "source": [
    "def hyperparameters_tuning(param_grid: dict, k_folds: int, num_epochs: int, batch_size: int = 128000,\n",
    "                           shuffle: bool = False, n_workers: int = 0, seed: int = 42, scaler: Scaler = None,\n",
    "                           partition: int = 0):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning using K-fold cross validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    param_grid : dict\n",
    "        Dictionary containing the values for each hyperparameter to be tested.\n",
    "    k_folds : int\n",
    "        Number of folds to be used in the cross validation.\n",
    "    num_epochs : int\n",
    "        Number of epochs to be used in the training.\n",
    "    batch_size : int\n",
    "        Batch size to be used in the training.\n",
    "    shuffle : bool\n",
    "        Whether to shuffle the training dataset.\n",
    "    n_workers : int\n",
    "        Number of workers to be used in the training.\n",
    "    seed : int\n",
    "        Random seed to be used in the training.\n",
    "    scaler : Scaler\n",
    "        Scaler to be used in the model.\n",
    "    partition : int\n",
    "        Partition of the hyperparameter grid to be used in this run.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame containing the results of the hyperparameter tuning.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate all combinations of parameter values\n",
    "    combinations = it.product(*(param_grid[key] for key in param_grid))\n",
    "\n",
    "    # create a new dictionary with keys as hyperparameter names and values as lists of combinations\n",
    "    hyperparameter_dict = {key: [] for key in param_grid}\n",
    "\n",
    "    # fill in the values for each key in the new dictionary\n",
    "    for combo in combinations:\n",
    "        for i, key in enumerate(param_grid):\n",
    "            hyperparameter_dict[key].append(combo[i])\n",
    "\n",
    "    hyperparameters_size = len(hyperparameter_dict[list(hyperparameter_dict.keys())[0]])\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = benchmark.train_dataset\n",
    "    input_size, output_size = infer_input_output_size(dataset)\n",
    "\n",
    "    extract_x, extract_y = dataset.extract_data()\n",
    "    results_df = pd.DataFrame(columns=[*param_grid.keys(), \"validation_loss\"])\n",
    "\n",
    "    try:\n",
    "        # open the file with checkpoints\n",
    "        with open(f\"CV/checkpoints/checkpoint_{partition}.txt\", \"rb\") as f:\n",
    "            checkpoint = int(f.read())\n",
    "    except:\n",
    "        if not os.path.isdir(\"CV/checkpoints\"):\n",
    "            os.makedirs(\"CV/checkpoints\")\n",
    "        checkpoint = -1\n",
    "\n",
    "    indices = range(hyperparameters_size)\n",
    "\n",
    "    if partition == 0:\n",
    "        indices = indices[:hyperparameters_size // 3]\n",
    "    elif partition == 1:\n",
    "        indices = indices[hyperparameters_size // 3: 2 * hyperparameters_size // 3]\n",
    "    elif partition == 2:\n",
    "        indices = indices[partition * hyperparameters_size // 3:]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid partition value. It must be 0, 1 or 2.\")\n",
    "\n",
    "    # remove the indices that have already been processed\n",
    "    indices = indices[checkpoint + 1:]\n",
    "\n",
    "    for i in tqdm(indices):\n",
    "        param_dict = {\n",
    "            'hidden_sizes': hyperparameter_dict[\"hidden_sizes\"][i],\n",
    "            'dropout': hyperparameter_dict[\"dropout\"][i],\n",
    "            'M': hyperparameter_dict[\"M\"][i],\n",
    "            'alpha': hyperparameter_dict[\"alpha\"][i],\n",
    "            'gamma': hyperparameter_dict[\"gamma\"][i],\n",
    "            'lr': hyperparameter_dict[\"lr\"][i]\n",
    "        }\n",
    "\n",
    "        print(f'Hyperparameters: {i}/hidden_sizes={hyperparameter_dict[\"hidden_sizes\"][i]}, \\\n",
    "              dropout={hyperparameter_dict[\"dropout\"][i]}, M={hyperparameter_dict[\"M\"][i]}, alpha={hyperparameter_dict[\"alpha\"][i]}, \\\n",
    "              gamma={hyperparameter_dict[\"gamma\"][i]}, lr={hyperparameter_dict[\"lr\"][i]}')\n",
    "\n",
    "        # define the K-fold Cross Validator\n",
    "        k_indices = build_k_indices(extract_y.shape[0], k_folds, seed=seed)\n",
    "        summed_validation_loss = 0\n",
    "\n",
    "        # k-fold Cross Validation model evaluation\n",
    "        for fold in range(k_folds):\n",
    "            print(f\"fold: {fold}\")\n",
    "\n",
    "            # initialize the Packed MLP model\n",
    "            model = PackedMLP(\n",
    "                input_size=input_size,\n",
    "                output_size=output_size,\n",
    "                hidden_sizes=hyperparameter_dict[\"hidden_sizes\"][i],\n",
    "                activation=F.relu,\n",
    "                device=device,\n",
    "                dropout=hyperparameter_dict[\"dropout\"][i],\n",
    "                M=hyperparameter_dict[\"M\"][i],\n",
    "                alpha=hyperparameter_dict[\"alpha\"][i],\n",
    "                gamma=hyperparameter_dict[\"gamma\"][i],\n",
    "                scaler=scaler\n",
    "            )\n",
    "            model.to(device)\n",
    "\n",
    "            val_ids = k_indices[fold]\n",
    "            train_ids = k_indices[~(np.arange(k_indices.shape[0]) == fold)]\n",
    "\n",
    "            train_x = extract_x[train_ids]\n",
    "            train_y = extract_y[train_ids]\n",
    "\n",
    "            train_x = train_x.reshape(train_x.shape[0] * train_x.shape[1], -1)\n",
    "            train_y = train_y.reshape(train_y.shape[0] * train_y.shape[1], -1)\n",
    "\n",
    "            val_x = extract_x[val_ids]\n",
    "            val_y = extract_y[val_ids]\n",
    "\n",
    "            trainloader = model.process_dataset(data=(train_x, train_y), training=True, batch_size=batch_size,\n",
    "                                                shuffle=shuffle, n_workers=n_workers)\n",
    "            validateloader = model.process_dataset(data=(val_x, val_y), training=False, batch_size=batch_size,\n",
    "                                                   shuffle=shuffle, n_workers=n_workers)\n",
    "\n",
    "            model, train_losses, val_losses = train(model=model, train_loader=trainloader, val_loader=validateloader,\n",
    "                                                    epochs=num_epochs, device=device, lr=hyperparameter_dict[\"lr\"][i],\n",
    "                                                    verbose=True)\n",
    "\n",
    "            summed_validation_loss += np.mean(val_losses)\n",
    "\n",
    "            # saving the losses\n",
    "            save_losses(train_losses_list=train_losses, val_losses_list=val_losses,\n",
    "                        folder=f\"CV/losses/hyperparameters_{i}\", file_name=f'fold_{fold}.csv')\n",
    "\n",
    "            # saving the curve\n",
    "            save_training_validation_losses_plot(train_losses_list=train_losses, val_losses_list=val_losses,\n",
    "                                                 hyperparam_dict=param_dict, folder=f\"CV/plots/hyperparameters_{i}\",\n",
    "                                                 plot_name=f'fold_{fold}.png')\n",
    "\n",
    "        mean_validation_loss = summed_validation_loss / k_folds\n",
    "        # print fold results\n",
    "        print(f'FOLD {fold} RESULTS FOR {i}th HYPERPARAMETERS')\n",
    "        print(f'Average validation loss: {mean_validation_loss}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        param_dict.update({'validation_loss': mean_validation_loss})\n",
    "        results_df.loc[len(results_df)] = param_dict\n",
    "        \n",
    "        checkpoint += 1\n",
    "        \n",
    "        # save the checkpoint\n",
    "        with open(f\"CV/checkpoints/checkpoint_{partition}.txt\", \"wb\") as f:\n",
    "            f.write(str(checkpoint).encode())\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1701507256997,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "ae107VZzmG7E",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.465900700Z",
     "start_time": "2023-12-03T17:43:03.456283300Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_sizes': [(48, 128, 48), (128, 256, 128)],\n",
    "    'dropout': [True, False],\n",
    "    \"alpha\": [2, 4],\n",
    "    \"gamma\": [2, 4],\n",
    "    \"M\": [4],\n",
    "    'lr': [1e-2, 1e-3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701505700914,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "HRm2njGrmG7E",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.479468100Z",
     "start_time": "2023-12-03T17:43:03.461846700Z"
    }
   },
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_sizes': [(48, 128, 48)],\n",
    "#     'dropout': [True],\n",
    "#     \"alpha\": [2],\n",
    "#     \"gamma\": [1],\n",
    "#     \"M\": [4],\n",
    "#     'lr': [3e-4],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701505701899,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "5g06af2oug-v",
    "outputId": "57fc0852-2843-448b-e18d-fe734928e87b",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.674263600Z",
     "start_time": "2023-12-03T17:43:03.471511900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `param_grid` will be divided in 3 partitions, each one will be executed on a different machine.\n",
    "\n",
    "Anton - partition 0\n",
    "Anthony - partition 1\n",
    "Alexi - partition 2\n",
    "\n",
    "Change it in the cell below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "partition = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.675222500Z",
     "start_time": "2023-12-03T17:43:03.649877500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 111517,
     "status": "error",
     "timestamp": 1701508586332,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "u_I6yZB6mG7F",
    "outputId": "a4b627fc-77c3-44b2-b6ad-cb027833b802",
    "ExecuteTime": {
     "end_time": "2023-12-03T17:50:44.000139Z",
     "start_time": "2023-12-03T17:50:41.931783300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: 5/hidden_sizes=(48, 128, 48),               dropout=True, M=4, alpha=4,               gamma=2, lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "fold: 1\n",
      "fold: 2\n",
      "I:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df = hyperparameters_tuning(param_grid, k_folds=4, num_epochs=100, batch_size=1280000, shuffle=True, n_workers=6,\n",
    "                                    scaler=StandardScaler(), partition=partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:24:52.411674600Z",
     "start_time": "2023-12-03T17:24:52.354304800Z"
    }
   },
   "outputs": [],
   "source": [
    "CV_RESULTS_FOLDER = \"CV/results\"\n",
    "\n",
    "# create folder if it does not exist\n",
    "if not os.path.isdir(CV_RESULTS_FOLDER):\n",
    "    os.makedirs(CV_RESULTS_FOLDER)\n",
    "\n",
    "results_df.to_csv(CV_RESULTS_FOLDER + \"/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cse1Puv6mG7F"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:22:24.132223400Z",
     "start_time": "2023-11-28T21:22:23.218956800Z"
    },
    "id": "U-hVXoVPmG7F"
   },
   "outputs": [],
   "source": [
    "input_size, output_size = infer_input_output_size(benchmark.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:22:26.041921400Z",
     "start_time": "2023-11-28T21:22:25.989340700Z"
    },
    "id": "M6TL7FmFmG7F"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PackedMLP(input_size=input_size,\n",
    "                  output_size=output_size,\n",
    "                  hidden_sizes=(50, 100, 50),\n",
    "                  activation=F.relu,\n",
    "                  device=device,\n",
    "                  dropout=True,\n",
    "                  )\n",
    "model.to(device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = model.process_dataset(benchmark.train_dataset, training=True, n_workers=6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:22:31.445847900Z",
     "start_time": "2023-11-28T21:22:31.411243900Z"
    },
    "id": "TOtho6vhmG7F"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:23:04.571539100Z",
     "start_time": "2023-11-28T21:22:46.174380400Z"
    },
    "id": "_-dtcqk8mG7G"
   },
   "outputs": [],
   "source": [
    "model, train_losses, _ = train(model, train_loader, epochs=1, device=device, lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_ctGjrHmG7G"
   },
   "source": [
    "##### prediction on `test_dataset`\n",
    "This dataset has the same distribution as the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T15:25:11.456699Z",
     "start_time": "2023-11-28T15:25:11.456699Z"
    },
    "id": "-fssWxBHmG7G"
   },
   "outputs": [],
   "source": [
    "predictions, observations = predict(model, benchmark._test_dataset, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T15:25:11.457701800Z"
    },
    "id": "sNjmwJbfmG7G"
   },
   "outputs": [],
   "source": [
    "print(\"Prediction dimensions: \", predictions[\"x-velocity\"].shape, predictions[\"y-velocity\"].shape,\n",
    "      predictions[\"pressure\"].shape, predictions[\"turbulent_viscosity\"].shape)\n",
    "print(\"Observation dimensions:\", observations[\"x-velocity\"].shape, observations[\"y-velocity\"].shape,\n",
    "      observations[\"pressure\"].shape, observations[\"turbulent_viscosity\"].shape)\n",
    "print(\"We have good dimensions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T15:25:11.459698900Z"
    },
    "id": "TFRk7TdYmG7G"
   },
   "outputs": [],
   "source": [
    "from lips.evaluation.airfrans_evaluation import AirfRANSEvaluation\n",
    "\n",
    "evaluator = AirfRANSEvaluation(config_path=BENCH_CONFIG_PATH,\n",
    "                               scenario=BENCHMARK_NAME,\n",
    "                               data_path=DIRECTORY_NAME,\n",
    "                               log_path=LOG_PATH)\n",
    "\n",
    "observation_metadata = benchmark._test_dataset.extra_data\n",
    "metrics = evaluator.evaluate(observations=observations,\n",
    "                             predictions=predictions,\n",
    "                             observation_metadata=observation_metadata)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0VVE2XzmG7G"
   },
   "source": [
    "##### Prediction on `test_ood_dataset`\n",
    "This dataset has a different distribution in comparison to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T15:25:11.460698700Z"
    },
    "id": "Q5SxkfOUmG7H"
   },
   "outputs": [],
   "source": [
    "predictions, observations = predict(model, benchmark._test_ood_dataset, device=device)\n",
    "evaluator = AirfRANSEvaluation(config_path=BENCH_CONFIG_PATH,\n",
    "                               scenario=BENCHMARK_NAME,\n",
    "                               data_path=DIRECTORY_NAME,\n",
    "                               log_path=LOG_PATH)\n",
    "\n",
    "metrics = evaluator.evaluate(observations=observations,\n",
    "                             predictions=predictions,\n",
    "                             observation_metadata=observation_metadata)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
