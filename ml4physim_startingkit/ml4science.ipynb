{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsOvkQ_CmG6z"
   },
   "source": [
    "# Packed Ensemble Application to the AirfRANS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `colab` to `True` if you wish to use it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.874572700Z",
     "start_time": "2023-12-03T17:42:05.617298100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2228,
     "status": "ok",
     "timestamp": 1701505614095,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "0rUcVZoTmI7A",
    "outputId": "25da29da-fbb4-40e5-b2a7-1e0c04edd420"
   },
   "outputs": [],
   "source": [
    "colab = False\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    !source/content/drive/MyDrive/my_colab_env/bin/activate\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    sys.path.append(\"/content/drive/MyDrive/my_colab_env/lib/python3.10/site-packages\")\n",
    "    os.chdir(\"/content/drive/MyDrive/ml4science/ml4physim_startingkit\")\n",
    "\n",
    "    sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQWKckDomG65"
   },
   "source": [
    "## Generic Step (Load the required data) <a id='generic_step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrkPT3UcmG66"
   },
   "source": [
    "Install the LIPS framework if it is not already done. For more information look at the LIPS framework [Github repository](https://github.com/IRT-SystemX/LIPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.879832900Z",
     "start_time": "2023-12-03T17:42:05.623864700Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701505615102,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "g54uEia8mG67"
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# or\n",
    "# !pip install -U ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD9DFGIamG69"
   },
   "source": [
    "\n",
    "Install the AirfRANS package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.881403Z",
     "start_time": "2023-12-03T17:42:05.632388500Z"
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1701505621668,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "DUEm2XlhmG69"
   },
   "outputs": [],
   "source": [
    "# !pip install airfrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDNZhB1tmG6-"
   },
   "source": [
    "## Generic Step (Load the required data) <a id='generic_step'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.942558300Z",
     "start_time": "2023-12-03T17:42:05.639414700Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701505622187,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "J8e2OwYLmG6-"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from lips import get_root_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.957815900Z",
     "start_time": "2023-12-03T17:42:05.648478300Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701505622617,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "23XBnSjnmG6_"
   },
   "outputs": [],
   "source": [
    "# indicate required paths\n",
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = '../ml4physim_startingkit/Dataset'\n",
    "BENCHMARK_NAME = \"Case1\"\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzJsnBWRmG6_"
   },
   "source": [
    "Define the configuration files path, that aim to describe specific caracteristics of the use case or the augmented simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.958819800Z",
     "start_time": "2023-12-03T17:42:05.652823600Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701505623020,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "fZ9nrutLmG7A"
   },
   "outputs": [],
   "source": [
    "BENCH_CONFIG_PATH = os.path.join(\"airfoilConfigurations\", \"benchmarks\",\n",
    "                                 \"confAirfoil.ini\")  #Configuration file related to the benchmark\n",
    "SIM_CONFIG_PATH = os.path.join(\"airfoilConfigurations\", \"simulators\", \"torch_fc.ini\")  #Configuration file re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qovzIDjOmG7A"
   },
   "source": [
    "Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:05.963513600Z",
     "start_time": "2023-12-03T17:42:05.662037Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701505624431,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "JdfkvIIimluX",
    "outputId": "4c52f233-08fd-4085-c636-86c2946d9233"
   },
   "outputs": [],
   "source": [
    "not os.path.isdir(DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:07.631632200Z",
     "start_time": "2023-12-03T17:42:05.675721100Z"
    },
    "executionInfo": {
     "elapsed": 2186,
     "status": "ok",
     "timestamp": 1701505627135,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "TIo_MeNVmG7B"
   },
   "outputs": [],
   "source": [
    "from lips.dataset.airfransDataSet import download_data\n",
    "\n",
    "if not os.path.isdir(DIRECTORY_NAME):\n",
    "    download_data(root_path=\".\", directory_name=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tWfcbpNmG7B"
   },
   "source": [
    "Loading the dataset using the dedicated class used by LIPS platform offers a list of advantages:\n",
    "\n",
    "1. Ease the importing of datasets\n",
    "1. A set of functions to organize the `inputs` and `outputs` required by augmented simulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:53.607076500Z",
     "start_time": "2023-12-03T17:42:07.630121300Z"
    },
    "executionInfo": {
     "elapsed": 20655,
     "status": "ok",
     "timestamp": 1701505649736,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "cZujz-mpmG7B"
   },
   "outputs": [],
   "source": [
    "# Load the required benchmark datasets\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    with open('benchmark.pkl', 'rb') as f:\n",
    "        benchmark = pickle.load(f)\n",
    "except:\n",
    "    benchmark = AirfRANSBenchmark(benchmark_path=DIRECTORY_NAME,\n",
    "                                  config_path=BENCH_CONFIG_PATH,\n",
    "                                  benchmark_name=BENCHMARK_NAME,\n",
    "                                  log_path=LOG_PATH)\n",
    "    benchmark.load(path=DIRECTORY_NAME)\n",
    "    with open('benchmark.pkl', 'wb') as f:\n",
    "        pickle.dump(benchmark, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0zh_rdsXmG7C"
   },
   "source": [
    "## Model selection (Cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmjFPEN9mG7C"
   },
   "source": [
    "Importing the necessary dependencies, as well as the `packed_ensemble` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:42:59.005101900Z",
     "start_time": "2023-12-03T17:42:53.614659800Z"
    },
    "executionInfo": {
     "elapsed": 2058,
     "status": "ok",
     "timestamp": 1701505651792,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "haMKF-humG7C"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import itertools as it\n",
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "from lips.dataset.scaler.standard_scaler import StandardScaler\n",
    "from my_packed_ensemble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.368486500Z",
     "start_time": "2023-12-03T17:43:03.359876600Z"
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1701505697714,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "uk8MmZ6hmG7D"
   },
   "outputs": [],
   "source": [
    "def build_k_indices(num_row, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_row : int\n",
    "        Number of rows in the dataset.\n",
    "    k_fold : int\n",
    "        Number of folds\n",
    "    seed : int\n",
    "        Seed for random generator\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k_indices : np.array\n",
    "        Array of indices for each fold\"\"\"\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.408817100Z",
     "start_time": "2023-12-03T17:43:03.376748200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_losses(train_losses_list: list, val_losses_list: list, folder: str, file_name: str):\n",
    "    \"\"\"\n",
    "    Saves the training and validation losses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_losses_list : list\n",
    "        List containing the training losses.\n",
    "    val_losses_list : list\n",
    "        List containing the validation losses.\n",
    "    folder : str\n",
    "        Folder where the losses will be saved.\n",
    "    file_name : str\n",
    "        Name of the file where the losses will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # create folder if it does not exist\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # save losses\n",
    "    df = pd.DataFrame({'train_loss': train_losses_list, 'val_loss': val_losses_list})\n",
    "    df.to_csv(folder + \"/\" + file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.409784900Z",
     "start_time": "2023-12-03T17:43:03.385490200Z"
    },
    "id": "QVFd98OY2Xm1"
   },
   "outputs": [],
   "source": [
    "def save_training_validation_losses_plot(train_losses_list: list, val_losses_list: list,\n",
    "                                         hyperparam_dict: dict, folder: str, plot_name: str):\n",
    "    \"\"\"\n",
    "    Saves the training and validation losses plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_losses_list : list\n",
    "        List containing the training losses.\n",
    "    val_losses_list : list\n",
    "        List containing the validation losses.\n",
    "    \"\"\"\n",
    "\n",
    "    # create folder if it does not exist\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # clear previous plot\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(train_losses_list, label='Training loss', color='blue')\n",
    "    plt.plot(val_losses_list, label='Validation loss', color='red')\n",
    "\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "\n",
    "    plt.title(\n",
    "        f'Losses for hidden_sizes={hyperparam_dict[\"hidden_sizes\"]}, dropout={hyperparam_dict[\"dropout\"]}, M={hyperparam_dict[\"M\"]}, \\n alpha={hyperparam_dict[\"alpha\"]}, gamma={hyperparam_dict[\"gamma\"]}, lr={hyperparam_dict[\"lr\"]}')\n",
    "    plt.legend()\n",
    "    plt.savefig(folder + \"/\" + plot_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create cross validation on hyperparameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyper_dict(param_grid: dict):\n",
    "    \"\"\"\n",
    "    Returns a grid of hyperparameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    param_grid : dict\n",
    "        Dictionary containing the hyperparameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hyperparameter_dict : dict\n",
    "        Dictionary containing the list of hyperparameters with the ith element of each list being one unique combination of the hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate all combinations of parameter values\n",
    "    combinations = it.product(*(param_grid[key] for key in param_grid))\n",
    "\n",
    "    # create a new dictionary with keys as hyperparameter names and values as lists of combinations\n",
    "    hyperparameter_dict = {key: [] for key in param_grid}\n",
    "\n",
    "    # fill in the values for each key in the new dictionary\n",
    "    for combo in combinations:\n",
    "        for i, key in enumerate(param_grid):\n",
    "            hyperparameter_dict[key].append(combo[i])\n",
    "    \n",
    "    return hyperparameter_dict\n",
    "\n",
    "def sample_data(data_x, data_y, size_scale, seed:Union[int, None] = None):\n",
    "    \"\"\"\n",
    "    Samples the data to reduce the size of the dataset to size_scale.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_x : np.array\n",
    "        Array containing the input data.\n",
    "    data_y : np.array\n",
    "        Array containing the output data.\n",
    "    size_scale : float\n",
    "        Percentage of the data to be sampled.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    processed_x : np.array\n",
    "        Array containing the sampled input data.\n",
    "    processed_y : np.array\n",
    "        Array containing the sampled output data.        \n",
    "    \"\"\"\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    sample_indices = np.arange(data_x.shape[0])\n",
    "    np.random.shuffle(sample_indices)\n",
    "    sample_indices = sample_indices[:int(size_scale*data_x.shape[0])]\n",
    "\n",
    "    processed_x = data_x[sample_indices]\n",
    "    processed_y = data_y[sample_indices]\n",
    "\n",
    "    return processed_x, processed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:50:05.625075900Z",
     "start_time": "2023-12-03T17:50:05.614159Z"
    },
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1701508470240,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "8yJvr53SmG7E"
   },
   "outputs": [],
   "source": [
    "def hyperparameters_tuning(benchmark: DataSet, param_grid: dict, k_folds: int, num_epochs: int, batch_size: int = 128000,\n",
    "                           shuffle: bool = False, n_workers: int = 0, seed: int = 42, scaler: Scaler = None,\n",
    "                           partition: int = 0, verbose=False, size_scale=0.3, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning using K-fold cross validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    param_grid : dict\n",
    "        Dictionary containing the values for each hyperparameter to be tested.\n",
    "    k_folds : int\n",
    "        Number of folds to be used in the cross validation.\n",
    "    num_epochs : int\n",
    "        Number of epochs to be used in the training.\n",
    "    batch_size : int\n",
    "        Batch size to be used in the training.\n",
    "    shuffle : bool\n",
    "        Whether to shuffle the training dataset.\n",
    "    n_workers : int\n",
    "        Number of workers to be used in the training.\n",
    "    seed : int\n",
    "        Random seed to be used in the training.\n",
    "    scaler : Scaler\n",
    "        Scaler to be used in the model.\n",
    "    partition : int\n",
    "        Partition of the hyperparameter grid to be used in this run.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    0 : int\n",
    "        Returns 0 if the function runs successfully.\n",
    "    \"\"\"\n",
    "\n",
    "    # set the random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if verbose: print(f\"Device: {device}\")\n",
    "\n",
    "    # get the hyperparameter grid dictionary\n",
    "    hyperparameter_dict = get_hyper_dict(param_grid)\n",
    "    hyperparameters_size = len(hyperparameter_dict[list(hyperparameter_dict.keys())[0]])\n",
    "\n",
    "    dataset = benchmark.train_dataset\n",
    "    input_size, output_size = infer_input_output_size(dataset)\n",
    "\n",
    "    extract_x, extract_y = dataset.extract_data()\n",
    "\n",
    "    # sample only a fraction of the dataset\n",
    "    extract_x, extract_y = sample_data(extract_x, extract_y, size_scale, seed=None)\n",
    "\n",
    "    results_df = pd.DataFrame(columns=[*param_grid.keys(), \"validation_loss\"])\n",
    "\n",
    "    try:\n",
    "        # read the checkpoint\n",
    "        with open(f\"CV/partition_{partition}/checkpoint_{partition}.txt\", \"rb\") as f:\n",
    "            checkpoint = int(f.read())\n",
    "    except:\n",
    "        checkpoint = -1\n",
    "\n",
    "    indices = range(hyperparameters_size)\n",
    "\n",
    "    if partition == 0:\n",
    "        indices = indices[:hyperparameters_size // 3]\n",
    "    elif partition == 1:\n",
    "        indices = indices[hyperparameters_size // 3: 2 * hyperparameters_size // 3]\n",
    "    elif partition == 2:\n",
    "        indices = indices[partition * hyperparameters_size // 3:]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid partition value. It must be 0, 1 or 2.\")\n",
    "\n",
    "    # remove the indices that have already been processed\n",
    "    indices = indices[checkpoint + 1:]\n",
    "    if verbose: indices = tqdm(indices)\n",
    "    for i in indices:\n",
    "        param_dict = {\n",
    "            'hidden_sizes': hyperparameter_dict[\"hidden_sizes\"][i],\n",
    "            'dropout': hyperparameter_dict[\"dropout\"][i],\n",
    "            'M': hyperparameter_dict[\"M\"][i],\n",
    "            'alpha': hyperparameter_dict[\"alpha\"][i],\n",
    "            'gamma': hyperparameter_dict[\"gamma\"][i],\n",
    "            'lr': hyperparameter_dict[\"lr\"][i]\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Hyperparameters: {i}/hidden_sizes={hyperparameter_dict[\"hidden_sizes\"][i]}, \\\n",
    "                    dropout={hyperparameter_dict[\"dropout\"][i]}, M={hyperparameter_dict[\"M\"][i]}, alpha={hyperparameter_dict[\"alpha\"][i]}, \\\n",
    "                    gamma={hyperparameter_dict[\"gamma\"][i]}, lr={hyperparameter_dict[\"lr\"][i]}')\n",
    "\n",
    "        # define the K-fold Cross Validator\n",
    "        k_indices = build_k_indices(extract_y.shape[0], k_folds, seed=seed)\n",
    "        summed_validation_loss = 0\n",
    "\n",
    "        # k-fold Cross Validation model evaluation\n",
    "        for fold in range(k_folds):\n",
    "            if verbose: print(f\"fold: {fold}\")\n",
    "\n",
    "            # initialize the Packed MLP model\n",
    "            model = PackedMLP(\n",
    "                input_size=input_size,\n",
    "                output_size=output_size,\n",
    "                hidden_sizes=hyperparameter_dict[\"hidden_sizes\"][i],\n",
    "                activation=F.relu,\n",
    "                device=device,\n",
    "                dropout=hyperparameter_dict[\"dropout\"][i],\n",
    "                M=hyperparameter_dict[\"M\"][i],\n",
    "                alpha=hyperparameter_dict[\"alpha\"][i],\n",
    "                gamma=hyperparameter_dict[\"gamma\"][i],\n",
    "                scaler=scaler\n",
    "            )\n",
    "            model.to(device)\n",
    "\n",
    "            val_ids = k_indices[fold]\n",
    "            train_ids = k_indices[~(np.arange(k_indices.shape[0]) == fold)]\n",
    "\n",
    "            train_x = extract_x[train_ids]\n",
    "            train_y = extract_y[train_ids]\n",
    "\n",
    "            train_x = train_x.reshape(train_x.shape[0] * train_x.shape[1], -1)\n",
    "            train_y = train_y.reshape(train_y.shape[0] * train_y.shape[1], -1)\n",
    "\n",
    "            val_x = extract_x[val_ids]\n",
    "            val_y = extract_y[val_ids]\n",
    "\n",
    "            trainloader = model.process_dataset(data=(train_x, train_y), training=True, batch_size=batch_size,\n",
    "                                                shuffle=shuffle, n_workers=n_workers)\n",
    "            validateloader = model.process_dataset(data=(val_x, val_y), training=False, batch_size=batch_size,\n",
    "                                                   shuffle=shuffle, n_workers=n_workers)\n",
    "\n",
    "            model, train_losses, val_losses = train(model=model, train_loader=trainloader, val_loader=validateloader,\n",
    "                                                    epochs=num_epochs, device=device, lr=hyperparameter_dict[\"lr\"][i],\n",
    "                                                    verbose=verbose)\n",
    "\n",
    "            summed_validation_loss += np.mean(val_losses)\n",
    "\n",
    "            # saving the losses\n",
    "            save_losses(train_losses_list=train_losses, val_losses_list=val_losses,\n",
    "                        folder=f\"CV/partition_{partition}/losses_{partition}/hyperparameters_{i}\", file_name=f'fold_{fold}.csv')\n",
    "\n",
    "            # saving the curve\n",
    "            save_training_validation_losses_plot(train_losses_list=train_losses, val_losses_list=val_losses,\n",
    "                                                 hyperparam_dict=param_dict, folder=f\"CV/partition_{partition}/plots_{partition}/hyperparameters_{i}\",\n",
    "                                                 plot_name=f'fold_{fold}.png')\n",
    "\n",
    "        mean_validation_loss = summed_validation_loss / k_folds\n",
    "        \n",
    "        if verbose:\n",
    "            # print fold results\n",
    "            print(f'FOLD {fold} RESULTS FOR {i}th HYPERPARAMETERS')\n",
    "            print(f'Average validation loss: {mean_validation_loss}')\n",
    "            print('-------------------------------- \\n\\n')\n",
    "\n",
    "        param_dict.update({'validation_loss': mean_validation_loss})\n",
    "        results_df.loc[len(results_df)] = param_dict\n",
    "        \n",
    "        # save the results\n",
    "        try:\n",
    "            pd.read_csv(f\"CV/partition_{partition}/results_{partition}.csv\").append(results_df).to_csv(f\"CV/partition_{partition}/results.csv\", index=False)\n",
    "        except:\n",
    "            results_df.to_csv(f\"CV/partition_{partition}/results_{partition}.csv\", index=False)\n",
    "\n",
    "        checkpoint += 1\n",
    "        # save the checkpoint\n",
    "        with open(f\"CV/partition_{partition}/checkpoint_{partition}.txt\", \"wb\") as f:\n",
    "            f.write(str(checkpoint).encode())\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.465900700Z",
     "start_time": "2023-12-03T17:43:03.456283300Z"
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1701507256997,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "ae107VZzmG7E"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_sizes': [(48, 128, 48), (128, 256, 128)],\n",
    "    'dropout': [True, False],\n",
    "    \"alpha\": [2, 4],\n",
    "    \"gamma\": [2, 4],\n",
    "    \"M\": [4],\n",
    "    'lr': [1e-2, 1e-3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.479468100Z",
     "start_time": "2023-12-03T17:43:03.461846700Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701505700914,
     "user": {
      "displayName": "Alexi Semiz",
      "userId": "03762052284520431505"
     },
     "user_tz": -60
    },
    "id": "HRm2njGrmG7E"
   },
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_sizes': [(48, 128, 48)],\n",
    "#     'dropout': [True],\n",
    "#     \"alpha\": [2],\n",
    "#     \"gamma\": [1],\n",
    "#     \"M\": [4],\n",
    "#     'lr': [3e-4],\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The `param_grid` will be divided in 3 partitions, each one will be executed on a different machine.\n",
    "\n",
    "- Anton - partition 0\n",
    "- Anthony - partition 1\n",
    "- Alexi - partition 2\n",
    "\n",
    "Change it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-03T17:43:03.675222500Z",
     "start_time": "2023-12-03T17:43:03.649877500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partition = 0\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# hyperparameter tuning using CV\n",
    "hyperparameters_tuning(benchmark=benchmark, param_grid=param_grid, k_folds=4, num_epochs=1, batch_size=1280000, shuffle=True, n_workers=6,\n",
    "                        scaler=StandardScaler(), partition=partition, verbose=True, size_scale=0.3, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cse1Puv6mG7F"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:22:24.132223400Z",
     "start_time": "2023-11-28T21:22:23.218956800Z"
    },
    "id": "U-hVXoVPmG7F"
   },
   "outputs": [],
   "source": [
    "input_size, output_size = infer_input_output_size(benchmark.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:22:26.041921400Z",
     "start_time": "2023-11-28T21:22:25.989340700Z"
    },
    "id": "M6TL7FmFmG7F"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PackedMLP(input_size=input_size,\n",
    "                  output_size=output_size,\n",
    "                  hidden_sizes=(50, 100, 50),\n",
    "                  activation=F.relu,\n",
    "                  device=device,\n",
    "                  dropout=True,\n",
    "                  )\n",
    "model.to(device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = model.process_dataset(benchmark.train_dataset, training=True, n_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:22:31.445847900Z",
     "start_time": "2023-11-28T21:22:31.411243900Z"
    },
    "id": "TOtho6vhmG7F"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T21:23:04.571539100Z",
     "start_time": "2023-11-28T21:22:46.174380400Z"
    },
    "id": "_-dtcqk8mG7G"
   },
   "outputs": [],
   "source": [
    "model, train_losses, _ = train(model, train_loader, epochs=1, device=device, lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_ctGjrHmG7G"
   },
   "source": [
    "##### prediction on `test_dataset`\n",
    "This dataset has the same distribution as the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-28T15:25:11.456699Z",
     "start_time": "2023-11-28T15:25:11.456699Z"
    },
    "id": "-fssWxBHmG7G"
   },
   "outputs": [],
   "source": [
    "predictions, observations = predict(model, benchmark._test_dataset, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T15:25:11.457701800Z"
    },
    "id": "sNjmwJbfmG7G"
   },
   "outputs": [],
   "source": [
    "print(\"Prediction dimensions: \", predictions[\"x-velocity\"].shape, predictions[\"y-velocity\"].shape,\n",
    "      predictions[\"pressure\"].shape, predictions[\"turbulent_viscosity\"].shape)\n",
    "print(\"Observation dimensions:\", observations[\"x-velocity\"].shape, observations[\"y-velocity\"].shape,\n",
    "      observations[\"pressure\"].shape, observations[\"turbulent_viscosity\"].shape)\n",
    "print(\"We have good dimensions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T15:25:11.459698900Z"
    },
    "id": "TFRk7TdYmG7G"
   },
   "outputs": [],
   "source": [
    "from lips.evaluation.airfrans_evaluation import AirfRANSEvaluation\n",
    "\n",
    "evaluator = AirfRANSEvaluation(config_path=BENCH_CONFIG_PATH,\n",
    "                               scenario=BENCHMARK_NAME,\n",
    "                               data_path=DIRECTORY_NAME,\n",
    "                               log_path=LOG_PATH)\n",
    "\n",
    "observation_metadata = benchmark._test_dataset.extra_data\n",
    "metrics = evaluator.evaluate(observations=observations,\n",
    "                             predictions=predictions,\n",
    "                             observation_metadata=observation_metadata)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0VVE2XzmG7G"
   },
   "source": [
    "##### Prediction on `test_ood_dataset`\n",
    "This dataset has a different distribution in comparison to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-28T15:25:11.460698700Z"
    },
    "id": "Q5SxkfOUmG7H"
   },
   "outputs": [],
   "source": [
    "predictions, observations = predict(model, benchmark._test_ood_dataset, device=device)\n",
    "evaluator = AirfRANSEvaluation(config_path=BENCH_CONFIG_PATH,\n",
    "                               scenario=BENCHMARK_NAME,\n",
    "                               data_path=DIRECTORY_NAME,\n",
    "                               log_path=LOG_PATH)\n",
    "\n",
    "metrics = evaluator.evaluate(observations=observations,\n",
    "                             predictions=predictions,\n",
    "                             observation_metadata=observation_metadata)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
