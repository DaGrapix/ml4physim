{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packed ensemble submission process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:53.997221600Z",
     "start_time": "2023-12-19T12:25:48.211731900Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lips import get_root_path\n",
    "from lips.dataset.scaler.standard_scaler import StandardScaler\n",
    "from lips.benchmark.airfransBenchmark import AirfRANSBenchmark\n",
    "from lips.dataset.airfransDataSet import download_data\n",
    "from lips.augmented_simulators.torch_simulator import TorchSimulator\n",
    "from lips.dataset.scaler.standard_scaler_iterative import StandardScalerIterative\n",
    "\n",
    "from my_augmented_simulator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Step (Load the required data) <a id='generic_step'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:54.013022400Z",
     "start_time": "2023-12-19T12:25:53.998103300Z"
    }
   },
   "outputs": [],
   "source": [
    "# indicate required paths\n",
    "LIPS_PATH = get_root_path()\n",
    "DIRECTORY_NAME = '../Dataset'\n",
    "BENCHMARK_NAME = \"Case1\"\n",
    "LOG_PATH = LIPS_PATH + \"lips_logs.log\"\n",
    "BENCHMARK_PATH = r\"C:/Users/antho/Desktop/ml4physim/src/benchmark.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the configuration files path, that aim to describe specific caracteristics of the use case or the augmented simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:54.033534200Z",
     "start_time": "2023-12-19T12:25:54.014022300Z"
    }
   },
   "outputs": [],
   "source": [
    "BENCH_CONFIG_PATH = os.path.join(\"..\", \"airfoilConfigurations\", \"benchmarks\",\n",
    "                                 \"confAirfoil.ini\")  #Configuration file related to the benchmark\n",
    "SIM_CONFIG_PATH = r\"config.ini\"  #Configuration file re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:54.046533700Z",
     "start_time": "2023-12-19T12:25:54.029532800Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(DIRECTORY_NAME):\n",
    "    download_data(root_path=\".\", directory_name=DIRECTORY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset using the dedicated class used by LIPS platform offers a list of advantages:\n",
    "\n",
    "1. Ease the importing of datasets\n",
    "1. A set of functions to organize the `inputs` and `outputs` required by augmented simulators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:54.065533700Z",
     "start_time": "2023-12-19T12:25:54.045532700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the required benchmark datasets\n",
    "def load_dataset(benchmark_path=BENCHMARK_PATH):\n",
    "    \"\"\"\n",
    "    Load the airfrans dataset as a benchmark object\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    benchmark : AirfRANSBenchmark\n",
    "        The airfrans benchmark object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(benchmark_path, 'rb') as f:\n",
    "            benchmark = pickle.load(f)\n",
    "    except:\n",
    "        benchmark = AirfRANSBenchmark(benchmark_path=DIRECTORY_NAME,\n",
    "                                    config_path=BENCH_CONFIG_PATH,\n",
    "                                    benchmark_name=BENCHMARK_NAME,\n",
    "                                    log_path=LOG_PATH)\n",
    "        benchmark.load(path=DIRECTORY_NAME)\n",
    "        with open(benchmark_path, 'wb') as f:\n",
    "            pickle.dump(benchmark, f)\n",
    "    \n",
    "    return benchmark\n",
    "\n",
    "#benchmark = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T12:25:54.075538100Z",
     "start_time": "2023-12-19T12:25:54.064533700Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate(config_names):\n",
    "    \"\"\"\n",
    "    Creates packed MLP models for each model defined by config_names, trains them and evaluates them on the test dataset.\n",
    "    The results are then saved in appropriate files (results + models).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config_names : list\n",
    "        List of the names of the configurations to be used for training the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        0 if we have successfully trained and evaluated the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    for config_name in config_names:\n",
    "        print(\"Config name : \", config_name)\n",
    "        print(\"loading data...\")\n",
    "        benchmark = load_dataset()\n",
    "\n",
    "        chunk_sizes=benchmark.train_dataset.get_simulations_sizes()\n",
    "        no_norm_x=benchmark.train_dataset.get_no_normalization_axis_indices()\n",
    "        scalerParams={\"chunk_sizes\":chunk_sizes,\"no_norm_x\":no_norm_x}\n",
    "\n",
    "        name = \"packed_mlp\"\n",
    "\n",
    "        print(\"defining model...\")\n",
    "        # PackedMLP model definition \n",
    "        torch_sim = TorchSimulator(name=name,\n",
    "                           model=PackedMLP,\n",
    "                           scaler=StandardScalerIterative,\n",
    "                           scalerParams=scalerParams,\n",
    "                           log_path=None,\n",
    "                           device=\"cuda:0\",\n",
    "                           seed=42,\n",
    "                           bench_config_path=BENCH_CONFIG_PATH,\n",
    "                           bench_config_name=\"Benchmark1\",\n",
    "                           sim_config_path=SIM_CONFIG_PATH,\n",
    "                           sim_config_name=config_name,\n",
    "                          )\n",
    "        \n",
    "        print(\"training...\")\n",
    "        # model training \n",
    "        start = time.perf_counter()\n",
    "        torch_sim.train(benchmark.train_dataset, \n",
    "                save_path=None,\n",
    "                pin_memory=True, \n",
    "                non_blocking=True, \n",
    "                num_workers=6\n",
    "                )\n",
    "        end = time.perf_counter()\n",
    "        train_time = end-start\n",
    "        \n",
    "        print(\"saving model...\")\n",
    "        # saving the model \n",
    "        torch_sim.save(path=\"./models_new\")\n",
    "\n",
    "        print(\"evaluating model...\")\n",
    "        # evaluating the model \n",
    "        start = time.perf_counter()\n",
    "        torch_sim_metrics = benchmark.evaluate_simulator(augmented_simulator=torch_sim,\n",
    "                                                  eval_batch_size=256000,\n",
    "                                                  dataset=\"all\",\n",
    "                                                  shuffle=False,\n",
    "                                                  save_path=\"./evaluations\",\n",
    "                                                  save_predictions=True\n",
    "                                                 )\n",
    "        end = time.perf_counter()\n",
    "        evaluation_time = end-start\n",
    "        \n",
    "        # save the evaluation time to file\n",
    "        with open(f\"evaluations/{name}_{config_name}/time.txt\", \"a\") as f:\n",
    "            f.write(f\"Training took {train_time:.2f} seconds\\n\")\n",
    "            f.write(f\"Evaluation took {evaluation_time:.2f} seconds\")\n",
    "        \n",
    "        del benchmark\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_names = [\"DEEP_SMALL_A6_DECAY\", \"DEEP_SMALL_A8_DECAY\"]\n",
    "simulate(config_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
